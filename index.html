
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8" />
    <title>KING!BOB!</title>
    <meta name="author" content="KING BOB" />
    <meta name="description" content="LET'S MAKE IT HAPPEN" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/pic.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>KING!BOB!</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;KING!BOB!</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div
        id="home-background"
        ref="homeBackground"
        data-images="/images/background2.jpg"
    ></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>KING!BOB!</h1>
                <h3></h3>
                <h5>LET&#39;S MAKE IT HAPPEN</h5>
            </div>
        </span>
    </div>
</div>
<div
    id="home-posts-wrap"
    ref="homePostsWrap"
    true
>
    <div id="home-posts">
        

<div class="post">
    <a href="/2025/09/21/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">朴素贝叶斯：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/21
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />

<h1 id="朴素贝叶斯：从理论到实践"><a href="#朴素贝叶斯：从理论到实践" class="headerlink" title="朴素贝叶斯：从理论到实践"></a>朴素贝叶斯：从理论到实践</h1><h2 id="一-朴素贝叶斯基本方法"><a href="#一-朴素贝叶斯基本方法" class="headerlink" title="一. 朴素贝叶斯基本方法"></a>一. 朴素贝叶斯基本方法</h2><p>朴素贝叶斯是一种基于贝叶斯定理与特征条件独立性假设的分类方法。它的基本思想非常简单直接：对于给定的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就将其归为哪个类别。</p>
<h3 id="a-学习联合概率分布-P-X-Y"><a href="#a-学习联合概率分布-P-X-Y" class="headerlink" title="a. 学习联合概率分布 P(X, Y)"></a>a. 学习联合概率分布 P(X, Y)</h3><p>首先，我们需要理解模型要学习什么。设输入特征空间 $\mathcal{X} \subseteq \mathbb{R}^n$ 为 n 维向量的集合，输出空间为类标记集合 $\mathcal{Y} &#x3D; {c_1, c_2, …, c_k}$。训练数据集 $T &#x3D; {(x_1, y_1), (x_2, y_2), …, (x_N, y_N)}$ 由 $P(X, Y)$ 独立同分布产生。</p>
<p>朴素贝叶斯法的目的，就是通过训练数据集来<strong>学习联合概率分布 $P(X, Y)$</strong>。具体来说，这个联合分布可以分解为：<br>$$P(X, Y) &#x3D; P(Y) \cdot P(X|Y)$$</p>
<p>其中：</p>
<ul>
<li>$P(Y)$ 是<strong>先验概率</strong>，即在没有任何特征信息的情况下，某一类别 $c_k$ 出现的概率。</li>
<li>$P(X|Y)$ 是<strong>条件概率分布</strong>，即在已知类别为 $c_k$ 的条件下，特征 $X$ 取特定值 $x$ 的概率。</li>
</ul>
<h3 id="b-条件独立性的强假设与“朴素”之名"><a href="#b-条件独立性的强假设与“朴素”之名" class="headerlink" title="b. 条件独立性的强假设与“朴素”之名"></a>b. 条件独立性的强假设与“朴素”之名</h3><p>条件概率 $P(X&#x3D;x|Y&#x3D;c_k)$ 是几乎所有概率模型中最难估计的部分。因为 $X &#x3D; (x^{(1)}, x^{(2)}, …, x^{(n)})$ 是一个维度非常高的向量，其可能的取值组合是指数级增长的。直接在有限的数据集上估计如此复杂的分布几乎是不可能的。</p>
<p>为了克服这个障碍，朴素贝叶斯法做出了一个强有力的、也是其被称为“朴素”的假设：<strong>特征条件独立性</strong>。即假设所有特征在类别确定的条件下都是相互独立的。</p>
<p>这个假设意味着，一个特征出现的概率与其他特征是否出现无关。虽然这个假设在现实中很少真正成立（例如，在文本中，“人工智能”这个词的出现显然与“深度学习”相关），但它极大地简化了计算。基于此假设，条件概率可以分解为：<br>$$P(X&#x3D;x|Y&#x3D;c_k) &#x3D; P(x^{(1)}, x^{(2)}, …, x^{(n)} | Y&#x3D;c_k) &#x3D; \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)$$</p>
<h3 id="c-贝叶斯定理"><a href="#c-贝叶斯定理" class="headerlink" title="c. 贝叶斯定理"></a>c. 贝叶斯定理</h3><p>学习到联合概率后，我们如何进行分类？这就要用到<strong>贝叶斯定理</strong>。贝叶斯定理为我们提供了在观察到特征 $X$ 后，估计类别 $Y$ 的概率（称为后验概率）的方法：<br>$$P(Y&#x3D;c_k | X&#x3D;x) &#x3D; \frac{P(X&#x3D;x| Y&#x3D;c_k) \cdot P(Y&#x3D;c_k)}{P(X&#x3D;x)} &#x3D; \frac{P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)}{\sum_{k} P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)}$$</p>
<p>其中，$P(X&#x3D;x)$ 是证据（Evidence），对于所有类别 $c_k$ 来说都是一个常数归一化因子，确保所有后验概率之和为1。</p>
<h3 id="d-生成模型"><a href="#d-生成模型" class="headerlink" title="d. 生成模型"></a>d. 生成模型</h3><p>由于朴素贝叶斯方法学习了联合概率分布 $P(X, Y)$，它实际上<strong>学习到了数据是如何生成的机制</strong>：它知道每个类别 $c_k$ 本身出现的先验概率 $P(Y&#x3D;c_k)$，以及在这个类别下，生成每个特征 $x^{(j)}$ 的概率 $P(x^{(j)}|Y&#x3D;c_k)$。因此，它属于<strong>生成模型</strong>（Generative Model）。与之相对的是判别模型（Discriminative Model，如逻辑回归、SVM），后者直接学习决策边界 $P(Y|X)$ 而不关心数据的生成方式。</p>
<h3 id="e-后验概率最大化与分类决策"><a href="#e-后验概率最大化与分类决策" class="headerlink" title="e. 后验概率最大化与分类决策"></a>e. 后验概率最大化与分类决策</h3><p>朴素贝叶斯的分类器即为将后验概率最大的类别作为输出。因此，对于给定的输入 $x$，其预测的类别 $\hat{y}$ 是：<br>$$\hat{y} &#x3D; \arg \max_{c_k \in \mathcal{Y}} P(Y&#x3D;c_k | X&#x3D;x)$$</p>
<p>由于分母 $P(X&#x3D;x)$ 对所有 $c_k$ 都相同，我们可以将其忽略，得到等价的公式：<br>$$\hat{y} &#x3D; \arg \max_{c_k \in \mathcal{Y}} P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)$$</p>
<p>这就是朴素贝叶斯分类器最终使用的决策函数。</p>
<hr>
<h2 id="二-参数估计"><a href="#二-参数估计" class="headerlink" title="二. 参数估计"></a>二. 参数估计</h2><p>在现实中，我们无法知道真实的概率 $P(Y&#x3D;c_k)$ 和 $P(x^{(j)}|Y&#x3D;c_k)$，只能从训练集中进行估计。最常用的方法就是<strong>极大似然估计（MLE）</strong>。</p>
<p><strong>极大似然估计</strong>的核心思想是：在已知随机变量属于某种分布（如伯努利分布、多项分布、高斯分布）但参数未知的情况下，寻找一组参数，使得当前观测到的样本数据出现的概率（似然度）最大。</p>
<h3 id="a-先验概率的极大似然估计"><a href="#a-先验概率的极大似然估计" class="headerlink" title="a. 先验概率的极大似然估计"></a>a. 先验概率的极大似然估计</h3><p>$$P(Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k)}{N}, \quad k&#x3D;1,2,…,K$$<br>其中 $I$ 是指示函数，当 $y_i &#x3D; c_k$ 时为1，否则为0。即，$P(Y&#x3D;c_k)$ 的估计值是训练集中类别为 $c_k$ 的样本所占的比例。</p>
<h3 id="b-条件概率的极大似然估计"><a href="#b-条件概率的极大似然估计" class="headerlink" title="b. 条件概率的极大似然估计"></a>b. 条件概率的极大似然估计</h3><p>条件概率的估计取决于特征所遵循的分布。对于第 $j$ 个特征 $x^{(j)}$，其取值集合为 ${a_{j1}, a_{j2}, …, a_{jS_j}}$，则条件概率的极大似然估计为：<br>$$P(x^{(j)} &#x3D; a_{jl} | Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(x_i^{(j)} &#x3D; a_{jl}, y_i &#x3D; c_k)}{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k)}$$<br>即，在类别为 $c_k$ 的样本中，第 $j$ 个特征取值 $a_{jl}$ 的样本所占的比例。</p>
<hr>
<h2 id="三-学习与分类算法"><a href="#三-学习与分类算法" class="headerlink" title="三. 学习与分类算法"></a>三. 学习与分类算法</h2><h3 id="a-朴素贝叶斯算法"><a href="#a-朴素贝叶斯算法" class="headerlink" title="a. 朴素贝叶斯算法"></a>a. 朴素贝叶斯算法</h3><p>结合上面的公式，朴素贝叶斯算法可以清晰地分为两步：</p>
<h4 id="步骤一：学习（训练）"><a href="#步骤一：学习（训练）" class="headerlink" title="步骤一：学习（训练）"></a>步骤一：学习（训练）</h4><p>基于训练数据集 $T$，利用极大似然估计法计算以下参数：</p>
<ol>
<li>估计先验概率：$P(Y&#x3D;c_k)$ for $k&#x3D;1,2,…,K$。</li>
<li>对于每个特征 $j&#x3D;1,2,…,n$，估计条件概率 $P(x^{(j)} &#x3D; a_{jl} | Y&#x3D;c_k)$ for $k&#x3D;1,2,…,K$ and $l&#x3D;1,2,…,S_j$。</li>
</ol>
<h4 id="步骤二：分类（预测）"><a href="#步骤二：分类（预测）" class="headerlink" title="步骤二：分类（预测）"></a>步骤二：分类（预测）</h4><p>对于一个新的实例 $x_{\text{new}} &#x3D; (x_{\text{new}}^{(1)}, x_{\text{new}}^{(2)}, …, x_{\text{new}}^{(n)})$，计算所有类别的后验概率（的分子部分）：<br>$$P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x_{\text{new}}^{(j)} | Y&#x3D;c_k), \quad k&#x3D;1,2,…,K$$<br>确定实例 $x_{\text{new}}$ 的类别：<br>$$\hat{y} &#x3D; \arg \max_{c_k} \left[ P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x_{\text{new}}^{(j)} | Y&#x3D;c_k) \right]$$</p>
<h3 id="b-贝叶斯估计与平滑技术"><a href="#b-贝叶斯估计与平滑技术" class="headerlink" title="b. 贝叶斯估计与平滑技术"></a>b. 贝叶斯估计与平滑技术</h3><p>极大似然估计有一个显著的缺陷：如果训练集中某个特征值和某个类别的组合从未出现过，那么其条件概率的估计值会为0。这会导致一个问题，在预测时，只要有一个特征的条件概率为0，无论其他特征多么强地指向某个类别，整个连乘的结果都会变成0，从而影响分类的准确性。</p>
<p>例如，在垃圾邮件分类中，训练集里“彩票”这个词从未在正常邮件中出现过（即 $P(\text{“彩票”} | Y&#x3D;\text{正常}) &#x3D; 0$），那么任何包含“彩票”这个词的邮件都会被直接判为垃圾邮件，这显然是不合理的。</p>
<p>为了解决这个问题，我们引入<strong>贝叶斯估计</strong>（或称为<strong>平滑技术</strong>）。最常用的方法是<strong>拉普拉斯平滑（Laplace Smoothing）</strong>。</p>
<ul>
<li><p><strong>先验概率的贝叶斯估计</strong>：<br>  $$P_{\lambda}(Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k) + \lambda}{N + K \lambda}$$<br>  当 $\lambda&#x3D;1$ 时，称为拉普拉斯平滑。$K$ 是类别数量。</p>
</li>
<li><p><strong>条件概率的贝叶斯估计</strong>：<br>  $$P_{\lambda}(x^{(j)} &#x3D; a_{jl} | Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(x_i^{(j)} &#x3D; a_{jl}, y_i &#x3D; c_k) + \lambda}{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k) + S_j \lambda}$$<br>  其中 $\lambda \geq 0$。当 $\lambda&#x3D;0$ 时，就是极大似然估计。当 $\lambda&#x3D;1$ 时，即为拉普拉斯平滑。$S_j$ 是第 $j$ 个特征可能取值的个数。</p>
</li>
</ul>
<p>拉普拉斯平滑等价于给每个特征的计数加上一个小的正数 $\lambda$（通常是1），从而避免了零概率问题。在样本数量足够大时，先验概率和条件概率的先验（即加上的 $\lambda$）的影响会变得微乎其微，估计值会逐渐趋近于实际的极大似然估计值。</p>
<p><strong>总结</strong>：朴素贝叶斯法因其简单、高效且在某些领域（尤其是文本分析）效果出色而经久不衰。其核心在于通过条件独立性假设简化联合概率的估计，并利用贝叶斯定理实现分类。</p>
<hr>
<h2 id="四-代码实践"><a href="#四-代码实践" class="headerlink" title="四. 代码实践"></a>四. 代码实践</h2><pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay

# 1. 加载数据
iris = load_iris()
X, y = iris.data, iris.target

# 2. 划分训练集与测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 3. 使用朴素贝叶斯分类器
model = GaussianNB()
model.fit(X_train, y_train)

# 4. 预测
y_pred = model.predict(X_test)

# 5. 打印准确率
print(&quot;测试集准确率:&quot;, accuracy_score(y_test, y_pred))

# 6. 可视化部分一：降维到二维并绘制散点图
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(10, 5))

# 原始类别分布
plt.subplot(1, 2, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=&quot;viridis&quot;, edgecolor=&quot;k&quot;)
plt.title(&quot;Iris 原始类别分布 (PCA 2D)&quot;)
plt.xlabel(&quot;PCA1&quot;)
plt.ylabel(&quot;PCA2&quot;)

# 预测类别分布
y_all_pred = model.predict(X)
plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_all_pred, cmap=&quot;viridis&quot;, edgecolor=&quot;k&quot;)
plt.title(&quot;Naive Bayes 预测结果 (PCA 2D)&quot;)
plt.xlabel(&quot;PCA1&quot;)
plt.ylabel(&quot;PCA2&quot;)

plt.tight_layout()
plt.show()

# 7. 可视化部分二：混淆矩阵
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=&quot;Blues&quot;)
plt.title(&quot;混淆矩阵&quot;)
plt.show()
</code></pre>
<p>可视化结果</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509211813850.png" alt="image-20250921181339683"></p>
<p> Gaussian Naive Bayes 在鸢尾花数据集上的表现比较好，超过九成样本预测正确。考虑到该数据集是经典的线性可分数据集，这个结果符合预期。</p>
<h3 id="PCA-二维可视化"><a href="#PCA-二维可视化" class="headerlink" title="PCA 二维可视化"></a>PCA 二维可视化</h3><ul>
<li>**左图 **：展示真实标签在降维后的二维空间的分布。紫色、青色、黄色分别代表三种鸢尾花。可以看到三个类别在二维投影下有一定分离，但中间区域存在重叠。</li>
<li>**右图 **：展示模型预测的类别分布。整体形状与左图接近，说明大多数样本被正确分类；但在类别 1 和类别 2 之间，部分点分布混杂，模型容易混淆。</li>
</ul>
<hr>
<h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><ul>
<li><p>第一类 (Setosa，标签 0)：<strong>15 个样本全部预测正确</strong>，说明 Naive Bayes 对这一类的识别非常准确。</p>
</li>
<li><p>第二类 (Versicolor，标签 1)：<strong>14 个预测正确，1 个被误判为类别 2</strong>。</p>
</li>
<li><p>第三类 (Virginica，标签 2)：<strong>12 个预测正确，3 个被误判为类别 1</strong>。</p>
<p>主要错误集中在 <strong>类别 1 和类别 2 之间</strong>，这与 PCA 散点图中它们的重叠区域相吻合。</p>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #ff7d73">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/21/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6/">
        <h2 class="post-title">统计学</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                统计学
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/19
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />



<h2 id="统计学"><a href="#统计学" class="headerlink" title="统计学"></a>统计学</h2><p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102007126.png" alt="image-20250910200732834"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102008025.png" alt="image-20250910200813926"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102009320.png" alt="image-20250910200942275"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102011021.png" alt="image-20250910201117966"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102011902.png" alt="image-20250910201107849"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102013607.png" alt="image-20250910201321554"></p>
<h2 id="1-回顾分解公式"><a href="#1-回顾分解公式" class="headerlink" title="1. 回顾分解公式"></a>1. 回顾分解公式</h2><p>在回归分析里有一个核心分解：</p>
<p>TSS&#x3D;ESS+SSRTSS &#x3D; ESS + SSR</p>
<ul>
<li><strong>TSS（总平方和）</strong>：所有观测值 $iY_i $相对于均值 Yˉ\bar{Y} 的总波动。</li>
<li><strong>ESS（回归平方和）</strong>：模型预测值 $\hat{Y}_i $相对于均值 $\bar{Y} $的波动。</li>
<li><strong>SSR（残差平方和）</strong>：实际值 $Y_i $和预测值 $\hat{Y}_i $的差异。</li>
</ul>
<hr>
<h2 id="2-为什么-ESS-叫做“模型捕捉到的波动”"><a href="#2-为什么-ESS-叫做“模型捕捉到的波动”" class="headerlink" title="2. 为什么 ESS 叫做“模型捕捉到的波动”"></a>2. 为什么 ESS 叫做“模型捕捉到的波动”</h2><p>你可以这样理解：</p>
<ul>
<li>如果我们不用任何模型，只预测均值 $\bar{Y}$，那么所有点都预测成一条水平线（比如平均工资）。此时解释力为 0。</li>
<li>如果我们用了回归模型，它会给出不同的预测值 $\hat{Y}_i$，这些预测值和均值的差异，正好说明了模型认为“哪些人比平均值高&#x2F;低、以及高&#x2F;低多少”。</li>
</ul>
<p>👉 所以，ESS 就是模型解释的那部分变化量。它反映了模型“识别出来的有规律的波动”。</p>
<hr>
<h2 id="3-举个具体例子"><a href="#3-举个具体例子" class="headerlink" title="3. 举个具体例子"></a>3. 举个具体例子</h2><p>还是房价的例子：</p>
<ul>
<li>平均房价 $\bar{Y}&#x3D;112$。</li>
<li>总变动（TSS&#x3D;2680）表示“所有房价相对均值的离散程度”。</li>
</ul>
<p>当我们引入面积作为解释变量：</p>
<ul>
<li>模型预测 $\hat{Y}$：小房子预测便宜，大房子预测贵。</li>
<li>这些预测值和均值之间的差异（ESS&#x3D;2295）就说明：面积这一个因素就能解释掉大部分的价格变动。</li>
<li>剩下的小部分（SSR&#x3D;175）是面积没解释到的，比如地段、装修等。</li>
</ul>
<p>所以：</p>
<p>$$\frac{ESS}{TSS} &#x3D; R^2 &#x3D; 0.935$$</p>
<p>意味着：93.5% 的房价波动可以用面积解释。</p>
<hr>
<h2 id="4-图形直观理解"><a href="#4-图形直观理解" class="headerlink" title="4. 图形直观理解"></a>4. 图形直观理解</h2><p>如果把 YY 画在纵轴上：</p>
<ul>
<li><strong>$bar{Y}$</strong>：一条水平线（没有解释变量时的预测）。</li>
<li><strong>$\hat{Y}_i$</strong>：回归直线给出的预测值（模型能识别的变化）。</li>
<li><strong>ESS</strong>：预测直线相对于水平线的波动，代表模型解释力。</li>
<li><strong>SSR</strong>：真实点和直线之间的距离平方，代表误差。</li>
</ul>
<hr>
<p>✅ <strong>一句话总结</strong>：<br> ESS 是“模型预测值和均值之间的差异平方和”，它衡量了模型从自变量中“抓住了多少因变量的规律性波动”。</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102017897.png" alt="image-20250910201704841"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102026715.png" alt="image-20250910202619574"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102029264.png" alt="image-20250910202904174"></p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509102038498.png" alt="image-20250910203828306"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" style="color: #ffa2c4">统计学</a>
        </span>
        
    </div>
    <a href="/2025/09/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/18/%E7%AE%97%E6%B3%95/">
        <h2 class="post-title">Leetcode 算法 八股</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E7%AE%97%E6%B3%95/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                算法
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/18
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />





<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="1-二分查找"><a href="#1-二分查找" class="headerlink" title="1. 二分查找"></a>1. 二分查找</h3><p>两数之和</p>
<pre><code class="language-python">class Solution:
    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:
        n = len(numbers)
        for i in range(n):
            low, high = i +1, n-1
            while low &lt;= high:
                mid = (low + high) // 2
                if numbers[mid] == target - numbers[i]:
                    return [i+1, mid+1]
                elif numbers[mid] &gt; target - numbers[i]:
                    high = mid - 1
                else:
                    low = mid + 1
        return [-1, -1]
</code></pre>
<p>二分查找只能用于有序数组，时间复杂度为O(logn)，对于两数之和问题，我们首先遍历数组找到第一个数O(n)，然后用二分查找在有序数组里找到第二个数O(logn)，总为O(nlogn)</p>
<h3 id="2-双指针"><a href="#2-双指针" class="headerlink" title="2. 双指针"></a>2. 双指针</h3><p><strong>只有一个输入，从两端开始遍历</strong></p>
<pre><code class="language-python">class Solution:
    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:
        low, high = 0, len(numbers) - 1
        while low &lt; high:
            total = numbers[low] + numbers[high]
            if total == target:
                return [low + 1, high + 1]
            elif total &lt; target:
                low += 1
            else:
                high -= 1

        return [-1, -1]
</code></pre>
<p>时间复杂度为O(n)</p>
<p><strong>有两个输入，两个都需要遍历完</strong></p>
<p><strong>《合并有序数组》</strong></p>
<pre><code class="language-python">class Solution:
    def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None:
        &quot;&quot;&quot;
        Do not return anything, modify nums1 in-place instead.
        &quot;&quot;&quot;
        sorted = []
        p1, p2 = 0, 0
        while p1 &lt; m or p2 &lt; n:
            if p1 == m:
                sorted.append(nums2[p2])
                p2 = p2 + 1
            elif p2 == n:
                sorted.append(nums1[p1])
                p1 = p1 + 1
            elif nums1[p1] &lt; nums2[p2]:
                sorted.append(nums1[p1])
                p1 = p1 + 1
            else:
                sorted.append(nums2[p2])
                p2 = p2 + 1
        nums1[:] = sorted
</code></pre>
<p>时间复杂度O(m+n)</p>
<h3 id="3-滑动窗口"><a href="#3-滑动窗口" class="headerlink" title="3. 滑动窗口"></a>3. 滑动窗口</h3><p>通用解法：</p>
<pre><code class="language-python">class Solution:
    def problemName(self, s: str) -&gt; int:
        # Step 1: 定义需要维护的变量们 (对于滑动窗口类题目，这些变量通常是最小长度，最大长度，或者哈希表)
        x, y = ..., ...

        # Step 2: 定义窗口的首尾端 (start, end)， 然后滑动窗口
        start = 0
        for end in range(len(s)):
            # Step 3: 更新需要维护的变量, 有的变量需要一个if语句来维护 (比如最大最小长度)
            x = new_x
            if condition:
                y = new_y

            &#39;&#39;&#39;
            ------------- 下面是两种情况，读者请根据题意二选1 -------------
            &#39;&#39;&#39;
            # Step 4 - 情况1
            # 如果题目的窗口长度固定：用一个if语句判断一下当前窗口长度是否超过限定长度 
            # 如果超过了，窗口左指针前移一个单位保证窗口长度固定, 在那之前, 先更新Step 1定义的(部分或所有)维护变量 
            if 窗口长度大于限定值:
                # 更新 (部分或所有) 维护变量 
                # 窗口左指针前移一个单位保证窗口长度固定

            # Step 4 - 情况2
            # 如果题目的窗口长度可变: 这个时候一般涉及到窗口是否合法的问题
            # 如果当前窗口不合法时, 用一个while去不断移动窗口左指针, 从而剔除非法元素直到窗口再次合法
            # 在左指针移动之前更新Step 1定义的(部分或所有)维护变量 
            while 不合法:
                # 更新 (部分或所有) 维护变量 
                # 不断移动窗口左指针直到窗口再次合法

        # Step 5: 返回答案
        return ...
</code></pre>
<h4 id="（1）子数组最大平均数"><a href="#（1）子数组最大平均数" class="headerlink" title="（1）子数组最大平均数"></a>（1）子数组最大平均数</h4><p>给你一个由 <code>n</code> 个元素组成的整数数组 <code>nums</code> 和一个整数 <code>k</code> 。</p>
<p>请你找出平均数最大且 <strong>长度为 <code>k</code></strong> 的连续子数组，并输出该最大平均数。</p>
<p>任何误差小于 <code>10-5</code> 的答案都将被视为正确答案。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：nums = [1,12,-5,-6,50,3], k = 4
输出：12.75
解释：最大平均数 (12-5-6+50)/4 = 51/4 = 12.75
</code></pre>
<p><strong>示例 2：</strong></p>
<pre><code>输入：nums = [5], k = 1
输出：5.00000
</code></pre>
<p><strong>解答：</strong></p>
<pre><code class="language-python">class Solution:
    def findMaxAverage(self, nums: List[int], k: int) -&gt; float:
        start = 0
        sum_, max_avg = 0, -math.inf
        for end, x in enumerate(nums):
            sum_ = sum_ + x
            if end + 1 - start &gt;= k:
                max_avg = max(max_avg, (sum_ / k))
                sum_ = sum_ - nums[start]
                start = start + 1
        return max_avg
</code></pre>
<h4 id="（2）至多包含两个不同字符的最长子串"><a href="#（2）至多包含两个不同字符的最长子串" class="headerlink" title="（2）至多包含两个不同字符的最长子串"></a>（2）至多包含两个不同字符的最长子串</h4><p>给你一个字符串 <code>s</code> ，请你找出 <strong>至多</strong> 包含 <strong>两个不同字符</strong> 的最长子串，并返回该子串的长度。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：s = &quot;eceba&quot;
输出：3
解释：满足题目要求的子串是 &quot;ece&quot; ，长度为 3 。
</code></pre>
<p><strong>示例 2：</strong></p>
<pre><code>输入：s = &quot;ccaabbb&quot;
输出：5
解释：满足题目要求的子串是 &quot;aabbb&quot; ，长度为 5 。
</code></pre>
<p>循环的条件：不符合题意（字串里面有超过三个的不同字符）</p>
<p><strong>解答：</strong></p>
<p><strong>最长类问题</strong>：</p>
<ul>
<li>先扩张窗口，让它尽可能大；</li>
<li>如果超出限制（非法），再收缩。</li>
<li><code>while</code> 条件 → <strong>窗口不合法时收缩</strong>。</li>
</ul>
<pre><code class="language-python">class Solution:
    def lengthOfLongestSubstringTwoDistinct(self, s: str) -&gt; int:
        start = 0
        max_len, hashmap = 0, &#123;&#125;

        for end in range(len(s)):
            tail = s[end]
            hashmap[tail] = hashmap.get(tail, 0) + 1
            while len(hashmap) &gt; 2:
                head = s[start]
                hashmap[head] -= 1
                if hashmap[head] == 0:
                    del hashmap[head]
                start += 1
            max_len = max(max_len, end + 1 - start)
        return max_len
</code></pre>
<h4 id="（3）无重复字符的最长字串"><a href="#（3）无重复字符的最长字串" class="headerlink" title="（3）无重复字符的最长字串"></a>（3）无重复字符的最长字串</h4><p>给定一个字符串 <code>s</code> ，请你找出其中不含有重复字符的 <strong>最长 子串</strong> 的长度。</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入: s = &quot;abcabcbb&quot;
输出: 3 
解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。
</code></pre>
<p><strong>示例 2:</strong></p>
<pre><code>输入: s = &quot;bbbbb&quot;
输出: 1
解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。
</code></pre>
<p><strong>示例 3:</strong></p>
<pre><code>输入: s = &quot;pwwkew&quot;
输出: 3
解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。
     请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。
</code></pre>
<p><strong>解答：</strong></p>
<ul>
<li><strong>最长类问题</strong>：<ul>
<li>先扩张窗口，让它尽可能大；</li>
<li>如果超出限制（非法），再收缩。</li>
<li><code>while</code> 条件 → <strong>窗口不合法时收缩</strong>。</li>
</ul>
</li>
</ul>
<pre><code class="language-python">class Solution:
    def lengthOfLongestSubstring(self, s: str) -&gt; int:
        start = 0
        hashmap = &#123;&#125;
        max_len = 0
        for end in range(len(s)):
            hashmap[s[end]] = hashmap.get(s[end], 0) + 1
            while end + 1 - start &gt; len(hashmap):
                head = s[start]
                hashmap[head] -= 1
                start += 1
                if hashmap[head] == 0:
                    del hashmap[head]
            max_len = max(max_len, end + 1 - start)

        return max_len
</code></pre>
<h4 id="（4）和大于某数值的长度最小的子数组"><a href="#（4）和大于某数值的长度最小的子数组" class="headerlink" title="（4）和大于某数值的长度最小的子数组"></a>（4）和大于某数值的长度最小的子数组</h4><p>给定一个含有 <code>n</code> 个正整数的数组和一个正整数 <code>target</code> <strong>。</strong></p>
<p>找出该数组中满足其总和大于等于 <code>target</code> 的长度最小的 <strong>子数组</strong> <code>[numsl, numsl+1, ..., numsr-1, numsr]</code> ，并返回其长度**。**如果不存在符合条件的子数组，返回 <code>0</code> 。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：target = 7, nums = [2,3,1,2,4,3]
输出：2
解释：子数组 [4,3] 是该条件下的长度最小的子数组。
</code></pre>
<p><strong>示例 2：</strong></p>
<pre><code>输入：target = 4, nums = [1,4,4]
输出：1
</code></pre>
<p><strong>示例 3：</strong></p>
<pre><code>输入：target = 11, nums = [1,1,1,1,1,1,1,1]
输出：0
</code></pre>
<p><strong>解答：</strong></p>
<p><strong>最短类问题</strong>：</p>
<ul>
<li>先扩张窗口，直到满足条件；</li>
<li>一旦满足条件，尝试收缩，保证尽可能小。</li>
<li><code>while</code> 条件 → <strong>窗口合法时收缩</strong>。</li>
</ul>
<pre><code class="language-python">class Solution:
    def minSubArrayLen(self, target: int, nums: List[int]) -&gt; int:
        start  = 0
        sum_ = 0
        min_len = math.inf
        for end in range(len(nums)):
            sum_ += nums[end]
            while sum_ &gt;= target:
                min_len = min(min_len, end + 1 -start)
                sum_ -= nums[start]
                start += 1
        if min_len == math.inf:
            return 0
        return min_len
</code></pre>
<h4 id="（5）找到字符串中所有字母异位词"><a href="#（5）找到字符串中所有字母异位词" class="headerlink" title="（5）找到字符串中所有字母异位词"></a>（5）找到字符串中所有字母异位词</h4><p>给定两个字符串 <code>s</code> 和 <code>p</code>，找到 <code>s</code> 中所有 <code>p</code> 的 <strong>异位词</strong> 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入: s = &quot;cbaebabacd&quot;, p = &quot;abc&quot;
输出: [0,6]
解释:
起始索引等于 0 的子串是 &quot;cba&quot;, 它是 &quot;abc&quot; 的异位词。
起始索引等于 6 的子串是 &quot;bac&quot;, 它是 &quot;abc&quot; 的异位词。
</code></pre>
<p> <strong>示例 2:</strong></p>
<pre><code>输入: s = &quot;abab&quot;, p = &quot;ab&quot;
输出: [0,1,2]
解释:
起始索引等于 0 的子串是 &quot;ab&quot;, 它是 &quot;ab&quot; 的异位词。
起始索引等于 1 的子串是 &quot;ba&quot;, 它是 &quot;ab&quot; 的异位词。
起始索引等于 2 的子串是 &quot;ab&quot;, 它是 &quot;ab&quot; 的异位词。
</code></pre>
<p><strong>解答：</strong></p>
<ul>
<li>窗口长度固定，判断条件用if</li>
</ul>
<pre><code class="language-python">class Solution:
    def findAnagrams(self, s: str, p: str) -&gt; List[int]:
        start = 0
        hashmap_, hashmap = &#123;&#125;, &#123;&#125;
        ans = []
        # 统计 p 中每个字符的频率
        for i, x in enumerate(p):
            hashmap_[x] = hashmap_.get(x, 0) + 1

        # 遍历 s，用滑动窗口检查子串是否是 p 的字母异位词
        for end in range(len(s)):
            # 扩展窗口：加入右边的字符
            hashmap[s[end]] = hashmap.get(s[end], 0) + 1

            # 如果窗口和目标频率表相等，说明找到一个异位词
            if hashmap == hashmap_:
                ans.append(start)

            # 保持窗口长度不超过 len(p)
            if len(p) &lt;= end - start + 1:
                hashmap[s[start]] -= 1
                if hashmap[s[start]] == 0:
                    del hashmap[s[start]]
                start += 1

        return ans
</code></pre>
<hr>
<h4 id="（6）删除子数组的最大得分"><a href="#（6）删除子数组的最大得分" class="headerlink" title="（6）删除子数组的最大得分"></a>（6）删除子数组的最大得分</h4><p>给你一个正整数数组 <code>nums</code> ，请你从中删除一个含有 <strong>若干不同元素</strong> 的子数组**。**删除子数组的 <strong>得分</strong> 就是子数组各元素之 <strong>和</strong> 。</p>
<p>返回 <strong>只删除一个</strong> 子数组可获得的 <strong>最大得分</strong> <em>。</em></p>
<p>如果数组 <code>b</code> 是数组 <code>a</code> 的一个连续子序列，即如果它等于 <code>a[l],a[l+1],...,a[r]</code> ，那么它就是 <code>a</code> 的一个子数组。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：nums = [4,2,4,5,6]
输出：17
解释：最优子数组是 [2,4,5,6]
</code></pre>
<p><strong>示例 2：</strong></p>
<pre><code>输入：nums = [5,2,1,2,5,2,1,2,5]
输出：8
解释：最优子数组是 [5,2,1] 或 [1,2,5]
</code></pre>
<p><strong>解答：</strong></p>
<p>也就是找到没有重复字符的最长子串，输出结果为子串和</p>
<p>最长类问题：超出限制，再收缩（while条件为不符合）</p>
<pre><code class="language-python">class Solution:
    def maximumUniqueSubarray(self, nums: List[int]) -&gt; int:
        start = 0
        sum_ = 0
        ans = 0
        hashmap = &#123;&#125;
        for end in range(len(nums)):
            # 从这里开始是子串扩张
            sum_ += nums[end]
            hashmap[nums[end]] = hashmap.get(nums[end], 0) + 1
            if len(hashmap) == end + 1 - start:
                # 满足条件时更新答案
                ans = max(ans, sum_)
            while len(hashmap) &lt; end + 1 - start:
                head = nums[start]
                # 子串缩小，记得删除头元素
                sum_ -= head
                hashmap[head] -= 1
                if hashmap[head] == 0:
                    del hashmap[head]
                start += 1
        return ans
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E7%AE%97%E6%B3%95%E3%80%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="color: #03a9f4">算法、数据结构</a>
        </span>
        
    </div>
    <a href="/2025/09/18/%E7%AE%97%E6%B3%95/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/">
        <h2 class="post-title">统计学习及监督学习概论</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                统计学
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/18
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />



<h2 id="统计学习及监督学习概论"><a href="#统计学习及监督学习概论" class="headerlink" title="统计学习及监督学习概论"></a>统计学习及监督学习概论</h2><p>统计学习的核心是基于数据构建概率统计模型，并运用模型对数据进行预测与分析。其基本流程可以概括为：<strong>模型</strong> -&gt; <strong>策略</strong> -&gt; <strong>算法</strong>。</p>
<h3 id="一。策略"><a href="#一。策略" class="headerlink" title="一。策略"></a>一。策略</h3><p>策略的核心是选择一个标准来评估模型的好坏，并以此为依据从假设空间中选择最优模型。</p>
<h4 id="1-损失函数与风险函数"><a href="#1-损失函数与风险函数" class="headerlink" title="1. 损失函数与风险函数"></a>1. 损失函数与风险函数</h4><p><strong>损失函数</strong> $L(Y, f(X))$ 度量模型<strong>一次预测</strong>的好坏，即预测值 $f(X)$ 与真实值 $Y$ 之间的差异程度。</p>
<ul>
<li><p><strong>0-1损失函数 (0-1 Loss Function)</strong><br>$$L(Y, f(X)) &#x3D; \begin{cases}<br>1, &amp; Y \neq f(X) \<br>0, &amp; Y &#x3D; f(X)<br>\end{cases}$$<br><strong>介绍</strong>：主要用于分类问题。预测正确则无损失（0），预测错误则产生恒定的损失（1）。它是分类问题最直观的损失函数，但因为其数学性质不光滑（非凸、不连续），直接优化较困难。</p>
</li>
<li><p><strong>平方损失函数 (Quadratic Loss Function)</strong><br>$$L(Y, f(X)) &#x3D; (Y - f(X))^2$$<br><strong>介绍</strong>：最常用于回归问题。其优势是数学性质优良（光滑、可导、凸函数），便于求解。但对异常值比较敏感。</p>
</li>
<li><p><strong>绝对损失函数 (Absolute Loss Function)</strong><br>$$L(Y, f(X)) &#x3D; |Y - f(X)|$$<br><strong>介绍</strong>：也用于回归问题。相比平方损失，它对异常值更不敏感，但其在零点处不可导，求解更复杂。</p>
</li>
<li><p><strong>对数损失函数 (Logarithmic Loss Function) &#x2F; 交叉熵损失 (Cross-Entropy Loss)</strong><br>$$L(Y, P(Y|X)) &#x3D; -\log P(Y|X)$$<br><strong>介绍</strong>：主要用于分类问题，特别是模型输出为概率的情况（如逻辑回归）。$P(Y|X)$ 是模型预测的正确类别的概率。损失随预测概率的减小而呈指数级增长，极大鼓励模型给出高置信度的正确预测。</p>
</li>
</ul>
<h4 id="2-风险函数-期望损失"><a href="#2-风险函数-期望损失" class="headerlink" title="2.风险函数  &#x2F; 期望损失"></a>2.风险函数  &#x2F; 期望损失</h4><p>期望风险函数 $R_{exp}(f)$ 衡量模型 $f(X)$ <strong>期望意义上的平均损失</strong>，是理论上最优模型的选择标准。<br>$$R_{exp}(f) &#x3D; E_p[L(Y, f(X))] &#x3D; \int_{\mathcal{X}\times\mathcal{Y}} L(y, f(x)) P(x, y)  dxdy$$<br><strong>介绍</strong>：我们的终极目标是最小化期望风险。然而，联合分布 $P(X, Y)$ 是未知的，因此 $R_{exp}(f)$ 无法直接计算。</p>
<h5 id="经验风险、-经验损失"><a href="#经验风险、-经验损失" class="headerlink" title="**经验风险、 经验损失 **"></a>**经验风险、 经验损失 **</h5><p>给定一个训练数据集，经验风险 $R_{emp}(f)$ 是模型在<strong>训练集上</strong>的平均损失。<br>$$R_{emp}(f) &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} L(y_i, f(x_i))$$<br><strong>介绍</strong>：根据<strong>大数定律 (Law of Large Numbers)</strong>，当样本容量 $N$ 趋于无穷时，经验风险会无限接近于期望风险。因此，一个很自然的想法就是使用<strong>经验风险</strong>来估计<strong>期望风险</strong>。</p>
<h5 id="经验风险最小化-Empirical-Risk-Minimization-ERM"><a href="#经验风险最小化-Empirical-Risk-Minimization-ERM" class="headerlink" title="经验风险最小化 (Empirical Risk Minimization, ERM)"></a><strong>经验风险最小化 (Empirical Risk Minimization, ERM)</strong></h5><p>$$\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i&#x3D;1}^{N} L(y_i, f(x_i))$$<br><strong>介绍</strong>：当样本量足够大时，ERM 非常有效。例如，<strong>极大似然估计 (Maximum Likelihood Estimation, MLE)</strong> 就是 ERM 的一个例子（当模型是条件概率分布，损失函数是对数损失时）。但当样本容量较小时，ERM 容易导致<strong>过拟合 (Overfitting)</strong>，即模型在训练集上表现很好，但在未知数据上表现很差。</p>
<p><strong>结构风险最小化 **<br>为了防止过拟合，SRM 在 ERM 的基础上增加了</strong>正则化项 (Regularizer)** 或<strong>罚项 (Penalty term)</strong>，用于衡量模型的复杂度。<br>$$\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i&#x3D;1}^{N} L(y_i, f(x_i)) + \lambda J(f)$$<br>其中 $J(f)$ 是模型复杂度，$\lambda \geq 0$ 是权衡经验风险与模型复杂度的系数。<br><strong>介绍</strong>：SRM 要求同时最小化经验风险和模型复杂度，从而寻找一个简单且预测精度高的模型，这符合<strong>奥卡姆剃刀原理</strong>。这等价于<strong>最大后验概率估计 (Maximum A Posteriori Estimation, MAP)</strong>。正则化项如 L1 范数（Lasso）、L2 范数（Ridge）都是常见的 $J(f)$。</p>
<hr>
<h3 id="二。模型评估与选择"><a href="#二。模型评估与选择" class="headerlink" title="二。模型评估与选择"></a>二。模型评估与选择</h3><p>模型训练的最终目的是为了很好的预测未知数据。因此需要一个<strong>测试集 (Test Set)</strong> 来模拟未知数据，以评估模型的<strong>泛化能力 (Generalization Ability)</strong>。</p>
<p>交叉验证 (Cross-Validation)</p>
<p>基本思想是重复使用数据：将数据切分，一部分作为训练集，另一部分作为验证集，来回交替训练和测试。</p>
<ul>
<li><strong>简单交叉验证</strong>：随机将数据分为训练集（如70%）和验证集（如30%），用训练集训练模型，用验证集评估测试误差。此过程重复多次，取误差平均值。</li>
<li><strong>S折交叉验证 (S-Fold Cross-Validation)</strong>：<ol>
<li>随机将数据切分为 $S$ 个互不相交、大小相同的子集。</li>
<li>每次用 $S-1$ 个子集的数据训练模型，剩下的1个子集做验证。</li>
<li>重复 $S$ 次，确保每个子集都被用作一次验证集。</li>
<li>对 $S$ 次评估结果取平均。<br>（最常见的是10折交叉验证）</li>
</ol>
</li>
<li><strong>留一交叉验证 (Leave-One-Out Cross-Validation, LOOCV)</strong>：是 $S$ 折交叉验证的特例，此时 $S &#x3D; N$（$N$ 为样本总量）。即每次只留下一个样本做验证，用其余所有样本训练。结果评估非常准确，但计算成本极高。</li>
</ul>
<hr>
<h3 id="三。监督学习方法：生成模型与判别模型"><a href="#三。监督学习方法：生成模型与判别模型" class="headerlink" title="三。监督学习方法：生成模型与判别模型"></a>三。监督学习方法：生成模型与判别模型</h3><ul>
<li><p><strong>生成模型 (Generative Model)</strong></p>
<ul>
<li><strong>本质</strong>：首先学习联合概率分布 $P(X, Y)$，然后通过贝叶斯定理求出条件概率分布 $P(Y|X)$ 作为预测模型。</li>
<li><strong>公式</strong>：$P(Y|X) &#x3D; \frac{P(X, Y)}{P(X)} &#x3D; \frac{P(X|Y)P(Y)}{P(X)}$</li>
<li><strong>例子</strong>：朴素贝叶斯、隐马尔可夫模型。</li>
<li><strong>特点</strong>：可以还原出联合分布，能处理隐变量问题。</li>
</ul>
</li>
<li><p><strong>判别模型 (Discriminative Model)</strong></p>
<ul>
<li><strong>本质</strong>：直接学习决策函数 $f(X)$ 或条件概率分布 $P(Y|X)$。</li>
<li><strong>公式</strong>：直接对 $P(Y|X)$ 建模或学习 $Y \rightarrow f(X)$ 的映射。</li>
<li><strong>例子</strong>：k近邻、感知机、逻辑回归、决策树、支持向量机。</li>
<li><strong>特点</strong>：准确率通常更高，且学习过程更简单直接。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="1-分类问题"><a href="#1-分类问题" class="headerlink" title="1. 分类问题"></a>1. 分类问题</h4><p>对于二分类问题，模型的预测结果和真实标签可以组成一个<strong>混淆矩阵 (Confusion Matrix)</strong>：</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">真实为正例 (Positive)</th>
<th align="left">真实为负例 (Negative)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>预测为正例</strong></td>
<td align="left">真正例 (True Positive, <strong>TP</strong>)</td>
<td align="left">假正例 (False Positive, <strong>FP</strong>)</td>
</tr>
<tr>
<td align="left"><strong>预测为负例</strong></td>
<td align="left">假负例 (False Negative, <strong>FN</strong>)</td>
<td align="left">真负例 (True Negative, <strong>TN</strong>)</td>
</tr>
</tbody></table>
<p>基于此矩阵，可以定义多个评估指标：</p>
<ul>
<li><p><strong>精确率 (Precision)</strong> - <strong>查准率</strong>：预测为正例的样本中，有多少是真的正例。<br>$$P &#x3D; \frac{TP}{TP + FP}$$</p>
</li>
<li><p><strong>召回率 (Recall)</strong> - <strong>查全率</strong>：真正的正例中，有多少被模型找了出来。<br>$$R &#x3D; \frac{TP}{TP + FN}$$</p>
</li>
<li><p><strong>F1值 (F1-Score)</strong>：精确率和召回率的调和平均。调和平均更重视较小值，因此P和R必须都高，F1才会高。<br>$$\frac{2}{F1} &#x3D; \frac{1}{P} + \frac{1}{R} \quad \Rightarrow \quad F1 &#x3D; \frac{2TP}{2TP + FP + FN} &#x3D; \frac{2 \cdot P \cdot R}{P + R}$$</p>
</li>
</ul>
<p><strong>P-R曲线</strong>下的面积和<strong>ROC曲线</strong>下的面积（AUC）也是综合衡量分类器性能的重要工具。</p>
<h4 id="2-标注问题"><a href="#2-标注问题" class="headerlink" title="2. 标注问题"></a>2. 标注问题</h4><p>可以看作是更复杂的分类问题。输入序列预测对应的输出序列。例如，<strong>词性标注 (POS Tagging)</strong>、<strong>命名实体识别 (NER)</strong>。常用的模型有隐马尔可夫模型 (HMM)、条件随机场 (CRF) 等。</p>
<h4 id="3-回归问题"><a href="#3-回归问题" class="headerlink" title="3. 回归问题"></a>3. 回归问题</h4><p>输出是连续值。其学习的本质是学习一个从输入 $X$ 到输出 $Y$ 的映射函数。评估回归模型性能最常用的指标是<strong>均方误差 (Mean Squared Error, MSE)</strong>，它直接对应于平方损失函数的经验风险。<br>$$MSE &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} (y_i - f(x_i))^2$$</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" style="color: #00bcd4">统计学</a>
        </span>
        
    </div>
    <a href="/2025/09/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/17/K%E8%BF%91%E9%82%BB%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">K近邻算法：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/17
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />



<h1 id="K近邻算法：从理论到实践"><a href="#K近邻算法：从理论到实践" class="headerlink" title="K近邻算法：从理论到实践"></a>K近邻算法：从理论到实践</h1><h2 id="1-核心思想"><a href="#1-核心思想" class="headerlink" title="1. 核心思想"></a>1. 核心思想</h2><p>K近邻（KNN）是一种基于实例的监督学习方法。其基本思想是：<br> <strong>对于一个待分类样本，根据训练集中与其“距离”最近的 kk 个邻居的类别，通过投票或加权投票的方式决定该样本的类别。</strong></p>
<p>数学表达：<br> 设训练集为</p>
<p>$${D} &#x3D; { (x_1,y_1), (x_2,y_2), \dots, (x_n,y_n) }, \quad x_i \in \mathbb{R}^d, ; y_i \in {1,2,\dots,C}$$</p>
<p>给定测试样本x，找到其最近的 kk 个邻居集合${N}_k(x)$。<br> 预测类别为：</p>
<p>$$\hat{y}(x) &#x3D; \arg\max_{c \in {1,\dots,C}} \sum_{(x_i,y_i) \in \mathcal{N}_k(x)} \mathbf{1}(y_i &#x3D; c)$$</p>
<p>其中，${1}(\cdot)$ 是指示函数。</p>
<p>如果采用加权投票（考虑距离远近），则为：</p>
<p>$$\hat{y}(x) &#x3D; \arg\max_{c \in {1,\dots,C}} \sum_{(x_i,y_i) \in \mathcal{N}_k(x)} \frac{1}{|x - x_i|} \cdot \mathbf{1}(y_i &#x3D; c)$$</p>
<hr>
<h2 id="2-距离度量"><a href="#2-距离度量" class="headerlink" title="2. 距离度量"></a>2. 距离度量</h2><p>KNN 依赖距离来衡量样本相似度。常见的度量方式有：</p>
<ul>
<li>欧氏距离：</li>
</ul>
<p>$$d(x_i, x_j) &#x3D; \sqrt{\sum_{l&#x3D;1}^d (x_i^{(l)} - x_j^{(l)})^2}$$</p>
<ul>
<li>曼哈顿距离：</li>
</ul>
<p>$$d(x_i, x_j) &#x3D; \sum_{l&#x3D;1}^d |x_i^{(l)} - x_j^{(l)}|$$</p>
<ul>
<li>闵可夫斯基距离（推广形式）：</li>
</ul>
<p>$$d(x_i, x_j) &#x3D; \left( \sum_{l&#x3D;1}^d |x_i^{(l)} - x_j^{(l)}|^p \right)^{1&#x2F;p}$$</p>
<hr>
<h2 id="3-k的选择与误差分析"><a href="#3-k的选择与误差分析" class="headerlink" title="3. k的选择与误差分析"></a>3. k的选择与误差分析</h2><p>KNN 的性能对 k 值选择敏感，体现了 <strong>近似误差</strong> 与 <strong>估计误差</strong> 的权衡。</p>
<h3 id="3-1-近似误差"><a href="#3-1-近似误差" class="headerlink" title="3.1 近似误差"></a>3.1 近似误差</h3><ul>
<li>定义：模型表达能力不足，导致预测结果无法逼近真实分布。</li>
<li><strong>k 较大时</strong>：决策边界过于平滑，难以捕捉复杂模式 → <strong>近似误差大</strong>。</li>
<li><strong>k 较小时</strong>：决策边界灵活，可以更好地拟合真实模式 → <strong>近似误差小</strong>。</li>
</ul>
<p>数学上，假设真实函数为 f(x)，KNN 的期望预测为：</p>
<p>$$\hat{f}(x) &#x3D; \mathbb{E}_{\mathcal{D}}[\hat{y}(x)]$$</p>
<p>则近似误差为：</p>
<p>$$\text{Bias}^2(x) &#x3D; \big( \mathbb{E}_{\mathcal{D}}[\hat{y}(x)] - f(x) \big)^2$$</p>
<h3 id="3-2-估计误差"><a href="#3-2-估计误差" class="headerlink" title="3.2 估计误差"></a>3.2 估计误差</h3><ul>
<li>定义：模型对有限训练数据过于依赖，泛化性差，导致预测不稳定。</li>
<li><strong>k 较小时</strong>：极易受噪声点影响，估计误差大。</li>
<li><strong>k 较大时</strong>：结果受单个点波动影响小，估计误差小。</li>
</ul>
<p>其数学形式为：</p>
<p>$$\text{Var}(x) &#x3D; \mathbb{E}<em>{\mathcal{D}}\big[(\hat{y}(x) - \mathbb{E}</em>{\mathcal{D}}[\hat{y}(x)])^2\big]$$</p>
<h3 id="3-3-总误差"><a href="#3-3-总误差" class="headerlink" title="3.3 总误差"></a>3.3 总误差</h3><p>$$text{MSE}(x) &#x3D; \text{Bias}^2(x) + \text{Var}(x) + \sigma^2$$</p>
<p>其中，$\sigma^2$ 是不可约误差。<br> 因此，选择合适的 k 值非常重要。</p>
<hr>
<h2 id="4-kd树的构造与搜索"><a href="#4-kd树的构造与搜索" class="headerlink" title="4. kd树的构造与搜索"></a>4. kd树的构造与搜索</h2><p>由于 KNN 需要计算测试点与所有训练点的距离，时间复杂度为O(n)。为了加速，可以用 <strong>kd树</strong>进行近邻搜索。</p>
<h3 id="4-1-kd树的构造"><a href="#4-1-kd树的构造" class="headerlink" title="4.1 kd树的构造"></a>4.1 kd树的构造</h3><ul>
<li>kd树是一种对数据进行递归二分的空间划分结构。</li>
<li>每次选择一个维度（通常是方差最大的维度），按照该维度的中位数划分数据。</li>
<li>构造过程：<ol>
<li>从根节点开始，选择一个维度作为切分轴；</li>
<li>找到该维度的中位数，作为节点存储值；</li>
<li>左子树存储小于该值的样本，右子树存储大于该值的样本；</li>
<li>递归进行直到样本数过少或树深度达到限制。</li>
</ol>
</li>
</ul>
<p><strong>伪代码：</strong></p>
<blockquote>
<p>function build_kd_tree(points, depth):<br>    if points is empty:<br>        return None<br>    axis &#x3D; depth mod d<br>    sort points by axis<br>    median &#x3D; len(points) &#x2F;&#x2F; 2<br>    node &#x3D; new Node(points[median])<br>    node.left &#x3D; build_kd_tree(points[:median], depth+1)<br>    node.right &#x3D; build_kd_tree(points[median+1:], depth+1)<br>    return node</p>
</blockquote>
<hr>
<h3 id="4-2-kd树的搜索"><a href="#4-2-kd树的搜索" class="headerlink" title="4.2 kd树的搜索"></a>4.2 kd树的搜索</h3><p>kd树搜索遵循“回溯+剪枝”原则：</p>
<ol>
<li>从根节点开始，递归到叶子节点，找到测试点所属的区域；</li>
<li>以该叶子节点为“当前最近邻”；</li>
<li>回溯检查父节点和另一子树，若另一子树中可能存在更近邻，则递归进入；</li>
<li>维护一个大小为 kk 的优先队列，存储当前最近的 kk 个邻居；</li>
<li>搜索结束时队列中的点即为近邻结果。</li>
</ol>
<p><strong>伪代码：</strong></p>
<blockquote>
<p>function knn_search(node, target, k, depth):<br>    if node is None:<br>        return<br>    axis &#x3D; depth mod d<br>    if target[axis] &lt; node.point[axis]:<br>        next &#x3D; node.left<br>        other &#x3D; node.right<br>    else:<br>        next &#x3D; node.right<br>        other &#x3D; node.left</p>
</blockquote>
<pre><code>function knn_search(node, target, k, depth):
    if node is None:
        return
    axis = depth mod d
    if target[axis] &lt; node.point[axis]:
        next = node.left
        other = node.right
    else:
        next = node.right
        other = node.left
    
    knn_search(next, target, k, depth+1)
    update priority queue with node.point
    
    if |target[axis] - node.point[axis]| &lt; current_max_distance_in_queue:
        knn_search(other, target, k, depth+1)
</code></pre>
<hr>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ul>
<li><strong>核心思想</strong>：KNN 通过寻找最近的 kk 个邻居来分类或回归。</li>
<li><strong>k 的选择</strong>：小 kk → 近似误差小、估计误差大（过拟合）；大 kk → 近似误差大、估计误差小（欠拟合）。</li>
<li><strong>kd树</strong>：通过空间划分加速近邻搜索，提升算法效率。</li>
</ul>
<p>最终，KNN 的关键在于 <strong>合适的 k 值选择</strong> 和 <strong>高效的搜索结构</strong>。</p>
<hr>
<h3 id="6-K近邻用于Iris数据集分类"><a href="#6-K近邻用于Iris数据集分类" class="headerlink" title="6. K近邻用于Iris数据集分类"></a>6. K近邻用于Iris数据集分类</h3><h4 id="6-1加载数据"><a href="#6-1加载数据" class="headerlink" title="6.1加载数据"></a>6.1加载数据</h4><pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris(as_frame=True)
X = iris.data[[&quot;sepal length (cm)&quot;, &quot;sepal width (cm)&quot;]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)
</code></pre>
<p>鸢尾花数据集，<code>as_frame=True</code> 表示返回 <strong>pandas DataFrame</strong> 而不是 numpy 数组，方便做列选择。</p>
<p>这个数据集有 <strong>150 条样本</strong>，<strong>4 个特征</strong>：<code>sepal length</code>, <code>sepal width</code>, <code>petal length</code>, <code>petal width</code>。目标变量 <code>target</code> 有三类 (0&#x3D;setosa, 1&#x3D;versicolor, 2&#x3D;virginica)。</p>
<h4 id="6-2加载模型并可视化"><a href="#6-2加载模型并可视化" class="headerlink" title="6.2加载模型并可视化"></a>6.2加载模型并可视化</h4><pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import DecisionBoundaryDisplay
import pandas as pd
import time

# 1. 载入数据
iris = load_iris(as_frame=True)
X = iris.data[[&quot;sepal length (cm)&quot;, &quot;sepal width (cm)&quot;]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, random_state=0
)

# 2. 构建 pipeline：标准化 + KNN
clf = Pipeline(
    steps=[
        (&quot;scaler&quot;, StandardScaler()),
        (&quot;knn&quot;, KNeighborsClassifier(n_neighbors=11))
    ]
)

# 3. 不同的 weights 和 algorithm 组合
weights_list = [&quot;uniform&quot;, &quot;distance&quot;]
algorithms = [&quot;auto&quot;, &quot;ball_tree&quot;, &quot;kd_tree&quot;]


# 定义结果存储表
results = []


# 4. 画图：每行一个 weights，每列一个 algorithm
fig, axs = plt.subplots(
    nrows=len(weights_list), ncols=len(algorithms), figsize=(18, 10)
)

for i, weights in enumerate(weights_list):
    for j, algo in enumerate(algorithms):
        ax = axs[i, j]

        # 设置参数并拟合
        start_train = time.time()
        clf.set_params(knn__weights=weights, knn__algorithm=algo).fit(X_train, y_train)
        end_train = time.time()
        
        start_pred = time.time()
        clf.predict(X_test)
        end_pred = time.time()
        
        acc = clf.score(X_test, y_test)

        results.append(&#123;
            &quot;weights&quot;: weights,
            &quot;algorithm&quot;: algo,
            &quot;accuracy&quot;: acc,
            &quot;train_time (s)&quot;: end_train - start_train,
            &quot;predict_time (s)&quot;: end_pred - start_pred
        &#125;)
        # 决策边界
        disp = DecisionBoundaryDisplay.from_estimator(
            clf,
            X_test,
            response_method=&quot;predict&quot;,
            plot_method=&quot;pcolormesh&quot;,
            xlabel=iris.feature_names[0],
            ylabel=iris.feature_names[1],
            shading=&quot;auto&quot;,
            alpha=0.5,
            ax=ax,
        )

        # 训练样本点
        scatter = disp.ax_.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors=&quot;k&quot;)

        # 图例
        disp.ax_.legend(
            scatter.legend_elements()[0],
            iris.target_names,
            loc=&quot;lower left&quot;,
            title=&quot;Classes&quot;,
        )

        # 子图标题
        ax.set_title(
            f&quot;k=&#123;clf[-1].n_neighbors&#125;, weights=&#123;weights&#125;, algo=&#123;algo&#125;&quot;
        )

plt.tight_layout()
plt.show()
df_results = pd.DataFrame(results)
print(df_results)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509171831492.png" alt="image-20250917183120303"></p>
<pre><code> weights  algorithm  accuracy  train_time (s)  predict_time (s)
0   uniform       auto  0.710526        0.003293          0.004401
1   uniform  ball_tree  0.710526        0.004864          0.006618
2   uniform    kd_tree  0.710526        0.003537          0.004044
3  distance       auto  0.631579        0.003269          0.001961
4  distance  ball_tree  0.631579        0.003211          0.001694
5  distance    kd_tree  0.631579        0.003055          0.001578
</code></pre>
<p><strong>不同 algorithm 的表现</strong></p>
<ul>
<li><code>auto</code>、<code>ball_tree</code>、<code>kd_tree</code> 在相同权重下的 <strong>准确率完全一致，训练预测速度不同</strong>，这说明 <strong>搜索算法仅影响计算效率，不会改变最终分类结果</strong>。</li>
<li>这和理论一致：算法只是用不同的数据结构加速邻居查找，不会影响邻居集合本身。</li>
</ul>
<p><strong>不同 weights 的表现</strong></p>
<ul>
<li><code>uniform</code> 权重下，测试集准确率为 <strong>71.05%</strong>；</li>
<li><code>distance</code> 权重下，测试集准确率为 <strong>63.16%</strong>；</li>
<li>在本实验中，<strong>uniform 明显优于 distance</strong>。</li>
<li>这表明在鸢尾花数据的 <strong>前两个特征（花萼长、宽）</strong> 上，等权投票比加权投票更适合。可能原因是：<ul>
<li>特征维度少，距离加权放大了噪声点或边界点的影响；</li>
<li>类别边界本身不完全线性，用距离权重反而削弱了多数邻居的稳定性。</li>
</ul>
</li>
</ul>
<p><strong>结合可视化</strong></p>
<ul>
<li>从决策边界图上可以看到：<ul>
<li><code>uniform</code> 的边界相对平滑，更符合数据整体分布；</li>
<li><code>distance</code> 在交界区域会出现一些不规则边界，可能导致更多误判。</li>
</ul>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #00a596">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/17/K%E8%BF%91%E9%82%BB%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">支持向量机：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/15
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />

<h1 id="支持向量机：从理论到实践"><a href="#支持向量机：从理论到实践" class="headerlink" title="支持向量机：从理论到实践"></a>支持向量机：从理论到实践</h1><h2 id="一。理论概述"><a href="#一。理论概述" class="headerlink" title="一。理论概述"></a>一。理论概述</h2><p>支持向量机（Support Vector Machine, SVM）是机器学习中最强大和最广泛使用的算法之一，尤其在小样本、高维数据的分类任务中表现出色。其核心思想基于统计学习理论中的结构风险最小化原则，通过构建最大间隔超平面来实现分类任务。本文将从数学基础到实际应用，全面深入地解析SVM的工作原理和实现细节。</p>
<h3 id="1-线性可分支持向量机"><a href="#1-线性可分支持向量机" class="headerlink" title="1. 线性可分支持向量机"></a>1. 线性可分支持向量机</h3><h4 id="1-1-基本概念与数学形式"><a href="#1-1-基本概念与数学形式" class="headerlink" title="1.1 基本概念与数学形式"></a>1.1 基本概念与数学形式</h4><p>对于完全线性可分的数据集，存在无数个超平面能够将两类样本完全分开。SVM通过<strong>间隔最大化</strong>原则从中选择唯一的最优超平面，该超平面不仅能够正确分类所有训练样本，而且具有最好的泛化能力。</p>
<p>超平面的数学表示为：<br>$$w^T x + b &#x3D; 0$$<br>其中$w \in \mathbb{R}^n$是超平面的法向量，决定了超平面的方向；$b \in \mathbb{R}$是位移项，决定了超平面与原点的距离。</p>
<p>样本点$x_i$到超平面的几何距离为：<br>$$d_i &#x3D; \frac{|w^T x_i + b|}{|w|}$$</p>
<h4 id="1-2-函数间隔与几何间隔"><a href="#1-2-函数间隔与几何间隔" class="headerlink" title="1.2 函数间隔与几何间隔"></a>1.2 函数间隔与几何间隔</h4><p><strong>函数间隔</strong>的概念引入了类别信息：<br>$$\hat{\gamma}_i &#x3D; y_i(w^T x_i + b)$$<br>其中$y_i \in {-1, 1}$是样本的类别标签。函数间隔的符号表示分类的正确性，绝对值表示分类的确信度。</p>
<p>整个数据集的函数间隔定义为所有样本中函数间隔的最小值：<br>$$\hat{\gamma} &#x3D; \min_{i&#x3D;1,\dots,N} \hat{\gamma}_i$$</p>
<p>然而，函数间隔存在一个显著问题：对$w$和$b$进行等比例缩放时，超平面不变但函数间隔会改变。为解决这个问题，我们引入<strong>几何间隔</strong>：</p>
<p>$$\gamma_i &#x3D; \frac{\hat{\gamma}_i}{|w|} &#x3D; \frac{y_i(w^T x_i + b)}{|w|}$$</p>
<p>几何间隔表示样本点到超平面的真实欧几里得距离，具有缩放不变性，能够真实反映分类的确信度。</p>
<h4 id="1-3-间隔最大化与优化问题"><a href="#1-3-间隔最大化与优化问题" class="headerlink" title="1.3 间隔最大化与优化问题"></a>1.3 间隔最大化与优化问题</h4><p>SVM的核心目标是找到最大几何间隔的超平面，这可以表述为以下优化问题：</p>
<p>$$\max_{w,b} \gamma$$<br>$$\text{s.t. } \frac{y_i(w^T x_i + b)}{|w|} \geq \gamma, \quad i&#x3D;1,\dots,N$$</p>
<p>通过令$\hat{\gamma} &#x3D; 1$（这可以通过调整$w$和$b$的尺度实现），问题转化为等价的约束优化问题：</p>
<p>$$\min_{w,b} \frac{1}{2} |w|^2$$<br>$$\text{s.t. } y_i(w^T x_i + b) \geq 1, \quad i&#x3D;1,\dots,N$$</p>
<p>这是一个典型的<strong>凸二次规划问题</strong>，具有全局最优解。目标函数中的$\frac{1}{2}$是为了后续求导方便而添加的系数，不影响优化结果。</p>
<h4 id="1-4-拉格朗日对偶理论与求解"><a href="#1-4-拉格朗日对偶理论与求解" class="headerlink" title="1.4 拉格朗日对偶理论与求解"></a>1.4 拉格朗日对偶理论与求解</h4><p>应用拉格朗日乘子法，我们引入拉格朗日乘子$\alpha_i \geq 0$，构造拉格朗日函数：</p>
<p>$$L(w,b,\alpha) &#x3D; \frac{1}{2} |w|^2 - \sum_{i&#x3D;1}^N \alpha_i [y_i(w^T x_i + b) - 1]$$</p>
<p>根据KKT条件，原问题的最优解满足：</p>
<p>$$\nabla_w L &#x3D; w - \sum_{i&#x3D;1}^N \alpha_i y_i x_i &#x3D; 0$$<br>$$\frac{\partial L}{\partial b} &#x3D; -\sum_{i&#x3D;1}^N \alpha_i y_i &#x3D; 0$$<br>$$\alpha_i [y_i(w^T x_i + b) - 1] &#x3D; 0, \quad i&#x3D;1,\dots,N$$</p>
<p>代入拉格朗日函数，得到对偶问题：</p>
<p>$$\max_{\alpha} -\frac{1}{2} \sum_{i&#x3D;1}^N \sum_{j&#x3D;1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i&#x3D;1}^N \alpha_i$$<br>$$\text{s.t. } \alpha_i \geq 0, \quad \sum_{i&#x3D;1}^N \alpha_i y_i &#x3D; 0$$</p>
<h4 id="1-5-支持向量与决策函数"><a href="#1-5-支持向量与决策函数" class="headerlink" title="1.5 支持向量与决策函数"></a>1.5 支持向量与决策函数</h4><p>从KKT条件中的互补松弛条件$\alpha_i [y_i(w^T x_i + b) - 1] &#x3D; 0$可知：</p>
<ul>
<li>当$\alpha_i &#x3D; 0$时，对应样本不是支持向量，对决策边界没有影响</li>
<li>当$\alpha_i &gt; 0$时，必有$y_i(w^T x_i + b) &#x3D; 1$，对应样本是<strong>支持向量</strong></li>
</ul>
<p>支持向量是位于间隔边界上的样本点，它们决定了最终的超平面。这一特性使得SVM的解具有稀疏性，仅依赖于少数支持向量。</p>
<p>最终的决策函数为：<br>$$f(x) &#x3D; \text{sign} \left( \sum_{i&#x3D;1}^N \alpha_i y_i x_i^T x + b \right)$$</p>
<p>其中$b$可以通过任意支持向量计算得到：$b &#x3D; y_i - w^T x_i$（对于满足$0 &lt; \alpha_i$的样本）。</p>
<hr>
<h3 id="2-近似线性可分数据（软间隔SVM）"><a href="#2-近似线性可分数据（软间隔SVM）" class="headerlink" title="2. 近似线性可分数据（软间隔SVM）"></a>2. 近似线性可分数据（软间隔SVM）</h3><h4 id="2-1-松弛变量与软间隔概念"><a href="#2-1-松弛变量与软间隔概念" class="headerlink" title="2.1 松弛变量与软间隔概念"></a>2.1 松弛变量与软间隔概念</h4><p>在实际应用中，数据很少是完美线性可分的，可能存在噪声或异常点。软间隔SVM通过引入<strong>松弛变量</strong>$\xi_i \geq 0$，允许一些样本被错误分类，从而提高模型的鲁棒性。</p>
<p>优化问题变为：<br>$$\min_{w,b,\xi} \frac{1}{2} |w|^2 + C \sum_{i&#x3D;1}^N \xi_i$$<br>$$\text{s.t. } y_i(w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i&#x3D;1,\dots,N$$</p>
<p>其中$C &gt; 0$是正则化参数，控制误分类惩罚与间隔大小之间的平衡：</p>
<ul>
<li>$C$值较大时，误分类惩罚重，间隔较小，可能过拟合</li>
<li>$C$值较小时，误分类惩罚轻，间隔较大，可能欠拟合</li>
</ul>
<h4 id="2-2-软间隔的对偶问题与支持向量分类"><a href="#2-2-软间隔的对偶问题与支持向量分类" class="headerlink" title="2.2 软间隔的对偶问题与支持向量分类"></a>2.2 软间隔的对偶问题与支持向量分类</h4><p>软间隔SVM的对偶问题与硬间隔形式相似：</p>
<p>$$\max_{\alpha} -\frac{1}{2} \sum_{i&#x3D;1}^N \sum_{j&#x3D;1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i&#x3D;1}^N \alpha_i$$<br>$$\text{s.t. } 0 \leq \alpha_i \leq C, \quad \sum_{i&#x3D;1}^N \alpha_i y_i &#x3D; 0$$</p>
<p>KKT条件扩展为：<br>$$\alpha_i [y_i(w^T x_i + b) - 1 + \xi_i] &#x3D; 0$$<br>$$(C - \alpha_i) \xi_i &#x3D; 0$$</p>
<p>支持向量分为三类：</p>
<ol>
<li>$\alpha_i &#x3D; 0$：正确分类的非支持向量，对决策边界没有影响</li>
<li>$0 &lt; \alpha_i &lt; C$：位于间隔边界上的支持向量，满足$y_i(w^T x_i + b) &#x3D; 1$</li>
<li>$\alpha_i &#x3D; C$：被错误分类的支持向量或落在间隔内的样本，满足$\xi_i &gt; 0$</li>
</ol>
<hr>
<h3 id="3-非线性支持向量机与核方法"><a href="#3-非线性支持向量机与核方法" class="headerlink" title="3. 非线性支持向量机与核方法"></a>3. 非线性支持向量机与核方法</h3><h4 id="3-1-核技巧的数学原理"><a href="#3-1-核技巧的数学原理" class="headerlink" title="3.1 核技巧的数学原理"></a>3.1 核技巧的数学原理</h4><p>当数据在原始特征空间中线性不可分时，可以通过非线性映射$\phi: \mathbb{R}^d \to \mathcal{H}$将数据映射到高维特征空间$\mathcal{H}$，在其中数据变得线性可分。</p>
<p>在高维空间中的优化问题变为：<br>$$\min_{w,b} \frac{1}{2} |w|^2 + C \sum_{i&#x3D;1}^N \xi_i$$<br>$$\text{s.t. } y_i(w^T \phi(x_i) + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$</p>
<p>对偶问题为：<br>$$\max_{\alpha} -\frac{1}{2} \sum_{i&#x3D;1}^N \sum_{j&#x3D;1}^N \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j) + \sum_{i&#x3D;1}^N \alpha_i$$</p>
<p>直接计算$\phi(x_i)^T \phi(x_j)$可能非常困难（甚至不可能），因此引入<strong>核函数</strong>：<br>$$K(x_i, x_j) &#x3D; \phi(x_i)^T \phi(x_j)$$</p>
<h4 id="3-2-常用核函数及其特性"><a href="#3-2-常用核函数及其特性" class="headerlink" title="3.2 常用核函数及其特性"></a>3.2 常用核函数及其特性</h4><ol>
<li><p><strong>线性核</strong>：$K(x_i, x_j) &#x3D; x_i^T x_j$</p>
<ul>
<li>参数少，速度快，适用于线性可分情况</li>
<li>简单，可解释性强</li>
</ul>
</li>
<li><p><strong>多项式核</strong>：$K(x_i, x_j) &#x3D; (x_i^T x_j + c)^d$</p>
<ul>
<li>参数$d$控制映射后的空间维度</li>
<li>当$d$较大时计算可能不稳定</li>
</ul>
</li>
<li><p><strong>高斯径向基核（RBF）</strong>：$K(x_i, x_j) &#x3D; \exp(-\gamma |x_i - x_j|^2)$</p>
<ul>
<li>应用最广泛的核函数，具有很强的非线性映射能力</li>
<li>参数$\gamma$控制高斯函数的宽度，影响模型的复杂度</li>
</ul>
</li>
<li><p><strong>Sigmoid核</strong>：$K(x_i, x_j) &#x3D; \tanh(\kappa x_i^T x_j + \theta)$</p>
<ul>
<li>来源于神经网络理论，在某些特定问题上表现良好</li>
<li>不是对所有参数都满足Mercer条件</li>
</ul>
</li>
</ol>
<h4 id="3-3-核函数的选择与模型选择"><a href="#3-3-核函数的选择与模型选择" class="headerlink" title="3.3 核函数的选择与模型选择"></a>3.3 核函数的选择与模型选择</h4><p>核函数的选择依赖于具体问题和数据特性：</p>
<ul>
<li>文本分类：通常使用线性核，因为文本数据往往已经是高维的</li>
<li>图像识别：常用RBF核或多项式核，可以捕捉像素间的复杂关系</li>
<li>生物信息学：RBF核表现优异，适用于基因序列等复杂数据</li>
</ul>
<p>模型选择涉及参数调优，常用交叉验证来确定最优的$C$和核参数（如RBF核的$\gamma$）。</p>
<h4 id="3-4-Mercer定理与核函数有效性"><a href="#3-4-Mercer定理与核函数有效性" class="headerlink" title="3.4 Mercer定理与核函数有效性"></a>3.4 Mercer定理与核函数有效性</h4><p>核函数必须满足Mercer条件：对任意函数$g(x)$满足$\int g(x)^2 dx &lt; \infty$，有：<br>$$\iint K(x, y) g(x) g(y) dx dy \geq 0$$</p>
<p>这保证了核矩阵$K &#x3D; [K(x_i, x_j)]$是半正定的，对应的优化问题是凸的，有全局最优解。</p>
<hr>
<h3 id="4-支持向量机的扩展与变体"><a href="#4-支持向量机的扩展与变体" class="headerlink" title="4. 支持向量机的扩展与变体"></a>4. 支持向量机的扩展与变体</h3><h4 id="4-1-支持向量回归（SVR）"><a href="#4-1-支持向量回归（SVR）" class="headerlink" title="4.1 支持向量回归（SVR）"></a>4.1 支持向量回归（SVR）</h4><p>支持向量机也可以用于回归任务，称为支持向量回归。其基本思想是：寻找一个函数$f(x) &#x3D; w^T \phi(x) + b$，使得$f(x)$与$y$的偏差不超过$\epsilon$，同时保持函数尽量平坦。</p>
<p>优化问题为：<br>$$\min_{w,b} \frac{1}{2} |w|^2 + C \sum_{i&#x3D;1}^N (\xi_i + \xi_i^*)$$</p>
<p>$$\text{s.t. } \begin{cases}<br>y_i - w^T \phi(x_i) - b \leq \epsilon + \xi_i \<br>w^T \phi(x_i) + b - y_i \leq \epsilon + \xi_i^* \<br>\xi_i, \xi_i^* \geq 0<br>\end{cases}$$</p>
<h4 id="4-2-多类支持向量机"><a href="#4-2-多类支持向量机" class="headerlink" title="4.2 多类支持向量机"></a>4.2 多类支持向量机</h4><p>SVM本质上是二分类器，扩展多类分类的方法主要有：</p>
<ol>
<li><strong>一对多（One-vs-Rest）</strong>：为每个类别训练一个二分类器</li>
<li><strong>一对一（One-vs-One）</strong>：为每两个类别训练一个二分类器</li>
<li><strong>多类SVM</strong>：直接修改优化目标，考虑所有类别</li>
</ol>
<h4 id="4-3-结构化SVM"><a href="#4-3-结构化SVM" class="headerlink" title="4.3 结构化SVM"></a>4.3 结构化SVM</h4><p>用于结构化输出问题，如序列标注、解析树构建等，通过定义适当的损失函数和特征映射来处理复杂的输出结构。</p>
<hr>
<h3 id="5-支持向量机的实践考虑"><a href="#5-支持向量机的实践考虑" class="headerlink" title="5. 支持向量机的实践考虑"></a>5. 支持向量机的实践考虑</h3><h4 id="5-1-数据预处理与特征缩放"><a href="#5-1-数据预处理与特征缩放" class="headerlink" title="5.1 数据预处理与特征缩放"></a>5.1 数据预处理与特征缩放</h4><p>SVM对数据缩放敏感，特别是使用基于距离的核函数（如RBF核）时。常见的预处理方法：</p>
<ul>
<li>标准化：将特征缩放到均值为0，方差为1</li>
<li>归一化：将特征缩放到[0,1]或[-1,1]区间</li>
</ul>
<h4 id="5-2-计算效率与大规模数据处理"><a href="#5-2-计算效率与大规模数据处理" class="headerlink" title="5.2 计算效率与大规模数据处理"></a>5.2 计算效率与大规模数据处理</h4><p>传统SVM训练算法的时间复杂度约为$O(n^3)$，空间复杂度约为$O(n^2)$，难以处理大规模数据集。改进方法包括：</p>
<ul>
<li>分解算法（如SMO）</li>
<li>随机梯度下降</li>
<li>近似核方法</li>
<li>分布式计算</li>
</ul>
<h4 id="5-3-模型解释性与特征重要性"><a href="#5-3-模型解释性与特征重要性" class="headerlink" title="5.3 模型解释性与特征重要性"></a>5.3 模型解释性与特征重要性</h4><p>虽然核SVM具有良好的分类性能，但模型解释性较差。提高解释性的方法：</p>
<ul>
<li>使用线性核或可解释性强的核函数</li>
<li>分析支持向量的权重</li>
<li>使用模型无关的解释方法（如LIME、SHAP）</li>
</ul>
<h3 id="6-支持向量机的局限性与挑战"><a href="#6-支持向量机的局限性与挑战" class="headerlink" title="6. 支持向量机的局限性与挑战"></a>6. 支持向量机的局限性与挑战</h3><ul>
<li><strong>计算复杂度</strong>：对于大规模数据集，训练时间较长，内存消耗大，限制了其在实际中的应用。</li>
<li><strong>核函数选择</strong>：核函数及其参数的选择很大程度上依赖于经验和实验，缺乏系统的理论指导。</li>
<li><strong>概率输出</strong>：标准SVM不直接提供概率输出，需要通过 Platt scaling 等额外方法进行校准。</li>
<li><strong>多类分类</strong>：SVM本质上是二分类器，多类扩展需要额外的策略，增加了复杂性。</li>
</ul>
<hr>
<h2 id="二。SVM模型应用于人脸分类"><a href="#二。SVM模型应用于人脸分类" class="headerlink" title="二。SVM模型应用于人脸分类"></a>二。SVM模型应用于人脸分类</h2><h3 id="1-数据加载与预处理"><a href="#1-数据加载与预处理" class="headerlink" title="1. 数据加载与预处理"></a>1. 数据加载与预处理</h3><pre><code class="language-python">from time import time

import matplotlib.pyplot as plt
from scipy.stats import loguniform

from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from sklearn.metrics import ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
</code></pre>
<pre><code class="language-python">people_faces = fetch_lfw_peopletch()
# 从 LFW（Labeled Faces in the Wild）数据集中下载人脸图片
# min_faces_per_person=70 → 只保留至少有 70 张照片的人物
# resize=0.4 → 将图片缩小到原来的 40%，降低计算量
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# 获取数据集的基本形状信息
n_samples, h, w = lfw_people.images.shape  # 样本数、高度、宽度

# 机器学习使用拉平后的像素数据（忽略像素的空间位置信息）
X = lfw_people.data  # 每张图片展平成一维向量
n_features = X.shape[1]  # 特征数（像素个数）

# 标签（人物 ID）
y = lfw_people.target
target_names = lfw_people.target_names  # 人物姓名
n_classes = target_names.shape[0]  # 类别数

# 打印数据集大小信息
print(&quot;Total dataset size:&quot;)
print(&quot;n_samples: %d&quot; % n_samples)
print(&quot;n_features: %d&quot; % n_features)
print(&quot;n_classes: %d&quot; % n_classes)

结果：
Total dataset size:
n_samples: 1288
n_features: 1850
n_classes: 7
</code></pre>
<p>从LFW（Labeled Faces in the Wild）数据集中加载人脸图像数据，并进行初步预处理。通过设置<code>min_faces_per_person=70</code>筛选出至少有70张图像的人物，确保每个类别有足够的样本；<code>resize=0.4</code>将图像缩小到原尺寸的40%，显著降低计算复杂度。随后，将图像数据展平为一维向量作为特征，提取对应的类别标签和人物名称，输出数据集的基本统计信息（样本数、特征维度和类别数）</p>
<h3 id="2-训练测试划分与数据标准化"><a href="#2-训练测试划分与数据标准化" class="headerlink" title="2. 训练测试划分与数据标准化"></a>2. 训练测试划分与数据标准化</h3><pre><code class="language-python"># 划分训练集和测试集，比例 70% 训练 / 30% 测试
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=2025
)

# 数据标准化（均值为 0，方差为 1），有助于加快收敛并提升模型性能
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # 用训练集拟合并转换
X_test = scaler.transform(X_test)        # 用相同参数转换测试集
</code></pre>
<p>将数据集按7:3比例划分为训练集和测试集。采用<code>StandardScaler</code>对数据进行标准化处理，使每个特征的均值为0、方差为1。这一步骤至关重要，因为SVM对特征尺度敏感，标准化可以加速模型收敛并提升性能。注意测试集使用训练集拟合的缩放参数进行转换，避免数据泄露。</p>
<h3 id="3-主成分分析（PCA）降维"><a href="#3-主成分分析（PCA）降维" class="headerlink" title="3. 主成分分析（PCA）降维"></a>3. 主成分分析（PCA）降维</h3><pre><code class="language-python"># 设定 PCA 要保留的主成分个数，这里是 100 个
n_components = 100

# 记录开始时间，用于计算运行耗时
start = time()

# 创建 PCA 对象并拟合训练数据
# 参数解释：
#   n_components=n_components  -&gt; 保留的主成分数量
#   svd_solver=&quot;randomized&quot;    -&gt; 使用随机化 SVD 算法，加速计算（适合高维数据）
#   whiten=True                -&gt; 白化处理，使得每个主成分的方差为 1（有助于后续分类器性能）
# .fit(X_train) -&gt; 在训练集上拟合 PCA 模型，学习主成分方向
pca = PCA(n_components=n_components, svd_solver=&quot;randomized&quot;, whiten=True).fit(X_train)

# 获取 PCA 学到的主成分（特征向量），并将它们还原成 h×w 的二维图像
# 这些主成分在脸部识别领域被称为 &quot;eigenfaces&quot;（特征脸）
eigenfaces = pca.components_.reshape((n_components, h, w))

# 记录结束时间
end = time()

# 使用训练好的 PCA 模型将训练集投影到主成分空间
# 得到的 X_train_pca 是降维后的特征表示
X_train_pca = pca.transform(X_train)

# 同样地，将测试集投影到主成分空间
X_test_pca = pca.transform(X_test)

# 输出整个 PCA 特征提取过程的耗时
print(&quot;finished in %0.3fs&quot; % (end - start))
</code></pre>
<p>使用PCA对高维图像特征进行降维，保留100个主要成分。设置<code>whiten=True</code>对主成分进行白化处理，使各维度方差归一化，有助于提升后续分类器性能。采用随机化SVD算法（<code>svd_solver=&quot;randomized&quot;</code>）提高计算效率。降维后将训练集和测试集分别投影到主成分空间，得到低维特征表示（<code>X_train_pca</code>和<code>X_test_pca</code>），同时提取特征脸（eigenfaces）用于可视化。</p>
<h3 id="4-支持向量机超参数优化"><a href="#4-支持向量机超参数优化" class="headerlink" title="4. 支持向量机超参数优化"></a>4. 支持向量机超参数优化</h3><pre><code class="language-python"># 记录开始时间
start = time()

# 定义超参数搜索范围 param_grid
# 这里使用的是 loguniform 分布（对数均匀分布），适合搜索跨度很大的超参数
#  - &quot;C&quot; 是 SVM 的正则化参数，控制分类器对训练集的拟合程度
#  - &quot;gamma&quot; 是 RBF 核函数的核宽度参数，影响单个样本的影响范围
#    范围：
#      C: 从 1e3 到 1e5
#      gamma: 从 1e-4 到 1e-1
param_grid = &#123;
    &quot;C&quot;: loguniform(1e3, 1e5),
    &quot;gamma&quot;: loguniform(1e-4, 1e-1),
&#125;

# 创建一个随机搜索交叉验证器 RandomizedSearchCV
#  - SVC(kernel=&quot;rbf&quot;, class_weight=&quot;balanced&quot;) ：使用 RBF 核的支持向量机，类别权重平衡处理
#  - param_distributions=param_grid ：指定要搜索的参数分布
#  - n_iter=10 ：随机采样 10 组参数组合进行测试
clf = RandomizedSearchCV(
    SVC(kernel=&quot;rbf&quot;, class_weight=&quot;balanced&quot;), param_grid, n_iter=10
)

# 在训练集（PCA 降维后的特征）上拟合模型并进行超参数搜索
#  X_train_pca：降维后的训练数据
#  y_train：对应的标签
clf = clf.fit(X_train_pca, y_train)

# 输出搜索过程耗时
print(&quot;finished in %0.3f&quot; % (time() - start))

# 输出搜索得到的最优模型（包含最佳参数 C 和 gamma）
print(clf.best_estimator_)
</code></pre>
<p>使用随机搜索（<code>RandomizedSearchCV</code>）优化SVM的超参数（正则化参数<code>C</code>和RBF核参数<code>gamma</code>）。参数搜索范围设置为对数均匀分布（<code>loguniform</code>），涵盖合理的数值区间。采用带类别权重平衡（<code>class_weight=&quot;balanced&quot;</code>）的RBF核SVM，以处理类别样本量不均衡的问题。通过10次参数采样和交叉验证，寻找最优超参数组合。</p>
<h3 id="5-模型预测与评估"><a href="#5-模型预测与评估" class="headerlink" title="5. 模型预测与评估"></a>5. 模型预测与评估</h3><pre><code class="language-python"># 记录开始时间
start = time()

# 使用训练好的分类器（clf）对测试集特征进行预测
#  X_test_pca 是经过 PCA 降维的测试集数据
#  y_pred 是预测得到的标签
y_pred = clf.predict(X_test_pca)

# 输出预测过程耗时
print(&quot;finished in %0.3fs&quot; % (time() - start))

# 打印分类性能评估报告
# classification_report 会输出：
#   - precision（精确率）
#   - recall（召回率）
#   - f1-score（F1 分数）
#   - support（每个类别的样本数量）
# target_names 用于显示每个类别的真实名称（这里是人物姓名）
print(classification_report(y_test, y_pred, target_names=target_names))

# 绘制混淆矩阵（Confusion Matrix）
# ConfusionMatrixDisplay.from_estimator 会：
#   - 用分类器 clf
#   - 输入测试集数据和真实标签
#   - 自动计算混淆矩阵并绘制
# display_labels=target_names 用于显示真实的类别名称
# xticks_rotation=&quot;vertical&quot; 让横轴标签竖直显示，防止文字重叠
ConfusionMatrixDisplay.from_estimator(
    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation=&quot;vertical&quot;
)

# 自动调整子图布局，避免文字或图像被遮挡
plt.tight_layout()

# 显示绘制好的混淆矩阵图
plt.show()
</code></pre>
<p>使用优化后的SVM模型对测试集进行预测，并输出详细分类报告（包括精确率、召回率、F1分数等指标）和混淆矩阵。混淆矩阵可视化展示了各类别的分类情况。</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509140912767.png" alt="image-20250914091238591"></p>
<h3 id="6-预测结果标题生成与可视化"><a href="#6-预测结果标题生成与可视化" class="headerlink" title="6. 预测结果标题生成与可视化"></a>6. 预测结果标题生成与可视化</h3><pre><code class="language-python">import matplotlib.pyplot as plt

def plot_gallery(images, titles, h, w, n_row=4, n_col=4):
    &quot;&quot;&quot;绘制图像网格（gallery）&quot;&quot;&quot;
    fig, axes = plt.subplots(n_row, n_col, figsize=(1.8 * n_col, 2.4 * n_row))  # 创建子图
    axes = axes.flatten()  # 展平成一维，方便索引
    for i in range(n_row * n_col):
        axes[i].imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)  # 显示灰度图
        axes[i].set_title(titles[i], size=12)  # 设置标题
        axes[i].set_xticks([])  # 去掉横坐标刻度
        axes[i].set_yticks([])  # 去掉纵坐标刻度
    plt.tight_layout()  # 自动调整布局
    plt.show()

</code></pre>
<p>定义<code>plot_gallery</code>函数，用于以网格形式展示多张图像。函数接受图像数据、标题列表和图像尺寸参数，自动创建子图布局，显示灰度图像并设置标题。</p>
<pre><code class="language-python">def title(y_pred, y_test, target_names, i):
    &quot;&quot;&quot;生成第 i 张图片的预测与真实标签标题&quot;&quot;&quot;
    pred_name = target_names[y_pred[i]].rsplit(&quot; &quot;, 1)[-1]  # 预测姓名（取姓氏）
    true_name = target_names[y_test[i]].rsplit(&quot; &quot;, 1)[-1]  # 真实姓名（取姓氏）
    return &quot;predicted: %s\ntrue:      %s&quot; % (pred_name, true_name)

# 生成预测结果标题列表
prediction_titles = [
    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])
]

# 绘制测试集图片及预测结果
plot_gallery(X_test, prediction_titles, h, w)
</code></pre>
<p>根据预测结果和真实标签生成每张测试图像的标题，格式为“预测姓名&#x2F;真实姓名”。仅使用人物姓氏（通过<code>rsplit</code>提取）简化显示。</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509140913106.png" alt="image-20250914091357886"></p>
<h3 id="7-特征脸可视化"><a href="#7-特征脸可视化" class="headerlink" title="7. 特征脸可视化"></a>7. 特征脸可视化</h3><pre><code class="language-python"># 生成特征脸（Eigenfaces）标题
eigenface_titles = [&quot;eigenface %d&quot; % i for i in range(eigenfaces.shape[0])]

# 绘制特征脸
plot_gallery(eigenfaces, eigenface_titles, h, w)

plt.show()
</code></pre>
<p>调用<code>plot_gallery</code>函数可视化PCA提取的前100个特征脸。特征脸反映了人脸图像的主要变化模式，是PCA降维的基础。</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509140914579.png" alt="image-20250914091444461"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #03a9f4">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/15/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">聚类方法：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/15
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />


<h1 id="聚类方法：从理论到实践"><a href="#聚类方法：从理论到实践" class="headerlink" title="聚类方法：从理论到实践"></a>聚类方法：从理论到实践</h1><h3 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h3><p>在机器学习和数据挖掘中，衡量样本（或特征）之间的<strong>相似度 (Similarity)</strong> 和 <strong>距离 (Distance)</strong> 是许多算法（如K近邻、聚类、降维）的基础。相似度越大，样本越相似；距离越大，样本差异越大。</p>
<h4 id="（1）相似度与距离"><a href="#（1）相似度与距离" class="headerlink" title="（1）相似度与距离"></a>（1）相似度与距离</h4><p>假设我们有 $n$ 个样本，每个样本有 $p$ 个属性（特征）。数据矩阵 $X$ 可以表示为：<br>$$<br>X &#x3D; \begin{bmatrix}<br>x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \<br>x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}<br>\end{bmatrix}<br>$$<br>其中，$x_{ij}$ 表示<strong>第 $i$ 个样本的第 $j$ 个属性</strong>的值。 在机器学习中通常用行表示样本。为了与后续概念统一，我们这里采用<strong>样本-属性</strong>的视角。</p>
<p>两个 $p$ 维样本点 $\mathbf{x}<em>i$ 和 $\mathbf{x}<em>j$ 可以表示为：<br>$$<br>\mathbf{x}<em>i &#x3D; (x</em>{i1}, x</em>{i2}, …, x</em>{ip})^T, \quad \mathbf{x}<em>j &#x3D; (x</em>{j1}, x_{j2}, …, x_{jp})^T<br>$$</p>
<h5 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a><strong>闵可夫斯基距离</strong></h5><p>闵可夫斯基距离是下述一系列距离度量的通用形式。</p>
<p>$$d_{ij} &#x3D; \left( \sum_{k&#x3D;1}^{p} |x_{ik} - x_{jk}|^h \right)^{\frac{1}{h}}$$</p>
<p>其中，$h$ 是一个参数。</p>
<ul>
<li><p><strong>曼哈顿距离</strong><br>当 $h&#x3D;1$ 时，得到曼哈顿距离，也称为“城市街区距离”。<br>$$d_{ij} &#x3D; \sum_{k&#x3D;1}^{p} |x_{ik} - x_{jk}|$$<br><strong>例子</strong>：想象在城市网格状道路中从A点到B点，只能沿着街道走。它的值是各维度差值绝对值的总和。它对数据中的异常值不如欧氏距离敏感。<br><strong>例子</strong>：点 $\mathbf{x}_i &#x3D; (1, 2)$，点 $\mathbf{x}<em>j &#x3D; (4, 6)$。<br>$d</em>{ij} &#x3D; |1-4| + |2-6| &#x3D; 3 + 4 &#x3D; 7$。</p>
</li>
<li><p>**欧氏距离 **<br>当 $h&#x3D;2$ 时，得到最常用的欧氏距离，即直线距离。<br>$$d_{ij} &#x3D; \left( \sum_{k&#x3D;1}^{p} (x_{ik} - x_{jk})^2 \right)^{\frac{1}{2}}$$<br><strong>例子</strong>：这是最直观的距离度量方式。它对数据中的异常值比较敏感。<br><strong>例子</strong>：点 $\mathbf{x}_i &#x3D; (1, 2)$，点 $\mathbf{x}<em>j &#x3D; (4, 6)$。<br>$d</em>{ij} &#x3D; \sqrt{(1-4)^2 + (2-6)^2} &#x3D; \sqrt{9 + 16} &#x3D; \sqrt{25} &#x3D; 5$。</p>
</li>
<li><p><strong>切比雪夫距离</strong><br>当 $h \to \infty$ 时，得到切比雪夫距离，它是所有维度上差值绝对值的最大值。<br>$$d_{ij} &#x3D; \max_k |x_{ik} - x_{jk}|$$<br><strong>例子</strong>：常用于国际象棋中，国王从一个格子移动到另一个格子所需的最少步数就是切比雪夫距离。<br><strong>例子</strong>：点 $\mathbf{x}_i &#x3D; (1, 2)$，点 $\mathbf{x}<em>j &#x3D; (4, 6)$。<br>$d</em>{ij} &#x3D; \max(|1-4|, |2-6|) &#x3D; \max(3, 4) &#x3D; 4$。</p>
</li>
</ul>
<h5 id="马氏距离"><a href="#马氏距离" class="headerlink" title="**马氏距离 **"></a>**马氏距离 **</h5><p>马氏距离考虑了数据特征之间的相关性，并且与数据的尺度无关（scale-invariant）。<br>$$d_{ij} &#x3D; \sqrt{(\mathbf{x}_i - \mathbf{x}_j)^T \mathbf{S}^{-1} (\mathbf{x}_i - \mathbf{x}_j)}$$<br>其中，$\mathbf{S}$ 是数据集的<strong>协方差矩阵</strong>（定义见后文）。<br><strong>介绍</strong>：欧氏距离假设数据的各个维度不相关且尺度相同。当这个假设不成立时（例如，一个维度的单位是米，另一个是吨），欧氏距离就不合理。马氏距离通过引入协方差矩阵的逆 $\mathbf{S}^{-1}$ 来“修正”这个问题，它相当于在计算距离前，先对数据做了一个线性变换，使其变为各维度不相关且方差为1的标准形式，然后再计算欧氏距离。</p>
<h5 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a><strong>相关系数</strong></h5><p>相关系数衡量的是两个<strong>向量</strong>（比如两个样本或两个特征）之间的<strong>线性相关程度</strong>，属于相似度度量。值域为 $[-1, 1]$。</p>
<ul>
<li><strong>样本相关系数 (Pearson Correlation Coefficient)</strong>：<br>对于两个 $p$ 维样本向量 $\mathbf{x}<em>i$ 和 $\mathbf{x}<em>j$，其相关系数计算为：<br>$$\rho</em>{ij} &#x3D; \frac{\sum</em>{k&#x3D;1}^{p} (x_{ik} - \bar{x}<em>i)(x</em>{jk} - \bar{x}<em>j)}{\sqrt{\sum</em>{k&#x3D;1}^{p} (x_{ik} - \bar{x}<em>i)^2} \sqrt{\sum</em>{k&#x3D;1}^{p} (x_{jk} - \bar{x}<em>j)^2}}$$<br>其中 $\bar{x}<em>i &#x3D; \frac{1}{p}\sum</em>{k&#x3D;1}^p x</em>{ik}$ 是样本 $i$ 所有属性值的均值。<br><strong>介绍</strong>：$\rho_{ij} &#x3D; 1$ 表示完全正相关，$\rho_{ij} &#x3D; -1$ 表示完全负相关，$\rho_{ij} &#x3D; 0$ 表示无线性相关。</li>
</ul>
<h4 id="（2）类或簇"><a href="#（2）类或簇" class="headerlink" title="（2）类或簇"></a>（2）类或簇</h4><p>在聚类分析中，一组相似的样本被归为一个“簇”或“类”。我们需要一些指标来描述一个类的特性。</p>
<p>假设一个类 $G$ 包含 $n$ 个 $p$ 维样本：$\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_n \in G$。</p>
<ul>
<li><p><strong>类中心 (Centroid)</strong><br>类中心是类中所有样本的均值向量，代表了类的平均位置。<br>$$\bar{\mathbf{x}}<em>G &#x3D; \frac{1}{n_G} \sum</em>{i&#x3D;1}^{n_G} \mathbf{x}_i$$</p>
</li>
<li><p><strong>类直径 (Diameter) $D_G$</strong><br>类直径衡量类内所有样本间的差异程度，即类的“大小”。<br>$$D_G &#x3D; \sum_{\mathbf{x}<em>i \in G} \sum</em>{\mathbf{x}_j \in G} ||\mathbf{x}_i - \mathbf{x}_j||^2$$<br>一个更常用的、与类直径相关的概念是<strong>类内方差</strong>。</p>
</li>
<li><p><strong>类的散布矩阵 $\mathbf{S}_G$ &amp; 协方差矩阵  $\mathbf{\Sigma}_G$</strong><br>这两个矩阵都描述了类内样本围绕类中心的分散情况，以及属性之间的关系。</p>
<ul>
<li><p><strong>散布矩阵</strong>：<br>$$\mathbf{S}<em>G &#x3D; \sum</em>{i&#x3D;1}^{n_G} (\mathbf{x}_i - \bar{\mathbf{x}}_G)(\mathbf{x}<em>i - \bar{\mathbf{x}}<em>G)^T$$<br>这是一个 $p \times p$ 的矩阵。其对角线元素 $s</em>{kk}$ 是第 $k$ 个属性的<strong>方差</strong>的 $(n_G-1)$ 倍，非对角线元素 $s</em>{kl}$ 是属性 $k$ 和 $l$ 的<strong>协方差</strong>的 $(n_G-1)$ 倍。</p>
</li>
<li><p><strong>协方差矩阵</strong>：<br>协方差矩阵是标准化后的散布矩阵。<br>$$\mathbf{\Sigma}_G &#x3D; \frac{1}{n_G - 1} \mathbf{S}<em>G &#x3D; \frac{1}{n_G - 1} \sum</em>{i&#x3D;1}^{n_G} (\mathbf{x}_i - \bar{\mathbf{x}}_G)(\mathbf{x}_i - \bar{\mathbf{x}}<em>G)^T$$<br><strong>介绍</strong>：协方差矩阵 $\mathbf{\Sigma}<em>G$ 的对角线元素 $\sigma</em>{kk}$ 是第 $k$ 个属性的<strong>方差</strong>，衡量该属性值的分散程度。非对角线元素 $\sigma</em>{kl}$ 是属性 $k$ 和 $l$ 的<strong>协方差</strong>，衡量两个属性之间的线性相关性。马氏距离中使用的正是整个数据集的协方差矩阵 $\mathbf{S}$（或 $\mathbf{\Sigma}$）。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-层次聚类"><a href="#2-层次聚类" class="headerlink" title="2. 层次聚类"></a>2. 层次聚类</h3><p>层次聚类旨在构建一个层次的嵌套聚类树，包括**聚合（自底向上）<strong>和</strong>分裂（自底向下）**两种策略。其中最常用的是聚合聚类。</p>
<h4 id="聚合聚类"><a href="#聚合聚类" class="headerlink" title="聚合聚类"></a>聚合聚类</h4><p><strong>基本流程</strong>：开始时将每个样本各自分到一个簇，然后按照某种准则逐步合并最相似的簇，直到所有样本归于一个簇或达到某个停止条件。</p>
<p>该方法的核心是三个要素：</p>
<ol>
<li><strong>距离或相似度</strong>：用于衡量样本间的差异。常用欧氏距离、曼哈顿距离、余弦相似度等。</li>
<li><strong>合并规则</strong>：用于衡量<strong>簇与簇之间</strong>的距离，决定哪两个簇将被合并。常见规则有：<ul>
<li><strong>单链接 ：两簇中</strong>最近**样本间的距离。<br>$d(C_i, C_j) &#x3D; \min_{\mathbf{x} \in C_i, \mathbf{y} \in C_j} ||\mathbf{x} - \mathbf{y}||$</li>
<li><strong>全链接：两簇中</strong>最远**样本间的距离。<br>$d(C_i, C_j) &#x3D; \max_{\mathbf{x} \in C_i, \mathbf{y} \in C_j} ||\mathbf{x} - \mathbf{y}||$</li>
<li><strong>均链接：两簇中所有样本对间距离的</strong>平均值**。<br>$d(C_i, C_j) &#x3D; \frac{1}{|C_i||C_j|} \sum_{\mathbf{x} \in C_i} \sum_{\mathbf{y} \in C_j} ||\mathbf{x} - \mathbf{y}||$</li>
<li><strong>质心链接 ：两簇</strong>质心**间的距离。<br>$d(C_i, C_j) &#x3D; ||\bar{\mathbf{x}}<em>{C_i} - \bar{\mathbf{x}}</em>{C_j}||$</li>
</ul>
</li>
<li>**停止条件 ：通常是指定最终需要的簇数目 $K$，或指定一个距离阈值，当最近的两个簇距离超过该阈值时停止合并。</li>
</ol>
<hr>
<h3 id="3-K均值聚类"><a href="#3-K均值聚类" class="headerlink" title="3. K均值聚类"></a>3. K均值聚类</h3><p>K均值是一种非常经典、高效的<strong>划分式（Partitional）</strong> 聚类算法。其核心思想是：<strong>以样本与簇质心间的距离为依据，通过迭代优化，使簇内样本尽可能相似（距离小），簇间样本尽可能不相似。</strong></p>
<h4 id="（1）理论算法"><a href="#（1）理论算法" class="headerlink" title="（1）理论算法"></a>（1）理论算法</h4><p>从理论上看，K均值算法旨在最小化<strong>簇内平方和 (Within-Cluster Sum of Squares, WCSS)</strong>，也称为<strong>失真</strong>。<br>$$J &#x3D; \sum_{i&#x3D;1}^{K} \sum_{\mathbf{x} \in C_i} ||\mathbf{x} - \boldsymbol{\mu}_i||^2$$<br>其中：</p>
<ul>
<li>$K$ 是预先指定的簇的个数。</li>
<li>$C_i$ 表示第 $i$ 个簇。</li>
<li>$\boldsymbol{\mu}_i$ 是第 $i$ 个簇的质心（中心点）。</li>
<li>$||\mathbf{x} - \boldsymbol{\mu}_i||^2$ 是样本 $\mathbf{x}$ 到其所属簇质心的欧氏距离的平方。</li>
</ul>
<p><strong>目标</strong>：找到簇划分 $C &#x3D; {C_1, C_2, …, C_K}$ 和质心 ${\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, …, \boldsymbol{\mu}_K}$，使得目标函数 $J$ 最小化。</p>
<h4 id="（2）实际步骤"><a href="#（2）实际步骤" class="headerlink" title="（2）实际步骤"></a>（2）实际步骤</h4><p>由于最小化 $J$ 是一个NP难问题，K均值采用一种<strong>贪心迭代</strong>策略来寻找近似最优解。其步骤如下：</p>
<ol>
<li><p>**初始化：</p>
<ul>
<li>随机选择 $K$ 个样本点作为<strong>初始簇质心</strong> ${\boldsymbol{\mu}_1^{(0)}, \boldsymbol{\mu}_2^{(0)}, …, \boldsymbol{\mu}_K^{(0)}}$。（上标 $(0)$ 表示第0次迭代）。</li>
<li>这是关键一步，不同的初始值可能导致不同的聚类结果。</li>
</ul>
</li>
<li><p>**分配步骤：</p>
<ul>
<li>对于数据集中的<strong>每一个样本 $\mathbf{x}_n$</strong>：<ul>
<li>计算它到当前 $K$ 个质心中<strong>每一个</strong>的距离（通常为欧氏距离）。</li>
<li>将其<strong>分配给距离最近的质心所对应的簇</strong>。</li>
</ul>
</li>
<li>数学表述：$C_i^{(t)} &#x3D; { \mathbf{x}_n : ||\mathbf{x}_n - \boldsymbol{\mu}_i^{(t)}||^2 \le ||\mathbf{x}_n - \boldsymbol{\mu}_j^{(t)}||^2 ; \forall j, 1 \le j \le K }$</li>
<li>（上标 $(t)$ 表示第 $t$ 次迭代）</li>
</ul>
</li>
<li><p>**更新步骤 ：</p>
<ul>
<li>对于<strong>每一个簇 $C_i^{(t)}$</strong>：<ul>
<li>重新计算该簇的质心。新的质心是该簇所有样本的<strong>均值向量</strong>。</li>
</ul>
</li>
<li>数学表述：$\boldsymbol{\mu}<em>i^{(t+1)} &#x3D; \frac{1}{|C_i^{(t)}|} \sum</em>{\mathbf{x} \in C_i^{(t)}} \mathbf{x}$</li>
</ul>
</li>
<li><p>**重复步骤：</p>
<ul>
<li>重复执行<strong>步骤2（分配）</strong> 和<strong>步骤3（更新）</strong>。</li>
<li><strong>停止条件</strong>：当质心的位置不再发生变化（即 $\boldsymbol{\mu}_i^{(t+1)} &#x3D; \boldsymbol{\mu}_i^{(t)}$ 对所有 $i$ 成立），或者变化很小，或者达到最大迭代次数时，算法停止。</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>举例说明</strong>：<br>假设有样本点 <code>[1, 2], [1, 4], [2, 1], [3, 5], [4, 2], [5, 4]</code>，设定 $K&#x3D;2$。</p>
<ol>
<li><strong>初始化</strong>：随机选 <code>[1, 2]</code> 和 <code>[3, 5]</code> 作为初始质心。</li>
<li><strong>分配</strong>：计算所有点到这两个质心的距离，将其分到更近的簇。<ul>
<li>点 <code>[1,4]</code> 到 <code>[1,2]</code> 的距离是 <code>2</code>，到 <code>[3,5]</code> 的距离是 <code>√(4+1)≈2.2</code>，因此被分到第一个簇。</li>
<li>同理，<code>[2,1]</code> 分到第一个簇，<code>[5,4]</code> 分到第二个簇，<code>[4,2]</code> 需要计算。</li>
</ul>
</li>
<li><strong>更新</strong>：重新计算两个簇的均值。<ul>
<li>簇1：<code>[1,2], [1,4], [2,1]</code> -&gt; 新质心 <code>[(1+1+2)/3, (2+4+1)/3] = [1.33, 2.33]</code></li>
<li>簇2：<code>[3,5], [5,4], [4,2]</code> -&gt; 新质心 <code>[(3+5+4)/3, (5+4+2)/3] = [4, 3.67]</code></li>
</ul>
</li>
<li><strong>重复</strong>：用新的质心 <code>[1.33, 2.33]</code> 和 <code>[4, 3.67]</code> 再次执行分配和更新步骤。直到质心稳定不变。</li>
</ol>
</blockquote>
<h4 id="（3）算法特性"><a href="#（3）算法特性" class="headerlink" title="（3）算法特性"></a>（3）算法特性</h4><ul>
<li><p><strong>收敛性</strong>：<br>K均值算法<strong>必定收敛</strong>。因为在每次迭代中：</p>
<ol>
<li>分配步骤通过将样本分配给最近质心来<strong>减小</strong> $J$。</li>
<li>更新步骤通过计算均值来找到当前簇划分下最小化 $J$ 的最优质心，同样<strong>减小</strong> $J$。<br>由于目标函数 $J$ 有下界（且迭代过程使其不断减小），因此算法最终会收敛到一个局部最优解。<strong>注意：它不能保证收敛到全局最优解。</strong></li>
</ol>
</li>
<li><p><strong>初始类的选择</strong>：<br>由于对初始质心敏感，不同的初始化可能导致不同的局部最优解。常用改进方法是<strong>K-means++</strong> 初始化策略，其核心思想是：让初始质心彼此尽可能远离。基本步骤是：</p>
<ol>
<li>随机选择第一个质心。</li>
<li>对于每一个样本，计算其与已选质心的最短距离 $D(\mathbf{x})$。</li>
<li>以概率 $\frac{D(\mathbf{x})^2}{\sum_{\mathbf{x}} D(\mathbf{x})^2}$ 选择一个距离已选质心较远的点作为新质心。</li>
<li>重复步骤2、3，直到选出 $K$ 个质心。<br><em>“先用层次聚类得到k类”也是一种有效的策略，但计算成本较高。</em></li>
</ol>
</li>
<li><p><strong>类别数k的选择</strong>：<br>$K$ 是一个超参数，需要预先指定。选择最佳 $K$ 值没有绝对正确的方法，常用方法是<strong>手肘法 (Elbow Method)</strong>。<br><strong>原理</strong>：随着 $K$ 值的增大，簇内样本更紧密，平均直径（或WCSS $J$）会不断减小。当 $K$ 增大到接近真实簇数时，WCSS的下降幅度会突然变缓，形成一个“手肘”一样的拐点。<br><strong>具体做法</strong>：</p>
<ol>
<li>分别计算 $K&#x3D;1, 2, 3, …$ 时聚类完成后的WCSS $J$。</li>
<li>绘制 $K$ 与 $J$ 的关系曲线图。</li>
<li>寻找曲线中的“拐点”（即下降速度由快变慢的点），对应的 $K$ 值通常是一个好的选择。</li>
</ol>
<p><strong>二分查找思想</strong>：并非字面上的二分查找算法，而是指一种策略：从一个较小的 $K_{min}$ 和一个较大的 $K_{max}$ 开始，通过观察不同 $K$ 值下 $J$ 的变化趋势，逐步缩小最佳 $K$ 值的候选范围，从而更高效地找到“手肘点”。</p>
</li>
</ul>
<h3 id="4-K均值聚类用于手写字体分类"><a href="#4-K均值聚类用于手写字体分类" class="headerlink" title="4. K均值聚类用于手写字体分类"></a>4. K均值聚类用于手写字体分类</h3><h4 id="（1）数据导入"><a href="#（1）数据导入" class="headerlink" title="（1）数据导入"></a>（1）数据导入</h4><pre><code class="language-python">import numpy as np

from sklearn.datasets import load_digits

data, labels = load_digits(return_X_y=True)
(n_samples, n_features), n_digits = data.shape, np.unique(labels).size

print(f&quot;数字数: &#123;n_digits&#125;; # 样本量: &#123;n_samples&#125;; # 特征数 &#123;n_features&#125;&quot;)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509151033021.png" alt="image-20250915103305988"></p>
<h4 id="（2）定义评估函数"><a href="#（2）定义评估函数" class="headerlink" title="（2）定义评估函数"></a>（2）定义评估函数</h4><pre><code class="language-python"># 导入所需模块
from time import time  
from sklearn import metrics  
from sklearn.pipeline import make_pipeline  # 用于创建数据处理管道
from sklearn.preprocessing import StandardScaler  


def bench_k_means(kmeans, name, data, labels):
    # 记录开始时间，用于计算整个pipeline的拟合时间
    t0 = time()
    
    # 创建一个数据处理管道：先标准化数据，然后进行K均值聚类
    # make_pipeline(StandardScaler(), kmeans) 创建了一个两步流程：
    # 1. StandardScaler(): 对数据进行标准化（减去均值，除以标准差）
    # 2. kmeans: 执行K均值聚类算法
    # .fit(data) 方法依次执行这两个步骤
    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)
    
    # 计算整个拟合过程所花费的时间
    fit_time = time() - t0
    
    # 初始化结果列表，包含算法名称、拟合时间和惯性值(inertia)
    # estimator[-1] 获取管道中的最后一个估计器（即KMeans实例）
    # inertia_ 是K均值的目标函数，表示簇内平方和，值越小表示聚类效果越好
    results = [name, fit_time, estimator[-1].inertia_]

    # 定义需要真实标签和预测标签的聚类评估指标
    # 这些指标用于衡量聚类结果与真实标签的一致性
    clustering_metrics = [
        metrics.homogeneity_score,  # 同质性：每个簇只包含一个类的成员
        metrics.completeness_score,  # 完整性：给定类的所有成员都被分配到同一个簇
        metrics.v_measure_score,     # V度量：同质性和完整性的调和平均
        metrics.adjusted_rand_score, # 调整兰德指数：衡量两个数据分配之间的相似度
        metrics.adjusted_mutual_info_score,  # 调整互信息：考虑机会因素的互信息
    ]
    
    # 每个指标函数m接受真实标签labels和预测标签estimator[-1].labels_
    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]

    # 轮廓系数需要完整的数据集（特征数据+预测标签）
    results += [
        metrics.silhouette_score(
            data, 
            estimator[-1].labels_,  # 聚类预测标签
            metric=&quot;euclidean&quot;,  # 使用欧氏距离计算
            sample_size=300,  
        )
    ]

    formatter_result = (
        &quot;&#123;:9s&#125;\t&#123;:.3f&#125;s\t&#123;:.0f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;&quot;
    )
  
    print(formatter_result.format(*results))
</code></pre>
<p>定义了一个基准测试函数 <code>bench_k_means</code>，用于全面评估K均值聚类算法的性能，主要评估以下几个方面：</p>
<ol>
<li><strong>效率评估</strong>：计算算法的运行时间 (<code>fit_time</code>)</li>
<li><strong>内部评估</strong>：计算簇内平方和 (<code>inertia_</code>) 和轮廓系数 (<code>silhouette_score</code>)</li>
<li><strong>外部评估</strong>：使用真实标签计算多个评估指标，衡量聚类结果与真实类别的一致性</li>
</ol>
<h4 id="（3）运行对比算法"><a href="#（3）运行对比算法" class="headerlink" title="（3）运行对比算法"></a>（3）运行对比算法</h4><pre><code class="language-python">from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

print(82 * &quot;_&quot;)
print(&quot;init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette&quot;)

kmeans = KMeans(init=&quot;k-means++&quot;, n_clusters=n_digits, n_init=4, random_state=0)
bench_k_means(kmeans=kmeans, name=&quot;k-means++&quot;, data=data, labels=labels)

kmeans = KMeans(init=&quot;random&quot;, n_clusters=n_digits, n_init=4, random_state=0)
bench_k_means(kmeans=kmeans, name=&quot;random&quot;, data=data, labels=labels)

pca = PCA(n_components=n_digits).fit(data)
kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)
bench_k_means(kmeans=kmeans, name=&quot;PCA-based&quot;, data=data, labels=labels)

print(82 * &quot;_&quot;)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509151032541.png" alt="image-20250915103248479"></p>
<p><strong>1. k-means++ 初始化 (<code>init=&#39;k-means++&#39;</code>)</strong></p>
<ul>
<li><strong>原理</strong>: 一种智能的初始化方法，通过让初始聚类中心彼此远离来选择初始质心。第一个质心随机选择，后续质心以与已选质心距离平方成正比的概率被选中。</li>
<li><strong>特性</strong>: <strong>随机性</strong> - 每次运行结果可能不同。</li>
<li><strong>实验设置</strong>: 运行4次 (<code>n_init=4</code>) 以考虑其随机性，然后选择最佳结果。</li>
</ul>
<p><strong>2. 随机初始化 (<code>init=&#39;random&#39;</code>)</strong></p>
<ul>
<li><strong>原理</strong>: 完全随机选择数据点作为初始质心。</li>
<li><strong>特性</strong>: <strong>随机性</strong> - 每次运行结果可能大不相同。</li>
<li><strong>实验设置</strong>: 运行4次 (<code>n_init=4</code>) 以考虑其随机性，然后选择最佳结果。</li>
</ul>
<p><strong>3. 基于PCA的初始化 (确定性方法)</strong></p>
<ul>
<li><strong>原理</strong>: 使用主成分分析(PCA)的前n个主成分方向来初始化质心。具体来说，沿着前n个主成分方向，在数据范围内均匀分布质心。</li>
<li><strong>特性</strong>: <strong>确定性</strong> - 每次运行结果相同。</li>
<li><strong>实验设置</strong>: 只需要运行1次 (<code>n_init=1</code>)。</li>
</ul>
<h4 id="（4）可视化对比结果"><a href="#（4）可视化对比结果" class="headerlink" title="（4）可视化对比结果"></a>（4）可视化对比结果</h4><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs

# 使用PCA降维到2维以便可视化
reduced_data = PCA(n_components=2).fit_transform(data)

# 设置网格步长
h = 0.02  # 网格点间距

# 计算绘图范围
x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1

# 创建网格点
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# 创建包含2个子图的图形
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# 初始化方法列表
init_methods = [&#39;k-means++&#39;, &#39;random&#39;]
axes = [ax1, ax2]

# 对每种初始化方法进行循环
for i, init_method in enumerate(init_methods):
    # 执行K均值聚类
    kmeans = KMeans(init=init_method, n_clusters=n_digits, n_init=4, random_state=42)
    kmeans.fit(reduced_data)
    
    # 预测网格点的聚类标签
    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # 在当前子图上绘制决策边界
    axes[i].imshow(
        Z,
        interpolation=&quot;nearest&quot;,
        extent=(xx.min(), xx.max(), yy.min(), yy.max()),
        cmap=plt.cm.Paired,
        aspect=&quot;auto&quot;,
        origin=&quot;lower&quot;,
    )
    
    # 绘制数据点
    axes[i].plot(reduced_data[:, 0], reduced_data[:, 1], &#39;k.&#39;, markersize=2)
    
    # 绘制聚类中心
    centroids = kmeans.cluster_centers_
    axes[i].scatter(
        centroids[:, 0],
        centroids[:, 1],
        marker=&quot;x&quot;,
        s=169,
        linewidths=3,
        color=&quot;w&quot;,
        zorder=10,
    )
    
    # 设置子图标题和坐标轴
    axes[i].set_title(f&quot;K-means with &#123;init_method&#125; initialization&quot;)
    axes[i].set_xlim(x_min, x_max)
    axes[i].set_ylim(y_min, y_max)
    axes[i].set_xticks(())
    axes[i].set_yticks(())
    
    # 在标题下方显示惯性值（inertia）
    axes[i].text(0.5, -0.1, f&quot;Inertia: &#123;kmeans.inertia_:.2f&#125;&quot;, 
                transform=axes[i].transAxes, ha=&#39;center&#39;, fontsize=12)

# 调整子图间距
plt.tight_layout()
plt.show()
</code></pre>
<ol>
<li><strong>数据预处理</strong>：首先使用PCA将高维数据降维到2维，这样可以在二维平面上进行可视化。</li>
<li><strong>聚类分析</strong>：使用K-means++、random随机init算法对降维后的数据进行聚类。</li>
<li><strong>决策边界绘制</strong>：<ul>
<li>创建一个覆盖整个数据范围的密集网格</li>
<li>对网格中的每个点进行聚类预测，确定它属于哪个簇</li>
<li>使用<code>imshow()</code>将不同簇的区域用不同颜色显示，形成Voronoi图效果的决策边界</li>
</ul>
</li>
<li><strong>数据点可视化</strong>：在决策背景上，用黑色小点绘制原始数据点的实际位置。</li>
<li><strong>聚类中心标记</strong>：用醒目的白色”X”标记每个簇的中心点（质心）。</li>
</ol>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509151046285.png" alt="image-20250915104629207"></p>
<p>最终输出每个方法的inertia：也就是簇内平方和，这个值越小说明分类越集中，效果更好</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #03a9f4">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/15/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2023/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1/">
        <h2 class="post-title">机器学习建模</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2023/3/20
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="机器学习建模"><a href="#机器学习建模" class="headerlink" title="机器学习建模"></a>机器学习建模</h2><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><h3 id="1-信息熵-Entropy"><a href="#1-信息熵-Entropy" class="headerlink" title="1. 信息熵 (Entropy)"></a><strong>1. 信息熵 (Entropy)</strong></h3><p>衡量一个系统（数据集）中不确定性的大小。</p>
<p>公式：</p>
<p>$H(X) &#x3D; - \sum_{i&#x3D;1}^n p_i \log_2 p_i$</p>
<p>其中，$p_i $表示随机变量 X 取第 i 类的概率。</p>
<p>作用：当类别分布越均匀，熵越大；当分布越偏向单一类别，熵越小。</p>
<hr>
<h3 id="2-条件熵-Conditional-Entropy"><a href="#2-条件熵-Conditional-Entropy" class="headerlink" title="2. 条件熵 (Conditional Entropy)"></a>2. <strong>条件熵 (Conditional Entropy)</strong></h3><p>表示在已知特征 A 的情况下，类别 Y还剩多少不确定性。</p>
<p>公式：</p>
<p>$H(Y|A) &#x3D; \sum_{j&#x3D;1}^m p(a_j) H(Y|A&#x3D;a_j)$</p>
<p>其中，$p(a_j)$ 是特征 A 取值为 $a_j $的概率，</p>
<p>$H(Y|A&#x3D;a_j) &#x3D; -\sum_{i&#x3D;1}^n p(y_i|a_j) \log_2 p(y_i|a_j)$</p>
<hr>
<h3 id="3-信息增益-Information-Gain-IG"><a href="#3-信息增益-Information-Gain-IG" class="headerlink" title="3. 信息增益 (Information Gain, IG)"></a>3. <strong>信息增益 (Information Gain, IG)</strong></h3><p>衡量特征 A 带来的不确定性减少量，是决策树（ID3, C4.5）的核心。</p>
<p>公式：</p>
<p>$IG(Y, A) &#x3D; H(Y) - H(Y|A)$</p>
<p>即原始信息熵减去条件熵，数值越大说明特征 A 越能有效区分样本。</p>
<hr>
<h3 id="4-基尼指数-Gini-Index"><a href="#4-基尼指数-Gini-Index" class="headerlink" title="4. 基尼指数 (Gini Index)"></a>4. <strong>基尼指数 (Gini Index)</strong></h3><p>另一种衡量数据纯度的方法（常用于 CART 决策树）。</p>
<p>公式：</p>
<p>$Gini(D) &#x3D; 1 - \sum_{i&#x3D;1}^n p_i^2$</p>
<p>其中，$p_i $是第 i 类的概率。越小表示样本越“纯”。</p>
<p>若用特征 A划分：</p>
<p>$Gini(D, A) &#x3D; \sum_{j&#x3D;1}^m \frac{|D_j|}{|D|} Gini(D_j)$</p>
<p>选择能最小化 Gini 的特征作为划分点。</p>
<hr>
<h3 id="5-在决策树中的作用"><a href="#5-在决策树中的作用" class="headerlink" title="5. 在决策树中的作用"></a>5. <strong>在决策树中的作用</strong></h3><ul>
<li><strong>ID3 决策树</strong>：用 <strong>信息增益</strong> 最大的特征作为划分点。</li>
<li><strong>C4.5 决策树</strong>：改进信息增益，使用 <strong>信息增益率</strong>。</li>
<li><strong>CART 决策树</strong>：用 <strong>基尼指数最小</strong> 的特征作为划分点。</li>
</ul>
<hr>
<h2 id="（1）决策树"><a href="#（1）决策树" class="headerlink" title="（1）决策树"></a>（1）决策树</h2><h3 id="1-ID3方法（分类任务）"><a href="#1-ID3方法（分类任务）" class="headerlink" title="1. ID3方法（分类任务）"></a>1. ID3方法（分类任务）</h3><ul>
<li>输入数据集：算法接受一个已标记的数据集，其中包含一系列样本，每个样本都有一组特征和一个类别标签。</li>
<li>特征选择：从所有可能的特征中选择一个最佳的特征来进行分割。这通常通过计算每个特征的信息增益（或信息熵）来完成。信息增益表示在选择某个特征后，数据集的不确定性减少的程度。信息增益高的特征被认为是最佳的选择。</li>
<li>创建一个决策节点：将选定的特征用于创建一个决策节点，并将数据集分割成多个子集，每个子集对应于该特征的不同取值。</li>
<li>递归操作：对每个子集重复上述步骤，直到满足停止条件。停止条件可以是以下之一：所有样本都属于同一类别，或者没有更多的特征可用于分割数据。</li>
<li>构建决策树：最终，算法通过连接所有的决策节点来构建一个完整的决策树，其中叶节点表示最终的分类结果。</li>
</ul>
<h3 id="2-C4-5算法（分类任务）"><a href="#2-C4-5算法（分类任务）" class="headerlink" title="2. C4.5算法（分类任务）"></a>2. C4.5算法（分类任务）</h3><p>根据信息增益率进行特征选择，引入悲观剪枝策略进行后剪枝</p>
<p>工作原理：</p>
<p>特征选择：</p>
<ul>
<li>特征选择：与ID3类似，C4.5也通过计算特征的信息增益来选择最佳的特征进行分割。不过，C4.5引入了一个新的概念，称为”信息增益比”，以解决ID3在特征取值较多时的不公平问题。信息增益比考虑了特征取值的数量，从而更加公平地对待不同数量的取值。</li>
<li>处理连续型特征：C4.5能够处理连续型特征，而不仅仅是离散型特征。它通过尝试不同的分割点来将连续特征离散化，并选择最佳的分割点以最大化信息增益或信息增益比。</li>
<li>剪枝：C4.5引入了剪枝机制，以减小生成的决策树的复杂性，防止过拟合。剪枝是通过验证数据集来确定哪些子树可以被删除或保留的。这有助于生成更简单、更具泛化能力的决策树。</li>
<li>处理缺失值：C4.5能够处理数据中的缺失值，允许在构建决策树时处理包含缺失数据的样本。</li>
<li>生成决策树：通过递归地选择最佳特征、分割数据、剪枝等步骤，C4.5最终生成一个用于分类或回归的决策树。</li>
</ul>
<h3 id="3-CART算法（分类-回归任务）"><a href="#3-CART算法（分类-回归任务）" class="headerlink" title="3. CART算法（分类&#x2F;回归任务）"></a>3. CART算法（分类&#x2F;回归任务）</h3><p>基于二叉树，既可分类也可回归，使用基尼指数来进行特征选择</p>
<p><strong>工作原理</strong>：</p>
<ul>
<li>二叉树结构：CART算法生成的决策树是二叉树结构，每个非叶子节点都有两个子节点。这意味着每个特征在每个节点处只进行一次二分分割，而不是多分割。</li>
<li>特征选择：CART算法使用一种称为“Gini不纯度”（Gini impurity）的指标来选择最佳的特征进行分割。Gini不纯度度量了一个数据集中样本被错误分类的概率。算法选择能够最大程度地减小Gini不纯度的特征进行分割。</li>
<li>处理连续型特征：CART能够处理连续型特征，它通过尝试不同的阈值来将连续特征二分化，并选择最佳的分割点。</li>
<li>剪枝：CART算法也支持剪枝，以减小决策树的复杂性和防止过拟合。剪枝是通过验证数据集来确定哪些子树可以被删除或保留的。</li>
<li>多分类和回归：CART算法不仅可以用于分类问题，还可以用于回归问题。对于分类问题，CART生成的树将每个叶节点标记为某个类别。对于回归问题，叶节点包含一个连续的数值。</li>
</ul>
<p>后剪枝与预剪枝</p>
<p>预剪枝：在建树的过程中对每一次分裂都进行判断（使用验证集计算分裂后的回归预测正确率），若效果下降，则减去该分支</p>
<p>后剪枝：建树后再自底向上逐一判断。。。</p>
<h2 id="（2）LightGBM"><a href="#（2）LightGBM" class="headerlink" title="（2）LightGBM"></a>（2）LightGBM</h2><h2 id="（3）GBDT梯度提升决策树"><a href="#（3）GBDT梯度提升决策树" class="headerlink" title="（3）GBDT梯度提升决策树"></a>（3）GBDT梯度提升决策树</h2><p>为boosting的一种（在计算过程中串行），使用多个弱决策器（树）的结果累加而成。首先使用一棵树进行模糊回归预测，得到较大的残差（与真实值），然后训练另外一棵树对该残差进行拟合（即目标是找到合适的分裂节点，能使得该残差往最小的方向去），依次累加，最终使得残差为0。然后将所有弱决策器的回归预测累加而成记得GBDT的最终回归预测。</p>
<blockquote>
<p>更准确的回答：GBDT 是一种串行的梯度提升方法，使用多棵浅回归树作为基学习器。先用常数或一棵树得到初始预测；第 mmm 轮计算当前模型在训练集上的<strong>负梯度（伪残差）</strong>，训练一棵小回归树去拟合它；再在每个叶子上求使<strong>损失函数</strong>最小的叶值，并以学习率缩减后加到当前模型上。重复多轮，最终模型是各棵树输出的加和。在 L2 回归时，这等价于“逐轮拟合残差并累加”；在分类&#x2F;排序等任务中则拟合相应损失的梯度信号</p>
</blockquote>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #00bcd4">机器学习</a>
        </span>
        
    </div>
    <a href="/2023/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/09/08/kaggle%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E5%BB%BA%E6%A8%A1%EF%BC%9AWrapper%20%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%20LightGBM%20+%20TPE%20%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/">
        <h2 class="post-title">Kaggle房价预测建模：Wrapper 特征选择与 LightGBM + TPE 超参数优化</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/Kaggle/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Kaggle
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/9/8
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />


<h1 id="KAGGLE-房价预测建模：Wrapper-特征选择与-LightGBM-TPE-超参数优化"><a href="#KAGGLE-房价预测建模：Wrapper-特征选择与-LightGBM-TPE-超参数优化" class="headerlink" title="KAGGLE 房价预测建模：Wrapper 特征选择与 LightGBM + TPE 超参数优化"></a>KAGGLE 房价预测建模：Wrapper 特征选择与 LightGBM + TPE 超参数优化</h1><hr>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在 Kaggle <strong>House Prices: Advanced Regression Techniques</strong> 比赛中，我们需要基于房屋的多维度特征（面积、位置、建造年份等）预测售价。本文展示了一个端到端的建模流程，核心方法包括：</p>
<ul>
<li><strong>缺失值合理填充（结合业务逻辑）</strong></li>
<li><strong>Wrapper 特征选择</strong></li>
<li><strong>LightGBM 模型训练</strong></li>
<li><strong>TPE 超参数调优</strong></li>
<li><strong>结果生成与提交</strong></li>
</ul>
<hr>
<h2 id="2-数据分析与预处理"><a href="#2-数据分析与预处理" class="headerlink" title="2. 数据分析与预处理"></a>2. 数据分析与预处理</h2><h3 id="数据加载与探索"><a href="#数据加载与探索" class="headerlink" title="数据加载与探索"></a>数据加载与探索</h3><pre><code class="language-python">import pandas as pd
import numpy as np

train_df = pd.read_csv(&#39;../input/house-prices-advanced-regression-techniques/train.csv&#39;)
test_df = pd.read_csv(&#39;../input/house-prices-advanced-regression-techniques/test.csv&#39;)

train_df.info()
train_df = train_df.drop(&#39;Id&#39;, axis=1)
</code></pre>
<pre><code class="language-python">train_df.head(5)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081008796.png" alt="image-20250908100815685"></p>
<p>查看数据类型，判断哪些是离散型变量（object），哪些是连续型变量（int64,float64）</p>
<pre><code class="language-python">train_df.info()
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081009889.png" alt="image-20250908100921833"></p>
<pre><code class="language-python">train_df[&#39;Id&#39;].nunique() == train_df.shape[0]  # 检查 Id 唯一性
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081008347.png" alt="image-20250908100851310"></p>
<h3 id="缺失值分析与处理"><a href="#缺失值分析与处理" class="headerlink" title="缺失值分析与处理"></a>缺失值分析与处理</h3><h4 id="离散型变量缺失值"><a href="#离散型变量缺失值" class="headerlink" title="离散型变量缺失值"></a>离散型变量缺失值</h4><pre><code class="language-python">cat_cols = train_df.select_dtypes(include=[&#39;object&#39;]).columns
missing_cat_cols = train_df[cat_cols].columns[train_df[cat_cols].isnull().any()]

missing_cat_dict = &#123;&#125;
for col in missing_cat_cols:
    missing_cat_dict[col] = train_df[col].unique()

# 填充为 &#39;missing&#39;
train_df[cat_cols] = train_df[cat_cols].fillna(&#39;missing&#39;)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081009024.png" alt="image-20250908100951906"></p>
<p>因为类别不多，所以都可以直接填充为missing，然后再使用编码器进行编码</p>
<h4 id="连续型变量缺失值"><a href="#连续型变量缺失值" class="headerlink" title="连续型变量缺失值"></a>连续型变量缺失值</h4><p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081010565.png" alt="image-20250908101020482"></p>
<pre><code class="language-python">num_cols = train_df.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
train_df[num_cols].isnull().sum()

# 查看缺失分布
train_df[train_df[num_cols].columns[train_df[num_cols].isnull().any()]].describe()

# 针对具体列的填充策略
train_df[&#39;LotFrontage&#39;] = train_df.groupby(&#39;Neighborhood&#39;)[&#39;LotFrontage&#39;].transform(lambda x: x.fillna(x.median()))
train_df[&#39;MasVnrArea&#39;] = train_df[&#39;MasVnrArea&#39;].fillna(0)
train_df[&#39;GarageYrBlt&#39;] = train_df[&#39;GarageYrBlt&#39;].fillna(train_df[&#39;GarageYrBlt&#39;].mean())
</code></pre>
<ul>
<li><strong>LotFrontage（地块临街长度）</strong>：按街区中位数填充，避免极端值影响。</li>
<li><strong>MasVnrArea（贴面石材面积）</strong>：缺失意味着“无贴面”，填充为 0。</li>
<li><strong>GarageYrBlt（车库建造年份）</strong>：缺失意味着“无车库”，采用均值填充。</li>
</ul>
<h3 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h3><pre><code class="language-python">from sklearn.preprocessing import LabelEncoder

for col in cat_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].astype(str))
    test_df[col] = le.transform(test_df[col].astype(str))
</code></pre>
<hr>
<h2 id="3-Wrapper-特征选择"><a href="#3-Wrapper-特征选择" class="headerlink" title="3. Wrapper 特征选择"></a>3. Wrapper 特征选择</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>**Wrapper 特征选择（封装式特征选择）**是一种基于模型性能来评估特征重要性的方法。与 <strong>Filter 方法</strong>（依靠统计量，如相关系数、卡方检验）或 <strong>Embedded 方法</strong>（如 L1 正则化直接稀疏化特征）不同，Wrapper 方法会在训练模型的过程中动态评估特征的贡献，并通过迭代或交叉验证来挑选最优特征子集。</p>
<p>在本项目中，Wrapper 特征选择的核心流程如下：</p>
<ol>
<li><strong>使用 LightGBM 训练并获取特征重要性</strong><ul>
<li>LightGBM 在训练过程中会计算每个特征的 <strong>分裂增益（Gain）</strong>，即该特征用于分裂时对损失函数减少的贡献。</li>
<li>在每棵树构建完成后，LightGBM 会累积各个特征的重要性得分。特征的重要性越高，说明它在分裂过程中越常被使用，并且能有效减少预测误差。</li>
</ul>
</li>
<li><strong>KFold 交叉验证累积特征重要性</strong><ul>
<li>单次模型训练可能会受样本划分影响，导致特征重要性不稳定。</li>
<li>为了减少偶然性，Wrapper 方法会采用 <strong>K 折交叉验证</strong>（KFold CV）：<ul>
<li>将数据划分为 K 个子集</li>
<li>每次用 K-1 个子集训练，1 个子集验证</li>
<li>重复 K 次，保证每个样本都参与训练和验证</li>
</ul>
</li>
<li>在每一次训练结束后，提取特征重要性，并累加到一个全局向量中。</li>
<li>最终，得到的特征重要性是 <strong>跨 K 折平均后的结果</strong>，比单次训练更稳健。</li>
</ul>
</li>
<li><strong>选取前 *k* 个重要特征</strong><ul>
<li>当所有 K 折训练结束后，会得到一个按特征排序的重要性得分序列。</li>
<li>通过排序（降序），选取前 <em>k</em> 个特征作为最终的特征子集。</li>
</ul>
</li>
</ol>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code class="language-python">from lightgbm import early_stopping, log_evaluation
from sklearn.model_selection import KFold
import lightgbm as lgb

def feature_select_wrapper(train, topk=50):
    label = &#39;SalePrice&#39;
    features = [col for col in train.columns if col != label and train[col].nunique() &gt; 1]
    y_log = np.log1p(train[label])

    params_initial = &#123;
        &#39;num_leaves&#39;: 30,
        &#39;learning_rate&#39;: 0.05,
        &#39;boosting&#39;: &#39;gbdt&#39;,
        &#39;min_child_samples&#39;: 10,
        &#39;bagging_seed&#39;: 2020,
        &#39;bagging_fraction&#39;: 0.7,
        &#39;bagging_freq&#39;: 1,
        &#39;feature_fraction&#39;: 0.7,
        &#39;max_depth&#39;: -1,
        &#39;metric&#39;: &#39;rmse&#39;,
        &#39;reg_alpha&#39;: 0,
        &#39;reg_lambda&#39;: 1,
        &#39;objective&#39;: &#39;regression&#39;
    &#125;

    NBR = 50000
    kf = KFold(n_splits=5, random_state=2020, shuffle=True)
    fse = pd.Series(0, index=features)

    for train_idx, eval_idx in kf.split(train[features], y_log):
        train_part = lgb.Dataset(train[features].iloc[train_idx], y_log.iloc[train_idx])
        eval_part = lgb.Dataset(train[features].iloc[eval_idx], y_log.iloc[eval_idx])
        
        bst = lgb.train(
            params_initial, train_part, num_boost_round=NBR,
            valid_sets=[train_part, eval_part],
            valid_names=[&#39;train&#39;, &#39;valid&#39;],
            callbacks=[early_stopping(100), log_evaluation(100)]
        )
        fse += pd.Series(bst.feature_importance(importance_type=&quot;gain&quot;), index=features)

    return fse.sort_values(ascending=False).index.tolist()[:topk]
</code></pre>
<hr>
<h2 id="4-LightGBM-建模"><a href="#4-LightGBM-建模" class="headerlink" title="4. LightGBM 建模"></a>4. LightGBM 建模</h2><h3 id="为什么选择-LightGBM"><a href="#为什么选择-LightGBM" class="headerlink" title="为什么选择 LightGBM"></a>为什么选择 LightGBM</h3><ul>
<li>速度快（直方图算法）</li>
<li>内存高效</li>
<li>支持类别特征</li>
<li>叶子优先生长策略（leaf-wise）</li>
</ul>
<h3 id="LightGBM-的原理"><a href="#LightGBM-的原理" class="headerlink" title="LightGBM 的原理"></a>LightGBM 的原理</h3><p>具体可参考之前的文章[集成学习之Boosting方法系列_LightGBM](<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_62895602/article/details/135789720?spm=1001.2014.3001.5501">集成学习之Boosting方法系列_LightGBM_light gradient boosting-CSDN博客</a>)</p>
<ul>
<li>每轮拟合残差</li>
<li>基于梯度提升更新</li>
<li>内置正则化减少过拟合</li>
</ul>
<hr>
<h2 id="5-TPE-超参数优化"><a href="#5-TPE-超参数优化" class="headerlink" title="5. TPE 超参数优化"></a>5. TPE 超参数优化</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>**TPE（Tree-structured Parzen Estimator，树结构化帕森估计器）**是一种基于贝叶斯思想的超参数优化方法，它的目标是在有限的试验次数内尽可能找到表现最优的参数组合。相比于传统的 <strong>Grid Search（网格搜索）</strong> 或 <strong>Random Search（随机搜索）</strong>，TPE 通过构建概率模型来“有策略地”探索参数空间，从而显著提升调参效率。</p>
<p>其核心思想可以分为以下几个步骤：</p>
<ol>
<li><p><strong>贝叶斯优化框架</strong></p>
<ul>
<li>在超参数优化问题中，我们要最小化一个目标函数 f(x)f(x)，其中 xx 是参数组合（如 LightGBM 的 <code>num_leaves</code>、<code>learning_rate</code> 等）。</li>
<li>贝叶斯优化的基本思想是：<ul>
<li>用一个概率模型（如高斯过程或 TPE）来近似目标函数。</li>
<li>在每一轮迭代时，利用概率模型选择最有希望的参数区域进行试验。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>概率建模</strong></p>
<ul>
<li>与高斯过程不同，TPE 使用了 <strong>密度估计</strong> 的方法来建模。</li>
<li>在优化过程中，TPE 将历史试验结果按照性能阈值 γ\gamma（如前 20% 的结果）划分为两类：<ul>
<li><strong>好的参数分布</strong>$l(x) &#x3D; p(x|f(x) &lt; \gamma)$</li>
<li><strong>差的参数分布</strong> $g(x) &#x3D; p(x|f(x) \geq \gamma)$</li>
</ul>
</li>
<li>这里的 $l(x) $和 $g(x) $都通过 <strong>Parzen 窗估计（核密度估计的一种）</strong> 来近似。</li>
</ul>
</li>
<li><p><strong>采样准则</strong></p>
<ul>
<li><p>TPE 选择新参数的原则是最大化以下比值：</p>
<p>$\text{argmax}_x \ \frac{l(x)}{g(x)}$</p>
</li>
<li><p>含义是：倾向于选择那些更可能来自“好分布”而不是“坏分布”的参数点。</p>
</li>
<li><p>与随机搜索相比，这种方法能够 <strong>自适应地聚焦在表现更优的区域</strong>，提高搜索效率。</p>
</li>
</ul>
</li>
<li><p><strong>树结构化空间支持</strong></p>
<ul>
<li>与普通贝叶斯优化不同，TPE 能够处理 <strong>树状依赖关系的搜索空间</strong>（Tree-structured Search Space）。</li>
<li>比如：如果选择 <code>boosting=gbdt</code>，才会去搜索 <code>num_leaves</code> 和 <code>feature_fraction</code>；如果选择 <code>boosting=rf</code>，则会去搜索 <code>subsample_freq</code> 等。</li>
<li>这种灵活性使得 TPE 特别适合机器学习调参中 <strong>条件依赖</strong> 的情况。</li>
</ul>
</li>
</ol>
<p>在本项目里，TPE 用于优化 LightGBM 的关键参数（如 <code>num_leaves</code>、<code>learning_rate</code>、<code>feature_fraction</code> 等），通过有限的 20 次试验，就能找到比默认参数更优的组合，从而降低 RMSE。</p>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code class="language-python">from hyperopt import fmin, tpe, hp

def params_append(params):
    params[&#39;feature_pre_filter&#39;] = False
    params[&#39;objective&#39;] = &#39;regression&#39;
    params[&#39;metric&#39;] = &#39;rmse&#39;
    params[&#39;bagging_seed&#39;] = 2020
    params[&#39;max_depth&#39;] = -1
    return params

def param_hyperopt(train_df, train_label):
    train_label_log = np.log1p(train_label)
    train_data = lgb.Dataset(train_df, label=train_label_log)

    def hyperopt_objective(params):
        params = params_append(params)
        res = lgb.cv(
            params,
            train_data,
            num_boost_round=800,
            nfold=3,
            stratified=False,
            shuffle=True,
            callbacks=[early_stopping(50), log_evaluation(100)]
        )
        return min(res[&#39;valid rmse-mean&#39;])

    params_space = &#123;
        &#39;learning_rate&#39;: hp.uniform(&#39;learning_rate&#39;, 0.05, 0.1),
        &#39;bagging_fraction&#39;: hp.uniform(&#39;bagging_fraction&#39;, 0.7, 1),
        &#39;feature_fraction&#39;: hp.uniform(&#39;feature_fraction&#39;, 0.7, 1),
        &#39;num_leaves&#39;: hp.choice(&#39;num_leaves&#39;, list(range(10, 30, 2))),
    &#125;

    params_best = fmin(
        fn=hyperopt_objective,
        space=params_space,
        algo=tpe.suggest,
        max_evals=20
    )
    return params_best
</code></pre>
<hr>
<h2 id="6-最终建模与结果提交"><a href="#6-最终建模与结果提交" class="headerlink" title="6. 最终建模与结果提交"></a>6. 最终建模与结果提交</h2><pre><code class="language-python"># 特征选择
feature_select = feature_select_wrapper(train_df, topk=50)
rf_train_df = train_df[feature_select]
rf_train_label = train_df[&#39;SalePrice&#39;]

# TPE 调优得到最佳参数
best_rf_params = param_hyperopt(rf_train_df, rf_train_label)

# 最优模型
best_model = lgb.LGBMRegressor(**best_rf_params, n_jobs=-1)
best_model.fit(rf_train_df, rf_train_label)

# 预测
rf_test_df = test_df[feature_select]
y_pred = best_model.predict(rf_test_df)

# 提交文件
submission = pd.DataFrame(&#123;
    &quot;Id&quot;: test_df[&quot;Id&quot;],
    &quot;SalePrice&quot;: y_pred
&#125;)
submission.to_csv(&quot;/kaggle/working/submission.csv&quot;, index=False)
</code></pre>
<hr>
<h2 id="7-实验结果与分析"><a href="#7-实验结果与分析" class="headerlink" title="7. 实验结果与分析"></a>7. 实验结果与分析</h2><ul>
<li><strong>Baseline LightGBM</strong>：RMSE ≈ 0.155</li>
<li><strong>Wrapper 特征选择后</strong>：RMSE ≈ 0.148</li>
<li><strong>Wrapper + LightGBM + TPE 调优</strong>：RMSE ≈ 0.141</li>
</ul>
<p>结果表明：</p>
<ul>
<li>Wrapper 有效减少冗余特征</li>
<li>TPE 自动调参提升了模型性能</li>
</ul>
<hr>
<h2 id="8-总结与展望"><a href="#8-总结与展望" class="headerlink" title="8. 总结与展望"></a>8. 总结与展望</h2><p>本文展示了完整的 Kaggle 房价预测流程：</p>
<ul>
<li>合理的缺失值处理</li>
<li>Wrapper 特征选择</li>
<li>LightGBM 高效建模</li>
<li>TPE 超参数调优</li>
</ul>
<p>未来可以进一步尝试：</p>
<ul>
<li>模型集成（XGBoost、CatBoost）</li>
<li>使用 Optuna 替代 Hyperopt</li>
<li>特征交互与自动特征工程</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Kaggle/" style="color: #ffa2c4">Kaggle</a>
        </span>
        
    </div>
    <a href="/2022/09/08/kaggle%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E5%BB%BA%E6%A8%A1%EF%BC%9AWrapper%20%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%20LightGBM%20+%20TPE%20%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/08/30/pandas/">
        <h2 class="post-title">pandas</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/python/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                python
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/8/30
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h1><h2 id="pandas读取csv文件"><a href="#pandas读取csv文件" class="headerlink" title="pandas读取csv文件"></a>pandas读取csv文件</h2><pre><code class="language-python">import pandas as pd

df = pd.read_csv(&#39;file_path&#39;)
</code></pre>
<h2 id="pandas读取excel文件"><a href="#pandas读取excel文件" class="headerlink" title="pandas读取excel文件"></a>pandas读取excel文件</h2><pre><code class="language-python">import pandas as pd

df = pd.read_excel(&#39;file_path&#39;)
</code></pre>
<p>pandas一些常用式子</p>
<ol>
<li>条件筛选</li>
</ol>
<pre><code class="language-python">df[df[&#39;columns_name&#39;] &gt; value]
</code></pre>
<ol start="2">
<li>多条件筛选</li>
</ol>
<pre><code class="language-python">df[(df[&#39;col1&#39;] &gt; val1) &amp; (df[&#39;col2&#39;] == val2)]
</code></pre>
<ol start="3">
<li>排序</li>
</ol>
<pre><code class="language-python">df.sort_values(&#39;column_name&#39;)
</code></pre>
<ol start="4">
<li>分组聚合</li>
</ol>
<pre><code class="language-python">df.groupby(&#39;column_name&#39;).agg(&#123;&#39;other_col&#39;: &#39;sum&#39;&#125;)
</code></pre>
<ol start="5">
<li>连接合并</li>
</ol>
<pre><code class="language-python">pd.merge(df1, df2, on=&#39;key&#39;)
</code></pre>
<ol start="6">
<li>计数</li>
</ol>
<pre><code class="language-python">df[&#39;column_name&#39;].value_counts()
</code></pre>
<ol start="7">
<li>将某列拆成多列</li>
</ol>
<pre><code class="language-python">test_df[[&#39;month&#39;, &#39;sector_norm&#39;]] = test_df[&#39;id&#39;].str.split(&#39;_&#39;, expand=True)
</code></pre>
<ol start="9">
<li>找出缺失列</li>
</ol>
<pre><code class="language-python">missing_cols = X_test.columns[X_test.isnull().any()]
</code></pre>
<ol start="10">
<li>转换日期至标准格式</li>
</ol>
<pre><code class="language-python"># 假设month_norm 原本是2022 Jan
# Y代表四位年份，b代表月份缩写，m代表两位月份
X_train[&#39;month_index&#39;] = pd.to_datetime(X_train[&#39;month_norm&#39;], format=&quot;%Y %b&quot;)
</code></pre>
<ol start="11">
<li>寻找离散型与连续型列</li>
</ol>
<pre><code class="language-python"># 离散型
cat_cols = city_search_index.select_dtypes(include=[&#39;object&#39;]).columns
# 连续型
num_cols = city_search_index.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
</code></pre>
<ol start="12">
<li>连续子图展示</li>
</ol>
<pre><code class="language-python">sns.set()
import math

ncols = 1
nrows = math.ceil(len(missing_cols2) / ncols)
fig, axes = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3* nrows))
axes = axes.flatten()
for i, col in enumerate(missing_cols2):
    sns.histplot(new_house_transactions[col], kde=True, ax=axes[i])

plt.tight_layout()
plt.show()
</code></pre>
<ol start="13">
<li>去除行</li>
</ol>
<pre><code class="language-python">city_indexes = city_indexes.drop_duplicates(subset=[&#39;year&#39;])
# subset：用于指定根据哪几列去重。如果不指定，默认对所有列进行去重。
# keep：
# &#39;first&#39;（默认）：保留第一次出现的重复行，删除后续的重复行。
# &#39;last&#39;：保留最后一次出现的重复行，删除之前的。
# False：删除所有重复的行（只保留唯一的行，没有重复的）。
# inplace：
# False（默认）：返回一个去重后的新DataFrame，不修改原始数据。
# True：在原始DataFrame上直接修改，不返回新对象。
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/pandas/" style="color: #03a9f4">pandas</a>
        </span>
        
    </div>
    <a href="/2022/08/30/pandas/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <span class="current">1</span>
    
    <a class="page-num" href="/page/2">
        2
    </a>
    
    
    
    
    <a class="page-num" href="/page/2/">
        <i class="fa-solid fa-caret-right fa-fw"></i>
    </a>
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/pic.jpg" alt="avatar" />
        </div>
        <div class="name">KING BOB</div>
        <div class="description">
            <p>Description<br>…</p>

        </div>
        
        
        <div class="friend-links">
            
            <div class="friend-link">
                <a target="_blank" rel="noopener" href="https://argvchs.github.io">Argvchs</a>
            </div>
            
        </div>
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2025 - 2025 KING!BOB!
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;KING BOB
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    <canvas
    id="fireworks"
    style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767"
></canvas>
<script src="https://s4.zstatic.net/ajax/libs/animejs/3.2.1/anime.min.js"></script>
<script src="/js/fireworks.min.js"></script>
    <canvas
    id="background"
    style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: -1"
></canvas>
<script src="/js/background.min.js"></script>
    
<!-- hexo injector body_end start --><script data-pjax src="https://unpkg.com/oh-my-live2d"></script><script>const oml2d = OML2D.loadOml2d({dockedPosition:"right",mobileDisplay:true,models:[{"path":"https://unpkg.com/live2d-widget-model-shizuku@1.0.5/assets/shizuku.model.json","mobilePosition":[-10,23],"mobileScale":0.1,"mobileStageStyle":{"width":180,"height":166},"motionPreloadStrategy":"IDLE","position":[-10,35],"scale":0.15,"stageStyle":{"width":250,"height":250}},{"path":"https://unpkg.com/live2d-widget-model-koharu@1.0.5/assets/koharu.model.json","scale":0.12,"position":[0,0],"stageStyle":{"width":250},"mobileScale":0.08,"mobilePosition":[0,0],"mobileStageStyle":{"width":180}},{"path":"https://unpkg.com/live2d-widget-model-haruto@1.0.5/assets/haruto.model.json","scale":0.12,"position":[0,0],"mobileScale":0.08,"mobilePosition":[0,0],"mobileStageStyle":{"width":180},"stageStyle":{"width":250}}],parentElement:document.body,primaryColor:"var(--btn-bg)",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body>
</html>
