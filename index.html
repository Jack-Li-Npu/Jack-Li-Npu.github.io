
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8" />
    <title>KING!BOB!</title>
    <meta name="author" content="KING BOB" />
    <meta name="description" content="LET'S MAKE IT HAPPEN" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/pic.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>KING!BOB!</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;KING!BOB!</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div
        id="home-background"
        ref="homeBackground"
        data-images="/images/background2.jpg"
    ></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>KING!BOB!</h1>
                <h3></h3>
                <h5>LET&#39;S MAKE IT HAPPEN</h5>
            </div>
        </span>
    </div>
</div>
<div
    id="home-posts-wrap"
    ref="homePostsWrap"
    true
>
    <div id="home-posts">
        

<div class="post">
    <a href="/2025/09/21/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">朴素贝叶斯：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/21
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />

<h1 id="朴素贝叶斯：从理论到实践"><a href="#朴素贝叶斯：从理论到实践" class="headerlink" title="朴素贝叶斯：从理论到实践"></a>朴素贝叶斯：从理论到实践</h1><h2 id="一-朴素贝叶斯基本方法"><a href="#一-朴素贝叶斯基本方法" class="headerlink" title="一. 朴素贝叶斯基本方法"></a>一. 朴素贝叶斯基本方法</h2><p>朴素贝叶斯是一种基于贝叶斯定理与特征条件独立性假设的分类方法。它的基本思想非常简单直接：对于给定的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就将其归为哪个类别。</p>
<h3 id="a-学习联合概率分布-P-X-Y"><a href="#a-学习联合概率分布-P-X-Y" class="headerlink" title="a. 学习联合概率分布 P(X, Y)"></a>a. 学习联合概率分布 P(X, Y)</h3><p>首先，我们需要理解模型要学习什么。设输入特征空间 $\mathcal{X} \subseteq \mathbb{R}^n$ 为 n 维向量的集合，输出空间为类标记集合 $\mathcal{Y} &#x3D; {c_1, c_2, …, c_k}$。训练数据集 $T &#x3D; {(x_1, y_1), (x_2, y_2), …, (x_N, y_N)}$ 由 $P(X, Y)$ 独立同分布产生。</p>
<p>朴素贝叶斯法的目的，就是通过训练数据集来<strong>学习联合概率分布 $P(X, Y)$</strong>。具体来说，这个联合分布可以分解为：<br>$$P(X, Y) &#x3D; P(Y) \cdot P(X|Y)$$</p>
<p>其中：</p>
<ul>
<li>$P(Y)$ 是<strong>先验概率</strong>，即在没有任何特征信息的情况下，某一类别 $c_k$ 出现的概率。</li>
<li>$P(X|Y)$ 是<strong>条件概率分布</strong>，即在已知类别为 $c_k$ 的条件下，特征 $X$ 取特定值 $x$ 的概率。</li>
</ul>
<h3 id="b-条件独立性的强假设与“朴素”之名"><a href="#b-条件独立性的强假设与“朴素”之名" class="headerlink" title="b. 条件独立性的强假设与“朴素”之名"></a>b. 条件独立性的强假设与“朴素”之名</h3><p>条件概率 $P(X&#x3D;x|Y&#x3D;c_k)$ 是几乎所有概率模型中最难估计的部分。因为 $X &#x3D; (x^{(1)}, x^{(2)}, …, x^{(n)})$ 是一个维度非常高的向量，其可能的取值组合是指数级增长的。直接在有限的数据集上估计如此复杂的分布几乎是不可能的。</p>
<p>为了克服这个障碍，朴素贝叶斯法做出了一个强有力的、也是其被称为“朴素”的假设：<strong>特征条件独立性</strong>。即假设所有特征在类别确定的条件下都是相互独立的。</p>
<p>这个假设意味着，一个特征出现的概率与其他特征是否出现无关。虽然这个假设在现实中很少真正成立（例如，在文本中，“人工智能”这个词的出现显然与“深度学习”相关），但它极大地简化了计算。基于此假设，条件概率可以分解为：<br>$$P(X&#x3D;x|Y&#x3D;c_k) &#x3D; P(x^{(1)}, x^{(2)}, …, x^{(n)} | Y&#x3D;c_k) &#x3D; \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)$$</p>
<h3 id="c-贝叶斯定理"><a href="#c-贝叶斯定理" class="headerlink" title="c. 贝叶斯定理"></a>c. 贝叶斯定理</h3><p>学习到联合概率后，我们如何进行分类？这就要用到<strong>贝叶斯定理</strong>。贝叶斯定理为我们提供了在观察到特征 $X$ 后，估计类别 $Y$ 的概率（称为后验概率）的方法：<br>$$P(Y&#x3D;c_k | X&#x3D;x) &#x3D; \frac{P(X&#x3D;x| Y&#x3D;c_k) \cdot P(Y&#x3D;c_k)}{P(X&#x3D;x)} &#x3D; \frac{P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)}{\sum_{k} P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)}$$</p>
<p>其中，$P(X&#x3D;x)$ 是证据（Evidence），对于所有类别 $c_k$ 来说都是一个常数归一化因子，确保所有后验概率之和为1。</p>
<h3 id="d-生成模型"><a href="#d-生成模型" class="headerlink" title="d. 生成模型"></a>d. 生成模型</h3><p>由于朴素贝叶斯方法学习了联合概率分布 $P(X, Y)$，它实际上<strong>学习到了数据是如何生成的机制</strong>：它知道每个类别 $c_k$ 本身出现的先验概率 $P(Y&#x3D;c_k)$，以及在这个类别下，生成每个特征 $x^{(j)}$ 的概率 $P(x^{(j)}|Y&#x3D;c_k)$。因此，它属于<strong>生成模型</strong>（Generative Model）。与之相对的是判别模型（Discriminative Model，如逻辑回归、SVM），后者直接学习决策边界 $P(Y|X)$ 而不关心数据的生成方式。</p>
<h3 id="e-后验概率最大化与分类决策"><a href="#e-后验概率最大化与分类决策" class="headerlink" title="e. 后验概率最大化与分类决策"></a>e. 后验概率最大化与分类决策</h3><p>朴素贝叶斯的分类器即为将后验概率最大的类别作为输出。因此，对于给定的输入 $x$，其预测的类别 $\hat{y}$ 是：<br>$$\hat{y} &#x3D; \arg \max_{c_k \in \mathcal{Y}} P(Y&#x3D;c_k | X&#x3D;x)$$</p>
<p>由于分母 $P(X&#x3D;x)$ 对所有 $c_k$ 都相同，我们可以将其忽略，得到等价的公式：<br>$$\hat{y} &#x3D; \arg \max_{c_k \in \mathcal{Y}} P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x^{(j)} | Y&#x3D;c_k)$$</p>
<p>这就是朴素贝叶斯分类器最终使用的决策函数。</p>
<hr>
<h2 id="二-参数估计"><a href="#二-参数估计" class="headerlink" title="二. 参数估计"></a>二. 参数估计</h2><p>在现实中，我们无法知道真实的概率 $P(Y&#x3D;c_k)$ 和 $P(x^{(j)}|Y&#x3D;c_k)$，只能从训练集中进行估计。最常用的方法就是<strong>极大似然估计（MLE）</strong>。</p>
<p><strong>极大似然估计</strong>的核心思想是：在已知随机变量属于某种分布（如伯努利分布、多项分布、高斯分布）但参数未知的情况下，寻找一组参数，使得当前观测到的样本数据出现的概率（似然度）最大。</p>
<h3 id="a-先验概率的极大似然估计"><a href="#a-先验概率的极大似然估计" class="headerlink" title="a. 先验概率的极大似然估计"></a>a. 先验概率的极大似然估计</h3><p>$$P(Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k)}{N}, \quad k&#x3D;1,2,…,K$$<br>其中 $I$ 是指示函数，当 $y_i &#x3D; c_k$ 时为1，否则为0。即，$P(Y&#x3D;c_k)$ 的估计值是训练集中类别为 $c_k$ 的样本所占的比例。</p>
<h3 id="b-条件概率的极大似然估计"><a href="#b-条件概率的极大似然估计" class="headerlink" title="b. 条件概率的极大似然估计"></a>b. 条件概率的极大似然估计</h3><p>条件概率的估计取决于特征所遵循的分布。对于第 $j$ 个特征 $x^{(j)}$，其取值集合为 ${a_{j1}, a_{j2}, …, a_{jS_j}}$，则条件概率的极大似然估计为：<br>$$P(x^{(j)} &#x3D; a_{jl} | Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(x_i^{(j)} &#x3D; a_{jl}, y_i &#x3D; c_k)}{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k)}$$<br>即，在类别为 $c_k$ 的样本中，第 $j$ 个特征取值 $a_{jl}$ 的样本所占的比例。</p>
<hr>
<h2 id="三-学习与分类算法"><a href="#三-学习与分类算法" class="headerlink" title="三. 学习与分类算法"></a>三. 学习与分类算法</h2><h3 id="a-朴素贝叶斯算法"><a href="#a-朴素贝叶斯算法" class="headerlink" title="a. 朴素贝叶斯算法"></a>a. 朴素贝叶斯算法</h3><p>结合上面的公式，朴素贝叶斯算法可以清晰地分为两步：</p>
<h4 id="步骤一：学习（训练）"><a href="#步骤一：学习（训练）" class="headerlink" title="步骤一：学习（训练）"></a>步骤一：学习（训练）</h4><p>基于训练数据集 $T$，利用极大似然估计法计算以下参数：</p>
<ol>
<li>估计先验概率：$P(Y&#x3D;c_k)$ for $k&#x3D;1,2,…,K$。</li>
<li>对于每个特征 $j&#x3D;1,2,…,n$，估计条件概率 $P(x^{(j)} &#x3D; a_{jl} | Y&#x3D;c_k)$ for $k&#x3D;1,2,…,K$ and $l&#x3D;1,2,…,S_j$。</li>
</ol>
<h4 id="步骤二：分类（预测）"><a href="#步骤二：分类（预测）" class="headerlink" title="步骤二：分类（预测）"></a>步骤二：分类（预测）</h4><p>对于一个新的实例 $x_{\text{new}} &#x3D; (x_{\text{new}}^{(1)}, x_{\text{new}}^{(2)}, …, x_{\text{new}}^{(n)})$，计算所有类别的后验概率（的分子部分）：<br>$$P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x_{\text{new}}^{(j)} | Y&#x3D;c_k), \quad k&#x3D;1,2,…,K$$<br>确定实例 $x_{\text{new}}$ 的类别：<br>$$\hat{y} &#x3D; \arg \max_{c_k} \left[ P(Y&#x3D;c_k) \prod_{j&#x3D;1}^{n} P(x_{\text{new}}^{(j)} | Y&#x3D;c_k) \right]$$</p>
<h3 id="b-贝叶斯估计与平滑技术"><a href="#b-贝叶斯估计与平滑技术" class="headerlink" title="b. 贝叶斯估计与平滑技术"></a>b. 贝叶斯估计与平滑技术</h3><p>极大似然估计有一个显著的缺陷：如果训练集中某个特征值和某个类别的组合从未出现过，那么其条件概率的估计值会为0。这会导致一个问题，在预测时，只要有一个特征的条件概率为0，无论其他特征多么强地指向某个类别，整个连乘的结果都会变成0，从而影响分类的准确性。</p>
<p>例如，在垃圾邮件分类中，训练集里“彩票”这个词从未在正常邮件中出现过（即 $P(\text{“彩票”} | Y&#x3D;\text{正常}) &#x3D; 0$），那么任何包含“彩票”这个词的邮件都会被直接判为垃圾邮件，这显然是不合理的。</p>
<p>为了解决这个问题，我们引入<strong>贝叶斯估计</strong>（或称为<strong>平滑技术</strong>）。最常用的方法是<strong>拉普拉斯平滑（Laplace Smoothing）</strong>。</p>
<ul>
<li><p><strong>先验概率的贝叶斯估计</strong>：<br>  $$P_{\lambda}(Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k) + \lambda}{N + K \lambda}$$<br>  当 $\lambda&#x3D;1$ 时，称为拉普拉斯平滑。$K$ 是类别数量。</p>
</li>
<li><p><strong>条件概率的贝叶斯估计</strong>：<br>  $$P_{\lambda}(x^{(j)} &#x3D; a_{jl} | Y&#x3D;c_k) &#x3D; \frac{\sum_{i&#x3D;1}^{N} I(x_i^{(j)} &#x3D; a_{jl}, y_i &#x3D; c_k) + \lambda}{\sum_{i&#x3D;1}^{N} I(y_i &#x3D; c_k) + S_j \lambda}$$<br>  其中 $\lambda \geq 0$。当 $\lambda&#x3D;0$ 时，就是极大似然估计。当 $\lambda&#x3D;1$ 时，即为拉普拉斯平滑。$S_j$ 是第 $j$ 个特征可能取值的个数。</p>
</li>
</ul>
<p>拉普拉斯平滑等价于给每个特征的计数加上一个小的正数 $\lambda$（通常是1），从而避免了零概率问题。在样本数量足够大时，先验概率和条件概率的先验（即加上的 $\lambda$）的影响会变得微乎其微，估计值会逐渐趋近于实际的极大似然估计值。</p>
<p><strong>总结</strong>：朴素贝叶斯法因其简单、高效且在某些领域（尤其是文本分析）效果出色而经久不衰。其核心在于通过条件独立性假设简化联合概率的估计，并利用贝叶斯定理实现分类。</p>
<hr>
<h2 id="四-代码实践"><a href="#四-代码实践" class="headerlink" title="四. 代码实践"></a>四. 代码实践</h2><pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay

# 1. 加载数据
iris = load_iris()
X, y = iris.data, iris.target

# 2. 划分训练集与测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 3. 使用朴素贝叶斯分类器
model = GaussianNB()
model.fit(X_train, y_train)

# 4. 预测
y_pred = model.predict(X_test)

# 5. 打印准确率
print(&quot;测试集准确率:&quot;, accuracy_score(y_test, y_pred))

# 6. 可视化部分一：降维到二维并绘制散点图
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(10, 5))

# 原始类别分布
plt.subplot(1, 2, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=&quot;viridis&quot;, edgecolor=&quot;k&quot;)
plt.title(&quot;Iris 原始类别分布 (PCA 2D)&quot;)
plt.xlabel(&quot;PCA1&quot;)
plt.ylabel(&quot;PCA2&quot;)

# 预测类别分布
y_all_pred = model.predict(X)
plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_all_pred, cmap=&quot;viridis&quot;, edgecolor=&quot;k&quot;)
plt.title(&quot;Naive Bayes 预测结果 (PCA 2D)&quot;)
plt.xlabel(&quot;PCA1&quot;)
plt.ylabel(&quot;PCA2&quot;)

plt.tight_layout()
plt.show()

# 7. 可视化部分二：混淆矩阵
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=&quot;Blues&quot;)
plt.title(&quot;混淆矩阵&quot;)
plt.show()
</code></pre>
<p>可视化结果</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509211813850.png" alt="image-20250921181339683"></p>
<p> Gaussian Naive Bayes 在鸢尾花数据集上的表现比较好，超过九成样本预测正确。考虑到该数据集是经典的线性可分数据集，这个结果符合预期。</p>
<h3 id="PCA-二维可视化"><a href="#PCA-二维可视化" class="headerlink" title="PCA 二维可视化"></a>PCA 二维可视化</h3><ul>
<li>**左图 **：展示真实标签在降维后的二维空间的分布。紫色、青色、黄色分别代表三种鸢尾花。可以看到三个类别在二维投影下有一定分离，但中间区域存在重叠。</li>
<li>**右图 **：展示模型预测的类别分布。整体形状与左图接近，说明大多数样本被正确分类；但在类别 1 和类别 2 之间，部分点分布混杂，模型容易混淆。</li>
</ul>
<hr>
<h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><ul>
<li><p>第一类 (Setosa，标签 0)：<strong>15 个样本全部预测正确</strong>，说明 Naive Bayes 对这一类的识别非常准确。</p>
</li>
<li><p>第二类 (Versicolor，标签 1)：<strong>14 个预测正确，1 个被误判为类别 2</strong>。</p>
</li>
<li><p>第三类 (Virginica，标签 2)：<strong>12 个预测正确，3 个被误判为类别 1</strong>。</p>
<p>主要错误集中在 <strong>类别 1 和类别 2 之间</strong>，这与 PCA 散点图中它们的重叠区域相吻合。</p>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #03a9f4">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/21/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/17/K%E8%BF%91%E9%82%BB%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">K近邻算法：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/17
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />



<h1 id="K近邻算法：从理论到实践"><a href="#K近邻算法：从理论到实践" class="headerlink" title="K近邻算法：从理论到实践"></a>K近邻算法：从理论到实践</h1><h2 id="1-核心思想"><a href="#1-核心思想" class="headerlink" title="1. 核心思想"></a>1. 核心思想</h2><p>K近邻（KNN）是一种基于实例的监督学习方法。其基本思想是：<br> <strong>对于一个待分类样本，根据训练集中与其“距离”最近的 kk 个邻居的类别，通过投票或加权投票的方式决定该样本的类别。</strong></p>
<p>数学表达：<br> 设训练集为</p>
<p>$${D} &#x3D; { (x_1,y_1), (x_2,y_2), \dots, (x_n,y_n) }, \quad x_i \in \mathbb{R}^d, ; y_i \in {1,2,\dots,C}$$</p>
<p>给定测试样本x，找到其最近的 kk 个邻居集合${N}_k(x)$。<br> 预测类别为：</p>
<p>$$\hat{y}(x) &#x3D; \arg\max_{c \in {1,\dots,C}} \sum_{(x_i,y_i) \in \mathcal{N}_k(x)} \mathbf{1}(y_i &#x3D; c)$$</p>
<p>其中，${1}(\cdot)$ 是指示函数。</p>
<p>如果采用加权投票（考虑距离远近），则为：</p>
<p>$$\hat{y}(x) &#x3D; \arg\max_{c \in {1,\dots,C}} \sum_{(x_i,y_i) \in \mathcal{N}_k(x)} \frac{1}{|x - x_i|} \cdot \mathbf{1}(y_i &#x3D; c)$$</p>
<hr>
<h2 id="2-距离度量"><a href="#2-距离度量" class="headerlink" title="2. 距离度量"></a>2. 距离度量</h2><p>KNN 依赖距离来衡量样本相似度。常见的度量方式有：</p>
<ul>
<li>欧氏距离：</li>
</ul>
<p>$$d(x_i, x_j) &#x3D; \sqrt{\sum_{l&#x3D;1}^d (x_i^{(l)} - x_j^{(l)})^2}$$</p>
<ul>
<li>曼哈顿距离：</li>
</ul>
<p>$$d(x_i, x_j) &#x3D; \sum_{l&#x3D;1}^d |x_i^{(l)} - x_j^{(l)}|$$</p>
<ul>
<li>闵可夫斯基距离（推广形式）：</li>
</ul>
<p>$$d(x_i, x_j) &#x3D; \left( \sum_{l&#x3D;1}^d |x_i^{(l)} - x_j^{(l)}|^p \right)^{1&#x2F;p}$$</p>
<hr>
<h2 id="3-k的选择与误差分析"><a href="#3-k的选择与误差分析" class="headerlink" title="3. k的选择与误差分析"></a>3. k的选择与误差分析</h2><p>KNN 的性能对 k 值选择敏感，体现了 <strong>近似误差</strong> 与 <strong>估计误差</strong> 的权衡。</p>
<h3 id="3-1-近似误差"><a href="#3-1-近似误差" class="headerlink" title="3.1 近似误差"></a>3.1 近似误差</h3><ul>
<li>定义：模型表达能力不足，导致预测结果无法逼近真实分布。</li>
<li><strong>k 较大时</strong>：决策边界过于平滑，难以捕捉复杂模式 → <strong>近似误差大</strong>。</li>
<li><strong>k 较小时</strong>：决策边界灵活，可以更好地拟合真实模式 → <strong>近似误差小</strong>。</li>
</ul>
<p>数学上，假设真实函数为 f(x)，KNN 的期望预测为：</p>
<p>$$\hat{f}(x) &#x3D; \mathbb{E}_{\mathcal{D}}[\hat{y}(x)]$$</p>
<p>则近似误差为：</p>
<p>$$\text{Bias}^2(x) &#x3D; \big( \mathbb{E}_{\mathcal{D}}[\hat{y}(x)] - f(x) \big)^2$$</p>
<h3 id="3-2-估计误差"><a href="#3-2-估计误差" class="headerlink" title="3.2 估计误差"></a>3.2 估计误差</h3><ul>
<li>定义：模型对有限训练数据过于依赖，泛化性差，导致预测不稳定。</li>
<li><strong>k 较小时</strong>：极易受噪声点影响，估计误差大。</li>
<li><strong>k 较大时</strong>：结果受单个点波动影响小，估计误差小。</li>
</ul>
<p>其数学形式为：</p>
<p>$$\text{Var}(x) &#x3D; \mathbb{E}<em>{\mathcal{D}}\big[(\hat{y}(x) - \mathbb{E}</em>{\mathcal{D}}[\hat{y}(x)])^2\big]$$</p>
<h3 id="3-3-总误差"><a href="#3-3-总误差" class="headerlink" title="3.3 总误差"></a>3.3 总误差</h3><p>$$text{MSE}(x) &#x3D; \text{Bias}^2(x) + \text{Var}(x) + \sigma^2$$</p>
<p>其中，$\sigma^2$ 是不可约误差。<br> 因此，选择合适的 k 值非常重要。</p>
<hr>
<h2 id="4-kd树的构造与搜索"><a href="#4-kd树的构造与搜索" class="headerlink" title="4. kd树的构造与搜索"></a>4. kd树的构造与搜索</h2><p>由于 KNN 需要计算测试点与所有训练点的距离，时间复杂度为O(n)。为了加速，可以用 <strong>kd树</strong>进行近邻搜索。</p>
<h3 id="4-1-kd树的构造"><a href="#4-1-kd树的构造" class="headerlink" title="4.1 kd树的构造"></a>4.1 kd树的构造</h3><ul>
<li>kd树是一种对数据进行递归二分的空间划分结构。</li>
<li>每次选择一个维度（通常是方差最大的维度），按照该维度的中位数划分数据。</li>
<li>构造过程：<ol>
<li>从根节点开始，选择一个维度作为切分轴；</li>
<li>找到该维度的中位数，作为节点存储值；</li>
<li>左子树存储小于该值的样本，右子树存储大于该值的样本；</li>
<li>递归进行直到样本数过少或树深度达到限制。</li>
</ol>
</li>
</ul>
<p><strong>伪代码：</strong></p>
<blockquote>
<p>function build_kd_tree(points, depth):<br>    if points is empty:<br>        return None<br>    axis &#x3D; depth mod d<br>    sort points by axis<br>    median &#x3D; len(points) &#x2F;&#x2F; 2<br>    node &#x3D; new Node(points[median])<br>    node.left &#x3D; build_kd_tree(points[:median], depth+1)<br>    node.right &#x3D; build_kd_tree(points[median+1:], depth+1)<br>    return node</p>
</blockquote>
<hr>
<h3 id="4-2-kd树的搜索"><a href="#4-2-kd树的搜索" class="headerlink" title="4.2 kd树的搜索"></a>4.2 kd树的搜索</h3><p>kd树搜索遵循“回溯+剪枝”原则：</p>
<ol>
<li>从根节点开始，递归到叶子节点，找到测试点所属的区域；</li>
<li>以该叶子节点为“当前最近邻”；</li>
<li>回溯检查父节点和另一子树，若另一子树中可能存在更近邻，则递归进入；</li>
<li>维护一个大小为 kk 的优先队列，存储当前最近的 kk 个邻居；</li>
<li>搜索结束时队列中的点即为近邻结果。</li>
</ol>
<p><strong>伪代码：</strong></p>
<blockquote>
<p>function knn_search(node, target, k, depth):<br>    if node is None:<br>        return<br>    axis &#x3D; depth mod d<br>    if target[axis] &lt; node.point[axis]:<br>        next &#x3D; node.left<br>        other &#x3D; node.right<br>    else:<br>        next &#x3D; node.right<br>        other &#x3D; node.left</p>
</blockquote>
<pre><code>function knn_search(node, target, k, depth):
    if node is None:
        return
    axis = depth mod d
    if target[axis] &lt; node.point[axis]:
        next = node.left
        other = node.right
    else:
        next = node.right
        other = node.left
    
    knn_search(next, target, k, depth+1)
    update priority queue with node.point
    
    if |target[axis] - node.point[axis]| &lt; current_max_distance_in_queue:
        knn_search(other, target, k, depth+1)
</code></pre>
<hr>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ul>
<li><strong>核心思想</strong>：KNN 通过寻找最近的 kk 个邻居来分类或回归。</li>
<li><strong>k 的选择</strong>：小 kk → 近似误差小、估计误差大（过拟合）；大 kk → 近似误差大、估计误差小（欠拟合）。</li>
<li><strong>kd树</strong>：通过空间划分加速近邻搜索，提升算法效率。</li>
</ul>
<p>最终，KNN 的关键在于 <strong>合适的 k 值选择</strong> 和 <strong>高效的搜索结构</strong>。</p>
<hr>
<h3 id="6-K近邻用于Iris数据集分类"><a href="#6-K近邻用于Iris数据集分类" class="headerlink" title="6. K近邻用于Iris数据集分类"></a>6. K近邻用于Iris数据集分类</h3><h4 id="6-1加载数据"><a href="#6-1加载数据" class="headerlink" title="6.1加载数据"></a>6.1加载数据</h4><pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris(as_frame=True)
X = iris.data[[&quot;sepal length (cm)&quot;, &quot;sepal width (cm)&quot;]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)
</code></pre>
<p>鸢尾花数据集，<code>as_frame=True</code> 表示返回 <strong>pandas DataFrame</strong> 而不是 numpy 数组，方便做列选择。</p>
<p>这个数据集有 <strong>150 条样本</strong>，<strong>4 个特征</strong>：<code>sepal length</code>, <code>sepal width</code>, <code>petal length</code>, <code>petal width</code>。目标变量 <code>target</code> 有三类 (0&#x3D;setosa, 1&#x3D;versicolor, 2&#x3D;virginica)。</p>
<h4 id="6-2加载模型并可视化"><a href="#6-2加载模型并可视化" class="headerlink" title="6.2加载模型并可视化"></a>6.2加载模型并可视化</h4><pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import DecisionBoundaryDisplay
import pandas as pd
import time

# 1. 载入数据
iris = load_iris(as_frame=True)
X = iris.data[[&quot;sepal length (cm)&quot;, &quot;sepal width (cm)&quot;]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, random_state=0
)

# 2. 构建 pipeline：标准化 + KNN
clf = Pipeline(
    steps=[
        (&quot;scaler&quot;, StandardScaler()),
        (&quot;knn&quot;, KNeighborsClassifier(n_neighbors=11))
    ]
)

# 3. 不同的 weights 和 algorithm 组合
weights_list = [&quot;uniform&quot;, &quot;distance&quot;]
algorithms = [&quot;auto&quot;, &quot;ball_tree&quot;, &quot;kd_tree&quot;]


# 定义结果存储表
results = []


# 4. 画图：每行一个 weights，每列一个 algorithm
fig, axs = plt.subplots(
    nrows=len(weights_list), ncols=len(algorithms), figsize=(18, 10)
)

for i, weights in enumerate(weights_list):
    for j, algo in enumerate(algorithms):
        ax = axs[i, j]

        # 设置参数并拟合
        start_train = time.time()
        clf.set_params(knn__weights=weights, knn__algorithm=algo).fit(X_train, y_train)
        end_train = time.time()
        
        start_pred = time.time()
        clf.predict(X_test)
        end_pred = time.time()
        
        acc = clf.score(X_test, y_test)

        results.append(&#123;
            &quot;weights&quot;: weights,
            &quot;algorithm&quot;: algo,
            &quot;accuracy&quot;: acc,
            &quot;train_time (s)&quot;: end_train - start_train,
            &quot;predict_time (s)&quot;: end_pred - start_pred
        &#125;)
        # 决策边界
        disp = DecisionBoundaryDisplay.from_estimator(
            clf,
            X_test,
            response_method=&quot;predict&quot;,
            plot_method=&quot;pcolormesh&quot;,
            xlabel=iris.feature_names[0],
            ylabel=iris.feature_names[1],
            shading=&quot;auto&quot;,
            alpha=0.5,
            ax=ax,
        )

        # 训练样本点
        scatter = disp.ax_.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors=&quot;k&quot;)

        # 图例
        disp.ax_.legend(
            scatter.legend_elements()[0],
            iris.target_names,
            loc=&quot;lower left&quot;,
            title=&quot;Classes&quot;,
        )

        # 子图标题
        ax.set_title(
            f&quot;k=&#123;clf[-1].n_neighbors&#125;, weights=&#123;weights&#125;, algo=&#123;algo&#125;&quot;
        )

plt.tight_layout()
plt.show()
df_results = pd.DataFrame(results)
print(df_results)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509171831492.png" alt="image-20250917183120303"></p>
<pre><code> weights  algorithm  accuracy  train_time (s)  predict_time (s)
0   uniform       auto  0.710526        0.003293          0.004401
1   uniform  ball_tree  0.710526        0.004864          0.006618
2   uniform    kd_tree  0.710526        0.003537          0.004044
3  distance       auto  0.631579        0.003269          0.001961
4  distance  ball_tree  0.631579        0.003211          0.001694
5  distance    kd_tree  0.631579        0.003055          0.001578
</code></pre>
<p><strong>不同 algorithm 的表现</strong></p>
<ul>
<li><code>auto</code>、<code>ball_tree</code>、<code>kd_tree</code> 在相同权重下的 <strong>准确率完全一致，训练预测速度不同</strong>，这说明 <strong>搜索算法仅影响计算效率，不会改变最终分类结果</strong>。</li>
<li>这和理论一致：算法只是用不同的数据结构加速邻居查找，不会影响邻居集合本身。</li>
</ul>
<p><strong>不同 weights 的表现</strong></p>
<ul>
<li><code>uniform</code> 权重下，测试集准确率为 <strong>71.05%</strong>；</li>
<li><code>distance</code> 权重下，测试集准确率为 <strong>63.16%</strong>；</li>
<li>在本实验中，<strong>uniform 明显优于 distance</strong>。</li>
<li>这表明在鸢尾花数据的 <strong>前两个特征（花萼长、宽）</strong> 上，等权投票比加权投票更适合。可能原因是：<ul>
<li>特征维度少，距离加权放大了噪声点或边界点的影响；</li>
<li>类别边界本身不完全线性，用距离权重反而削弱了多数邻居的稳定性。</li>
</ul>
</li>
</ul>
<p><strong>结合可视化</strong></p>
<ul>
<li>从决策边界图上可以看到：<ul>
<li><code>uniform</code> 的边界相对平滑，更符合数据整体分布；</li>
<li><code>distance</code> 在交界区域会出现一些不规则边界，可能导致更多误判。</li>
</ul>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #ffa2c4">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/17/K%E8%BF%91%E9%82%BB%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">支持向量机：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/15
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />

<h1 id="支持向量机：从理论到实践"><a href="#支持向量机：从理论到实践" class="headerlink" title="支持向量机：从理论到实践"></a>支持向量机：从理论到实践</h1><h2 id="一。理论概述"><a href="#一。理论概述" class="headerlink" title="一。理论概述"></a>一。理论概述</h2><p>支持向量机（Support Vector Machine, SVM）是机器学习中最强大和最广泛使用的算法之一，尤其在小样本、高维数据的分类任务中表现出色。其核心思想基于统计学习理论中的结构风险最小化原则，通过构建最大间隔超平面来实现分类任务。本文将从数学基础到实际应用，全面深入地解析SVM的工作原理和实现细节。</p>
<h3 id="1-线性可分支持向量机"><a href="#1-线性可分支持向量机" class="headerlink" title="1. 线性可分支持向量机"></a>1. 线性可分支持向量机</h3><h4 id="1-1-基本概念与数学形式"><a href="#1-1-基本概念与数学形式" class="headerlink" title="1.1 基本概念与数学形式"></a>1.1 基本概念与数学形式</h4><p>对于完全线性可分的数据集，存在无数个超平面能够将两类样本完全分开。SVM通过<strong>间隔最大化</strong>原则从中选择唯一的最优超平面，该超平面不仅能够正确分类所有训练样本，而且具有最好的泛化能力。</p>
<p>超平面的数学表示为：<br>$$w^T x + b &#x3D; 0$$<br>其中$w \in \mathbb{R}^n$是超平面的法向量，决定了超平面的方向；$b \in \mathbb{R}$是位移项，决定了超平面与原点的距离。</p>
<p>样本点$x_i$到超平面的几何距离为：<br>$$d_i &#x3D; \frac{|w^T x_i + b|}{|w|}$$</p>
<h4 id="1-2-函数间隔与几何间隔"><a href="#1-2-函数间隔与几何间隔" class="headerlink" title="1.2 函数间隔与几何间隔"></a>1.2 函数间隔与几何间隔</h4><p><strong>函数间隔</strong>的概念引入了类别信息：<br>$$\hat{\gamma}_i &#x3D; y_i(w^T x_i + b)$$<br>其中$y_i \in {-1, 1}$是样本的类别标签。函数间隔的符号表示分类的正确性，绝对值表示分类的确信度。</p>
<p>整个数据集的函数间隔定义为所有样本中函数间隔的最小值：<br>$$\hat{\gamma} &#x3D; \min_{i&#x3D;1,\dots,N} \hat{\gamma}_i$$</p>
<p>然而，函数间隔存在一个显著问题：对$w$和$b$进行等比例缩放时，超平面不变但函数间隔会改变。为解决这个问题，我们引入<strong>几何间隔</strong>：</p>
<p>$$\gamma_i &#x3D; \frac{\hat{\gamma}_i}{|w|} &#x3D; \frac{y_i(w^T x_i + b)}{|w|}$$</p>
<p>几何间隔表示样本点到超平面的真实欧几里得距离，具有缩放不变性，能够真实反映分类的确信度。</p>
<h4 id="1-3-间隔最大化与优化问题"><a href="#1-3-间隔最大化与优化问题" class="headerlink" title="1.3 间隔最大化与优化问题"></a>1.3 间隔最大化与优化问题</h4><p>SVM的核心目标是找到最大几何间隔的超平面，这可以表述为以下优化问题：</p>
<p>$$\max_{w,b} \gamma$$<br>$$\text{s.t. } \frac{y_i(w^T x_i + b)}{|w|} \geq \gamma, \quad i&#x3D;1,\dots,N$$</p>
<p>通过令$\hat{\gamma} &#x3D; 1$（这可以通过调整$w$和$b$的尺度实现），问题转化为等价的约束优化问题：</p>
<p>$$\min_{w,b} \frac{1}{2} |w|^2$$<br>$$\text{s.t. } y_i(w^T x_i + b) \geq 1, \quad i&#x3D;1,\dots,N$$</p>
<p>这是一个典型的<strong>凸二次规划问题</strong>，具有全局最优解。目标函数中的$\frac{1}{2}$是为了后续求导方便而添加的系数，不影响优化结果。</p>
<h4 id="1-4-拉格朗日对偶理论与求解"><a href="#1-4-拉格朗日对偶理论与求解" class="headerlink" title="1.4 拉格朗日对偶理论与求解"></a>1.4 拉格朗日对偶理论与求解</h4><p>应用拉格朗日乘子法，我们引入拉格朗日乘子$\alpha_i \geq 0$，构造拉格朗日函数：</p>
<p>$$L(w,b,\alpha) &#x3D; \frac{1}{2} |w|^2 - \sum_{i&#x3D;1}^N \alpha_i [y_i(w^T x_i + b) - 1]$$</p>
<p>根据KKT条件，原问题的最优解满足：</p>
<p>$$\nabla_w L &#x3D; w - \sum_{i&#x3D;1}^N \alpha_i y_i x_i &#x3D; 0$$<br>$$\frac{\partial L}{\partial b} &#x3D; -\sum_{i&#x3D;1}^N \alpha_i y_i &#x3D; 0$$<br>$$\alpha_i [y_i(w^T x_i + b) - 1] &#x3D; 0, \quad i&#x3D;1,\dots,N$$</p>
<p>代入拉格朗日函数，得到对偶问题：</p>
<p>$$\max_{\alpha} -\frac{1}{2} \sum_{i&#x3D;1}^N \sum_{j&#x3D;1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i&#x3D;1}^N \alpha_i$$<br>$$\text{s.t. } \alpha_i \geq 0, \quad \sum_{i&#x3D;1}^N \alpha_i y_i &#x3D; 0$$</p>
<h4 id="1-5-支持向量与决策函数"><a href="#1-5-支持向量与决策函数" class="headerlink" title="1.5 支持向量与决策函数"></a>1.5 支持向量与决策函数</h4><p>从KKT条件中的互补松弛条件$\alpha_i [y_i(w^T x_i + b) - 1] &#x3D; 0$可知：</p>
<ul>
<li>当$\alpha_i &#x3D; 0$时，对应样本不是支持向量，对决策边界没有影响</li>
<li>当$\alpha_i &gt; 0$时，必有$y_i(w^T x_i + b) &#x3D; 1$，对应样本是<strong>支持向量</strong></li>
</ul>
<p>支持向量是位于间隔边界上的样本点，它们决定了最终的超平面。这一特性使得SVM的解具有稀疏性，仅依赖于少数支持向量。</p>
<p>最终的决策函数为：<br>$$f(x) &#x3D; \text{sign} \left( \sum_{i&#x3D;1}^N \alpha_i y_i x_i^T x + b \right)$$</p>
<p>其中$b$可以通过任意支持向量计算得到：$b &#x3D; y_i - w^T x_i$（对于满足$0 &lt; \alpha_i$的样本）。</p>
<hr>
<h3 id="2-近似线性可分数据（软间隔SVM）"><a href="#2-近似线性可分数据（软间隔SVM）" class="headerlink" title="2. 近似线性可分数据（软间隔SVM）"></a>2. 近似线性可分数据（软间隔SVM）</h3><h4 id="2-1-松弛变量与软间隔概念"><a href="#2-1-松弛变量与软间隔概念" class="headerlink" title="2.1 松弛变量与软间隔概念"></a>2.1 松弛变量与软间隔概念</h4><p>在实际应用中，数据很少是完美线性可分的，可能存在噪声或异常点。软间隔SVM通过引入<strong>松弛变量</strong>$\xi_i \geq 0$，允许一些样本被错误分类，从而提高模型的鲁棒性。</p>
<p>优化问题变为：<br>$$\min_{w,b,\xi} \frac{1}{2} |w|^2 + C \sum_{i&#x3D;1}^N \xi_i$$<br>$$\text{s.t. } y_i(w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i&#x3D;1,\dots,N$$</p>
<p>其中$C &gt; 0$是正则化参数，控制误分类惩罚与间隔大小之间的平衡：</p>
<ul>
<li>$C$值较大时，误分类惩罚重，间隔较小，可能过拟合</li>
<li>$C$值较小时，误分类惩罚轻，间隔较大，可能欠拟合</li>
</ul>
<h4 id="2-2-软间隔的对偶问题与支持向量分类"><a href="#2-2-软间隔的对偶问题与支持向量分类" class="headerlink" title="2.2 软间隔的对偶问题与支持向量分类"></a>2.2 软间隔的对偶问题与支持向量分类</h4><p>软间隔SVM的对偶问题与硬间隔形式相似：</p>
<p>$$\max_{\alpha} -\frac{1}{2} \sum_{i&#x3D;1}^N \sum_{j&#x3D;1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i&#x3D;1}^N \alpha_i$$<br>$$\text{s.t. } 0 \leq \alpha_i \leq C, \quad \sum_{i&#x3D;1}^N \alpha_i y_i &#x3D; 0$$</p>
<p>KKT条件扩展为：<br>$$\alpha_i [y_i(w^T x_i + b) - 1 + \xi_i] &#x3D; 0$$<br>$$(C - \alpha_i) \xi_i &#x3D; 0$$</p>
<p>支持向量分为三类：</p>
<ol>
<li>$\alpha_i &#x3D; 0$：正确分类的非支持向量，对决策边界没有影响</li>
<li>$0 &lt; \alpha_i &lt; C$：位于间隔边界上的支持向量，满足$y_i(w^T x_i + b) &#x3D; 1$</li>
<li>$\alpha_i &#x3D; C$：被错误分类的支持向量或落在间隔内的样本，满足$\xi_i &gt; 0$</li>
</ol>
<hr>
<h3 id="3-非线性支持向量机与核方法"><a href="#3-非线性支持向量机与核方法" class="headerlink" title="3. 非线性支持向量机与核方法"></a>3. 非线性支持向量机与核方法</h3><h4 id="3-1-核技巧的数学原理"><a href="#3-1-核技巧的数学原理" class="headerlink" title="3.1 核技巧的数学原理"></a>3.1 核技巧的数学原理</h4><p>当数据在原始特征空间中线性不可分时，可以通过非线性映射$\phi: \mathbb{R}^d \to \mathcal{H}$将数据映射到高维特征空间$\mathcal{H}$，在其中数据变得线性可分。</p>
<p>在高维空间中的优化问题变为：<br>$$\min_{w,b} \frac{1}{2} |w|^2 + C \sum_{i&#x3D;1}^N \xi_i$$<br>$$\text{s.t. } y_i(w^T \phi(x_i) + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$</p>
<p>对偶问题为：<br>$$\max_{\alpha} -\frac{1}{2} \sum_{i&#x3D;1}^N \sum_{j&#x3D;1}^N \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j) + \sum_{i&#x3D;1}^N \alpha_i$$</p>
<p>直接计算$\phi(x_i)^T \phi(x_j)$可能非常困难（甚至不可能），因此引入<strong>核函数</strong>：<br>$$K(x_i, x_j) &#x3D; \phi(x_i)^T \phi(x_j)$$</p>
<h4 id="3-2-常用核函数及其特性"><a href="#3-2-常用核函数及其特性" class="headerlink" title="3.2 常用核函数及其特性"></a>3.2 常用核函数及其特性</h4><ol>
<li><p><strong>线性核</strong>：$K(x_i, x_j) &#x3D; x_i^T x_j$</p>
<ul>
<li>参数少，速度快，适用于线性可分情况</li>
<li>简单，可解释性强</li>
</ul>
</li>
<li><p><strong>多项式核</strong>：$K(x_i, x_j) &#x3D; (x_i^T x_j + c)^d$</p>
<ul>
<li>参数$d$控制映射后的空间维度</li>
<li>当$d$较大时计算可能不稳定</li>
</ul>
</li>
<li><p><strong>高斯径向基核（RBF）</strong>：$K(x_i, x_j) &#x3D; \exp(-\gamma |x_i - x_j|^2)$</p>
<ul>
<li>应用最广泛的核函数，具有很强的非线性映射能力</li>
<li>参数$\gamma$控制高斯函数的宽度，影响模型的复杂度</li>
</ul>
</li>
<li><p><strong>Sigmoid核</strong>：$K(x_i, x_j) &#x3D; \tanh(\kappa x_i^T x_j + \theta)$</p>
<ul>
<li>来源于神经网络理论，在某些特定问题上表现良好</li>
<li>不是对所有参数都满足Mercer条件</li>
</ul>
</li>
</ol>
<h4 id="3-3-核函数的选择与模型选择"><a href="#3-3-核函数的选择与模型选择" class="headerlink" title="3.3 核函数的选择与模型选择"></a>3.3 核函数的选择与模型选择</h4><p>核函数的选择依赖于具体问题和数据特性：</p>
<ul>
<li>文本分类：通常使用线性核，因为文本数据往往已经是高维的</li>
<li>图像识别：常用RBF核或多项式核，可以捕捉像素间的复杂关系</li>
<li>生物信息学：RBF核表现优异，适用于基因序列等复杂数据</li>
</ul>
<p>模型选择涉及参数调优，常用交叉验证来确定最优的$C$和核参数（如RBF核的$\gamma$）。</p>
<h4 id="3-4-Mercer定理与核函数有效性"><a href="#3-4-Mercer定理与核函数有效性" class="headerlink" title="3.4 Mercer定理与核函数有效性"></a>3.4 Mercer定理与核函数有效性</h4><p>核函数必须满足Mercer条件：对任意函数$g(x)$满足$\int g(x)^2 dx &lt; \infty$，有：<br>$$\iint K(x, y) g(x) g(y) dx dy \geq 0$$</p>
<p>这保证了核矩阵$K &#x3D; [K(x_i, x_j)]$是半正定的，对应的优化问题是凸的，有全局最优解。</p>
<hr>
<h3 id="4-支持向量机的扩展与变体"><a href="#4-支持向量机的扩展与变体" class="headerlink" title="4. 支持向量机的扩展与变体"></a>4. 支持向量机的扩展与变体</h3><h4 id="4-1-支持向量回归（SVR）"><a href="#4-1-支持向量回归（SVR）" class="headerlink" title="4.1 支持向量回归（SVR）"></a>4.1 支持向量回归（SVR）</h4><p>支持向量机也可以用于回归任务，称为支持向量回归。其基本思想是：寻找一个函数$f(x) &#x3D; w^T \phi(x) + b$，使得$f(x)$与$y$的偏差不超过$\epsilon$，同时保持函数尽量平坦。</p>
<p>优化问题为：<br>$$\min_{w,b} \frac{1}{2} |w|^2 + C \sum_{i&#x3D;1}^N (\xi_i + \xi_i^*)$$</p>
<p>$$\text{s.t. } \begin{cases}<br>y_i - w^T \phi(x_i) - b \leq \epsilon + \xi_i \<br>w^T \phi(x_i) + b - y_i \leq \epsilon + \xi_i^* \<br>\xi_i, \xi_i^* \geq 0<br>\end{cases}$$</p>
<h4 id="4-2-多类支持向量机"><a href="#4-2-多类支持向量机" class="headerlink" title="4.2 多类支持向量机"></a>4.2 多类支持向量机</h4><p>SVM本质上是二分类器，扩展多类分类的方法主要有：</p>
<ol>
<li><strong>一对多（One-vs-Rest）</strong>：为每个类别训练一个二分类器</li>
<li><strong>一对一（One-vs-One）</strong>：为每两个类别训练一个二分类器</li>
<li><strong>多类SVM</strong>：直接修改优化目标，考虑所有类别</li>
</ol>
<h4 id="4-3-结构化SVM"><a href="#4-3-结构化SVM" class="headerlink" title="4.3 结构化SVM"></a>4.3 结构化SVM</h4><p>用于结构化输出问题，如序列标注、解析树构建等，通过定义适当的损失函数和特征映射来处理复杂的输出结构。</p>
<hr>
<h3 id="5-支持向量机的实践考虑"><a href="#5-支持向量机的实践考虑" class="headerlink" title="5. 支持向量机的实践考虑"></a>5. 支持向量机的实践考虑</h3><h4 id="5-1-数据预处理与特征缩放"><a href="#5-1-数据预处理与特征缩放" class="headerlink" title="5.1 数据预处理与特征缩放"></a>5.1 数据预处理与特征缩放</h4><p>SVM对数据缩放敏感，特别是使用基于距离的核函数（如RBF核）时。常见的预处理方法：</p>
<ul>
<li>标准化：将特征缩放到均值为0，方差为1</li>
<li>归一化：将特征缩放到[0,1]或[-1,1]区间</li>
</ul>
<h4 id="5-2-计算效率与大规模数据处理"><a href="#5-2-计算效率与大规模数据处理" class="headerlink" title="5.2 计算效率与大规模数据处理"></a>5.2 计算效率与大规模数据处理</h4><p>传统SVM训练算法的时间复杂度约为$O(n^3)$，空间复杂度约为$O(n^2)$，难以处理大规模数据集。改进方法包括：</p>
<ul>
<li>分解算法（如SMO）</li>
<li>随机梯度下降</li>
<li>近似核方法</li>
<li>分布式计算</li>
</ul>
<h4 id="5-3-模型解释性与特征重要性"><a href="#5-3-模型解释性与特征重要性" class="headerlink" title="5.3 模型解释性与特征重要性"></a>5.3 模型解释性与特征重要性</h4><p>虽然核SVM具有良好的分类性能，但模型解释性较差。提高解释性的方法：</p>
<ul>
<li>使用线性核或可解释性强的核函数</li>
<li>分析支持向量的权重</li>
<li>使用模型无关的解释方法（如LIME、SHAP）</li>
</ul>
<h3 id="6-支持向量机的局限性与挑战"><a href="#6-支持向量机的局限性与挑战" class="headerlink" title="6. 支持向量机的局限性与挑战"></a>6. 支持向量机的局限性与挑战</h3><ul>
<li><strong>计算复杂度</strong>：对于大规模数据集，训练时间较长，内存消耗大，限制了其在实际中的应用。</li>
<li><strong>核函数选择</strong>：核函数及其参数的选择很大程度上依赖于经验和实验，缺乏系统的理论指导。</li>
<li><strong>概率输出</strong>：标准SVM不直接提供概率输出，需要通过 Platt scaling 等额外方法进行校准。</li>
<li><strong>多类分类</strong>：SVM本质上是二分类器，多类扩展需要额外的策略，增加了复杂性。</li>
</ul>
<hr>
<h2 id="二。SVM模型应用于人脸分类"><a href="#二。SVM模型应用于人脸分类" class="headerlink" title="二。SVM模型应用于人脸分类"></a>二。SVM模型应用于人脸分类</h2><h3 id="1-数据加载与预处理"><a href="#1-数据加载与预处理" class="headerlink" title="1. 数据加载与预处理"></a>1. 数据加载与预处理</h3><pre><code class="language-python">from time import time

import matplotlib.pyplot as plt
from scipy.stats import loguniform

from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from sklearn.metrics import ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
</code></pre>
<pre><code class="language-python">people_faces = fetch_lfw_peopletch()
# 从 LFW（Labeled Faces in the Wild）数据集中下载人脸图片
# min_faces_per_person=70 → 只保留至少有 70 张照片的人物
# resize=0.4 → 将图片缩小到原来的 40%，降低计算量
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# 获取数据集的基本形状信息
n_samples, h, w = lfw_people.images.shape  # 样本数、高度、宽度

# 机器学习使用拉平后的像素数据（忽略像素的空间位置信息）
X = lfw_people.data  # 每张图片展平成一维向量
n_features = X.shape[1]  # 特征数（像素个数）

# 标签（人物 ID）
y = lfw_people.target
target_names = lfw_people.target_names  # 人物姓名
n_classes = target_names.shape[0]  # 类别数

# 打印数据集大小信息
print(&quot;Total dataset size:&quot;)
print(&quot;n_samples: %d&quot; % n_samples)
print(&quot;n_features: %d&quot; % n_features)
print(&quot;n_classes: %d&quot; % n_classes)

结果：
Total dataset size:
n_samples: 1288
n_features: 1850
n_classes: 7
</code></pre>
<p>从LFW（Labeled Faces in the Wild）数据集中加载人脸图像数据，并进行初步预处理。通过设置<code>min_faces_per_person=70</code>筛选出至少有70张图像的人物，确保每个类别有足够的样本；<code>resize=0.4</code>将图像缩小到原尺寸的40%，显著降低计算复杂度。随后，将图像数据展平为一维向量作为特征，提取对应的类别标签和人物名称，输出数据集的基本统计信息（样本数、特征维度和类别数）</p>
<h3 id="2-训练测试划分与数据标准化"><a href="#2-训练测试划分与数据标准化" class="headerlink" title="2. 训练测试划分与数据标准化"></a>2. 训练测试划分与数据标准化</h3><pre><code class="language-python"># 划分训练集和测试集，比例 70% 训练 / 30% 测试
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=2025
)

# 数据标准化（均值为 0，方差为 1），有助于加快收敛并提升模型性能
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # 用训练集拟合并转换
X_test = scaler.transform(X_test)        # 用相同参数转换测试集
</code></pre>
<p>将数据集按7:3比例划分为训练集和测试集。采用<code>StandardScaler</code>对数据进行标准化处理，使每个特征的均值为0、方差为1。这一步骤至关重要，因为SVM对特征尺度敏感，标准化可以加速模型收敛并提升性能。注意测试集使用训练集拟合的缩放参数进行转换，避免数据泄露。</p>
<h3 id="3-主成分分析（PCA）降维"><a href="#3-主成分分析（PCA）降维" class="headerlink" title="3. 主成分分析（PCA）降维"></a>3. 主成分分析（PCA）降维</h3><pre><code class="language-python"># 设定 PCA 要保留的主成分个数，这里是 100 个
n_components = 100

# 记录开始时间，用于计算运行耗时
start = time()

# 创建 PCA 对象并拟合训练数据
# 参数解释：
#   n_components=n_components  -&gt; 保留的主成分数量
#   svd_solver=&quot;randomized&quot;    -&gt; 使用随机化 SVD 算法，加速计算（适合高维数据）
#   whiten=True                -&gt; 白化处理，使得每个主成分的方差为 1（有助于后续分类器性能）
# .fit(X_train) -&gt; 在训练集上拟合 PCA 模型，学习主成分方向
pca = PCA(n_components=n_components, svd_solver=&quot;randomized&quot;, whiten=True).fit(X_train)

# 获取 PCA 学到的主成分（特征向量），并将它们还原成 h×w 的二维图像
# 这些主成分在脸部识别领域被称为 &quot;eigenfaces&quot;（特征脸）
eigenfaces = pca.components_.reshape((n_components, h, w))

# 记录结束时间
end = time()

# 使用训练好的 PCA 模型将训练集投影到主成分空间
# 得到的 X_train_pca 是降维后的特征表示
X_train_pca = pca.transform(X_train)

# 同样地，将测试集投影到主成分空间
X_test_pca = pca.transform(X_test)

# 输出整个 PCA 特征提取过程的耗时
print(&quot;finished in %0.3fs&quot; % (end - start))
</code></pre>
<p>使用PCA对高维图像特征进行降维，保留100个主要成分。设置<code>whiten=True</code>对主成分进行白化处理，使各维度方差归一化，有助于提升后续分类器性能。采用随机化SVD算法（<code>svd_solver=&quot;randomized&quot;</code>）提高计算效率。降维后将训练集和测试集分别投影到主成分空间，得到低维特征表示（<code>X_train_pca</code>和<code>X_test_pca</code>），同时提取特征脸（eigenfaces）用于可视化。</p>
<h3 id="4-支持向量机超参数优化"><a href="#4-支持向量机超参数优化" class="headerlink" title="4. 支持向量机超参数优化"></a>4. 支持向量机超参数优化</h3><pre><code class="language-python"># 记录开始时间
start = time()

# 定义超参数搜索范围 param_grid
# 这里使用的是 loguniform 分布（对数均匀分布），适合搜索跨度很大的超参数
#  - &quot;C&quot; 是 SVM 的正则化参数，控制分类器对训练集的拟合程度
#  - &quot;gamma&quot; 是 RBF 核函数的核宽度参数，影响单个样本的影响范围
#    范围：
#      C: 从 1e3 到 1e5
#      gamma: 从 1e-4 到 1e-1
param_grid = &#123;
    &quot;C&quot;: loguniform(1e3, 1e5),
    &quot;gamma&quot;: loguniform(1e-4, 1e-1),
&#125;

# 创建一个随机搜索交叉验证器 RandomizedSearchCV
#  - SVC(kernel=&quot;rbf&quot;, class_weight=&quot;balanced&quot;) ：使用 RBF 核的支持向量机，类别权重平衡处理
#  - param_distributions=param_grid ：指定要搜索的参数分布
#  - n_iter=10 ：随机采样 10 组参数组合进行测试
clf = RandomizedSearchCV(
    SVC(kernel=&quot;rbf&quot;, class_weight=&quot;balanced&quot;), param_grid, n_iter=10
)

# 在训练集（PCA 降维后的特征）上拟合模型并进行超参数搜索
#  X_train_pca：降维后的训练数据
#  y_train：对应的标签
clf = clf.fit(X_train_pca, y_train)

# 输出搜索过程耗时
print(&quot;finished in %0.3f&quot; % (time() - start))

# 输出搜索得到的最优模型（包含最佳参数 C 和 gamma）
print(clf.best_estimator_)
</code></pre>
<p>使用随机搜索（<code>RandomizedSearchCV</code>）优化SVM的超参数（正则化参数<code>C</code>和RBF核参数<code>gamma</code>）。参数搜索范围设置为对数均匀分布（<code>loguniform</code>），涵盖合理的数值区间。采用带类别权重平衡（<code>class_weight=&quot;balanced&quot;</code>）的RBF核SVM，以处理类别样本量不均衡的问题。通过10次参数采样和交叉验证，寻找最优超参数组合。</p>
<h3 id="5-模型预测与评估"><a href="#5-模型预测与评估" class="headerlink" title="5. 模型预测与评估"></a>5. 模型预测与评估</h3><pre><code class="language-python"># 记录开始时间
start = time()

# 使用训练好的分类器（clf）对测试集特征进行预测
#  X_test_pca 是经过 PCA 降维的测试集数据
#  y_pred 是预测得到的标签
y_pred = clf.predict(X_test_pca)

# 输出预测过程耗时
print(&quot;finished in %0.3fs&quot; % (time() - start))

# 打印分类性能评估报告
# classification_report 会输出：
#   - precision（精确率）
#   - recall（召回率）
#   - f1-score（F1 分数）
#   - support（每个类别的样本数量）
# target_names 用于显示每个类别的真实名称（这里是人物姓名）
print(classification_report(y_test, y_pred, target_names=target_names))

# 绘制混淆矩阵（Confusion Matrix）
# ConfusionMatrixDisplay.from_estimator 会：
#   - 用分类器 clf
#   - 输入测试集数据和真实标签
#   - 自动计算混淆矩阵并绘制
# display_labels=target_names 用于显示真实的类别名称
# xticks_rotation=&quot;vertical&quot; 让横轴标签竖直显示，防止文字重叠
ConfusionMatrixDisplay.from_estimator(
    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation=&quot;vertical&quot;
)

# 自动调整子图布局，避免文字或图像被遮挡
plt.tight_layout()

# 显示绘制好的混淆矩阵图
plt.show()
</code></pre>
<p>使用优化后的SVM模型对测试集进行预测，并输出详细分类报告（包括精确率、召回率、F1分数等指标）和混淆矩阵。混淆矩阵可视化展示了各类别的分类情况。</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509140912767.png" alt="image-20250914091238591"></p>
<h3 id="6-预测结果标题生成与可视化"><a href="#6-预测结果标题生成与可视化" class="headerlink" title="6. 预测结果标题生成与可视化"></a>6. 预测结果标题生成与可视化</h3><pre><code class="language-python">import matplotlib.pyplot as plt

def plot_gallery(images, titles, h, w, n_row=4, n_col=4):
    &quot;&quot;&quot;绘制图像网格（gallery）&quot;&quot;&quot;
    fig, axes = plt.subplots(n_row, n_col, figsize=(1.8 * n_col, 2.4 * n_row))  # 创建子图
    axes = axes.flatten()  # 展平成一维，方便索引
    for i in range(n_row * n_col):
        axes[i].imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)  # 显示灰度图
        axes[i].set_title(titles[i], size=12)  # 设置标题
        axes[i].set_xticks([])  # 去掉横坐标刻度
        axes[i].set_yticks([])  # 去掉纵坐标刻度
    plt.tight_layout()  # 自动调整布局
    plt.show()

</code></pre>
<p>定义<code>plot_gallery</code>函数，用于以网格形式展示多张图像。函数接受图像数据、标题列表和图像尺寸参数，自动创建子图布局，显示灰度图像并设置标题。</p>
<pre><code class="language-python">def title(y_pred, y_test, target_names, i):
    &quot;&quot;&quot;生成第 i 张图片的预测与真实标签标题&quot;&quot;&quot;
    pred_name = target_names[y_pred[i]].rsplit(&quot; &quot;, 1)[-1]  # 预测姓名（取姓氏）
    true_name = target_names[y_test[i]].rsplit(&quot; &quot;, 1)[-1]  # 真实姓名（取姓氏）
    return &quot;predicted: %s\ntrue:      %s&quot; % (pred_name, true_name)

# 生成预测结果标题列表
prediction_titles = [
    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])
]

# 绘制测试集图片及预测结果
plot_gallery(X_test, prediction_titles, h, w)
</code></pre>
<p>根据预测结果和真实标签生成每张测试图像的标题，格式为“预测姓名&#x2F;真实姓名”。仅使用人物姓氏（通过<code>rsplit</code>提取）简化显示。</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509140913106.png" alt="image-20250914091357886"></p>
<h3 id="7-特征脸可视化"><a href="#7-特征脸可视化" class="headerlink" title="7. 特征脸可视化"></a>7. 特征脸可视化</h3><pre><code class="language-python"># 生成特征脸（Eigenfaces）标题
eigenface_titles = [&quot;eigenface %d&quot; % i for i in range(eigenfaces.shape[0])]

# 绘制特征脸
plot_gallery(eigenfaces, eigenface_titles, h, w)

plt.show()
</code></pre>
<p>调用<code>plot_gallery</code>函数可视化PCA提取的前100个特征脸。特征脸反映了人脸图像的主要变化模式，是PCA降维的基础。</p>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509140914579.png" alt="image-20250914091444461"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #00a596">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/09/15/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/">
        <h2 class="post-title">聚类方法：从理论到实践</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/9/15
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />


<h1 id="聚类方法：从理论到实践"><a href="#聚类方法：从理论到实践" class="headerlink" title="聚类方法：从理论到实践"></a>聚类方法：从理论到实践</h1><h3 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h3><p>在机器学习和数据挖掘中，衡量样本（或特征）之间的<strong>相似度 (Similarity)</strong> 和 <strong>距离 (Distance)</strong> 是许多算法（如K近邻、聚类、降维）的基础。相似度越大，样本越相似；距离越大，样本差异越大。</p>
<h4 id="（1）相似度与距离"><a href="#（1）相似度与距离" class="headerlink" title="（1）相似度与距离"></a>（1）相似度与距离</h4><p>假设我们有 $n$ 个样本，每个样本有 $p$ 个属性（特征）。数据矩阵 $X$ 可以表示为：<br>$$<br>X &#x3D; \begin{bmatrix}<br>x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \<br>x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}<br>\end{bmatrix}<br>$$<br>其中，$x_{ij}$ 表示<strong>第 $i$ 个样本的第 $j$ 个属性</strong>的值。 在机器学习中通常用行表示样本。为了与后续概念统一，我们这里采用<strong>样本-属性</strong>的视角。</p>
<p>两个 $p$ 维样本点 $\mathbf{x}<em>i$ 和 $\mathbf{x}<em>j$ 可以表示为：<br>$$<br>\mathbf{x}<em>i &#x3D; (x</em>{i1}, x</em>{i2}, …, x</em>{ip})^T, \quad \mathbf{x}<em>j &#x3D; (x</em>{j1}, x_{j2}, …, x_{jp})^T<br>$$</p>
<h5 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a><strong>闵可夫斯基距离</strong></h5><p>闵可夫斯基距离是下述一系列距离度量的通用形式。</p>
<p>$$d_{ij} &#x3D; \left( \sum_{k&#x3D;1}^{p} |x_{ik} - x_{jk}|^h \right)^{\frac{1}{h}}$$</p>
<p>其中，$h$ 是一个参数。</p>
<ul>
<li><p><strong>曼哈顿距离</strong><br>当 $h&#x3D;1$ 时，得到曼哈顿距离，也称为“城市街区距离”。<br>$$d_{ij} &#x3D; \sum_{k&#x3D;1}^{p} |x_{ik} - x_{jk}|$$<br><strong>例子</strong>：想象在城市网格状道路中从A点到B点，只能沿着街道走。它的值是各维度差值绝对值的总和。它对数据中的异常值不如欧氏距离敏感。<br><strong>例子</strong>：点 $\mathbf{x}_i &#x3D; (1, 2)$，点 $\mathbf{x}<em>j &#x3D; (4, 6)$。<br>$d</em>{ij} &#x3D; |1-4| + |2-6| &#x3D; 3 + 4 &#x3D; 7$。</p>
</li>
<li><p>**欧氏距离 **<br>当 $h&#x3D;2$ 时，得到最常用的欧氏距离，即直线距离。<br>$$d_{ij} &#x3D; \left( \sum_{k&#x3D;1}^{p} (x_{ik} - x_{jk})^2 \right)^{\frac{1}{2}}$$<br><strong>例子</strong>：这是最直观的距离度量方式。它对数据中的异常值比较敏感。<br><strong>例子</strong>：点 $\mathbf{x}_i &#x3D; (1, 2)$，点 $\mathbf{x}<em>j &#x3D; (4, 6)$。<br>$d</em>{ij} &#x3D; \sqrt{(1-4)^2 + (2-6)^2} &#x3D; \sqrt{9 + 16} &#x3D; \sqrt{25} &#x3D; 5$。</p>
</li>
<li><p><strong>切比雪夫距离</strong><br>当 $h \to \infty$ 时，得到切比雪夫距离，它是所有维度上差值绝对值的最大值。<br>$$d_{ij} &#x3D; \max_k |x_{ik} - x_{jk}|$$<br><strong>例子</strong>：常用于国际象棋中，国王从一个格子移动到另一个格子所需的最少步数就是切比雪夫距离。<br><strong>例子</strong>：点 $\mathbf{x}_i &#x3D; (1, 2)$，点 $\mathbf{x}<em>j &#x3D; (4, 6)$。<br>$d</em>{ij} &#x3D; \max(|1-4|, |2-6|) &#x3D; \max(3, 4) &#x3D; 4$。</p>
</li>
</ul>
<h5 id="马氏距离"><a href="#马氏距离" class="headerlink" title="**马氏距离 **"></a>**马氏距离 **</h5><p>马氏距离考虑了数据特征之间的相关性，并且与数据的尺度无关（scale-invariant）。<br>$$d_{ij} &#x3D; \sqrt{(\mathbf{x}_i - \mathbf{x}_j)^T \mathbf{S}^{-1} (\mathbf{x}_i - \mathbf{x}_j)}$$<br>其中，$\mathbf{S}$ 是数据集的<strong>协方差矩阵</strong>（定义见后文）。<br><strong>介绍</strong>：欧氏距离假设数据的各个维度不相关且尺度相同。当这个假设不成立时（例如，一个维度的单位是米，另一个是吨），欧氏距离就不合理。马氏距离通过引入协方差矩阵的逆 $\mathbf{S}^{-1}$ 来“修正”这个问题，它相当于在计算距离前，先对数据做了一个线性变换，使其变为各维度不相关且方差为1的标准形式，然后再计算欧氏距离。</p>
<h5 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a><strong>相关系数</strong></h5><p>相关系数衡量的是两个<strong>向量</strong>（比如两个样本或两个特征）之间的<strong>线性相关程度</strong>，属于相似度度量。值域为 $[-1, 1]$。</p>
<ul>
<li><strong>样本相关系数 (Pearson Correlation Coefficient)</strong>：<br>对于两个 $p$ 维样本向量 $\mathbf{x}<em>i$ 和 $\mathbf{x}<em>j$，其相关系数计算为：<br>$$\rho</em>{ij} &#x3D; \frac{\sum</em>{k&#x3D;1}^{p} (x_{ik} - \bar{x}<em>i)(x</em>{jk} - \bar{x}<em>j)}{\sqrt{\sum</em>{k&#x3D;1}^{p} (x_{ik} - \bar{x}<em>i)^2} \sqrt{\sum</em>{k&#x3D;1}^{p} (x_{jk} - \bar{x}<em>j)^2}}$$<br>其中 $\bar{x}<em>i &#x3D; \frac{1}{p}\sum</em>{k&#x3D;1}^p x</em>{ik}$ 是样本 $i$ 所有属性值的均值。<br><strong>介绍</strong>：$\rho_{ij} &#x3D; 1$ 表示完全正相关，$\rho_{ij} &#x3D; -1$ 表示完全负相关，$\rho_{ij} &#x3D; 0$ 表示无线性相关。</li>
</ul>
<h4 id="（2）类或簇"><a href="#（2）类或簇" class="headerlink" title="（2）类或簇"></a>（2）类或簇</h4><p>在聚类分析中，一组相似的样本被归为一个“簇”或“类”。我们需要一些指标来描述一个类的特性。</p>
<p>假设一个类 $G$ 包含 $n$ 个 $p$ 维样本：$\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_n \in G$。</p>
<ul>
<li><p><strong>类中心 (Centroid)</strong><br>类中心是类中所有样本的均值向量，代表了类的平均位置。<br>$$\bar{\mathbf{x}}<em>G &#x3D; \frac{1}{n_G} \sum</em>{i&#x3D;1}^{n_G} \mathbf{x}_i$$</p>
</li>
<li><p><strong>类直径 (Diameter) $D_G$</strong><br>类直径衡量类内所有样本间的差异程度，即类的“大小”。<br>$$D_G &#x3D; \sum_{\mathbf{x}<em>i \in G} \sum</em>{\mathbf{x}_j \in G} ||\mathbf{x}_i - \mathbf{x}_j||^2$$<br>一个更常用的、与类直径相关的概念是<strong>类内方差</strong>。</p>
</li>
<li><p><strong>类的散布矩阵 $\mathbf{S}_G$ &amp; 协方差矩阵  $\mathbf{\Sigma}_G$</strong><br>这两个矩阵都描述了类内样本围绕类中心的分散情况，以及属性之间的关系。</p>
<ul>
<li><p><strong>散布矩阵</strong>：<br>$$\mathbf{S}<em>G &#x3D; \sum</em>{i&#x3D;1}^{n_G} (\mathbf{x}_i - \bar{\mathbf{x}}_G)(\mathbf{x}<em>i - \bar{\mathbf{x}}<em>G)^T$$<br>这是一个 $p \times p$ 的矩阵。其对角线元素 $s</em>{kk}$ 是第 $k$ 个属性的<strong>方差</strong>的 $(n_G-1)$ 倍，非对角线元素 $s</em>{kl}$ 是属性 $k$ 和 $l$ 的<strong>协方差</strong>的 $(n_G-1)$ 倍。</p>
</li>
<li><p><strong>协方差矩阵</strong>：<br>协方差矩阵是标准化后的散布矩阵。<br>$$\mathbf{\Sigma}_G &#x3D; \frac{1}{n_G - 1} \mathbf{S}<em>G &#x3D; \frac{1}{n_G - 1} \sum</em>{i&#x3D;1}^{n_G} (\mathbf{x}_i - \bar{\mathbf{x}}_G)(\mathbf{x}_i - \bar{\mathbf{x}}<em>G)^T$$<br><strong>介绍</strong>：协方差矩阵 $\mathbf{\Sigma}<em>G$ 的对角线元素 $\sigma</em>{kk}$ 是第 $k$ 个属性的<strong>方差</strong>，衡量该属性值的分散程度。非对角线元素 $\sigma</em>{kl}$ 是属性 $k$ 和 $l$ 的<strong>协方差</strong>，衡量两个属性之间的线性相关性。马氏距离中使用的正是整个数据集的协方差矩阵 $\mathbf{S}$（或 $\mathbf{\Sigma}$）。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-层次聚类"><a href="#2-层次聚类" class="headerlink" title="2. 层次聚类"></a>2. 层次聚类</h3><p>层次聚类旨在构建一个层次的嵌套聚类树，包括**聚合（自底向上）<strong>和</strong>分裂（自底向下）**两种策略。其中最常用的是聚合聚类。</p>
<h4 id="聚合聚类"><a href="#聚合聚类" class="headerlink" title="聚合聚类"></a>聚合聚类</h4><p><strong>基本流程</strong>：开始时将每个样本各自分到一个簇，然后按照某种准则逐步合并最相似的簇，直到所有样本归于一个簇或达到某个停止条件。</p>
<p>该方法的核心是三个要素：</p>
<ol>
<li><strong>距离或相似度</strong>：用于衡量样本间的差异。常用欧氏距离、曼哈顿距离、余弦相似度等。</li>
<li><strong>合并规则</strong>：用于衡量<strong>簇与簇之间</strong>的距离，决定哪两个簇将被合并。常见规则有：<ul>
<li><strong>单链接 ：两簇中</strong>最近**样本间的距离。<br>$d(C_i, C_j) &#x3D; \min_{\mathbf{x} \in C_i, \mathbf{y} \in C_j} ||\mathbf{x} - \mathbf{y}||$</li>
<li><strong>全链接：两簇中</strong>最远**样本间的距离。<br>$d(C_i, C_j) &#x3D; \max_{\mathbf{x} \in C_i, \mathbf{y} \in C_j} ||\mathbf{x} - \mathbf{y}||$</li>
<li><strong>均链接：两簇中所有样本对间距离的</strong>平均值**。<br>$d(C_i, C_j) &#x3D; \frac{1}{|C_i||C_j|} \sum_{\mathbf{x} \in C_i} \sum_{\mathbf{y} \in C_j} ||\mathbf{x} - \mathbf{y}||$</li>
<li><strong>质心链接 ：两簇</strong>质心**间的距离。<br>$d(C_i, C_j) &#x3D; ||\bar{\mathbf{x}}<em>{C_i} - \bar{\mathbf{x}}</em>{C_j}||$</li>
</ul>
</li>
<li>**停止条件 ：通常是指定最终需要的簇数目 $K$，或指定一个距离阈值，当最近的两个簇距离超过该阈值时停止合并。</li>
</ol>
<hr>
<h3 id="3-K均值聚类"><a href="#3-K均值聚类" class="headerlink" title="3. K均值聚类"></a>3. K均值聚类</h3><p>K均值是一种非常经典、高效的<strong>划分式（Partitional）</strong> 聚类算法。其核心思想是：<strong>以样本与簇质心间的距离为依据，通过迭代优化，使簇内样本尽可能相似（距离小），簇间样本尽可能不相似。</strong></p>
<h4 id="（1）理论算法"><a href="#（1）理论算法" class="headerlink" title="（1）理论算法"></a>（1）理论算法</h4><p>从理论上看，K均值算法旨在最小化<strong>簇内平方和 (Within-Cluster Sum of Squares, WCSS)</strong>，也称为<strong>失真</strong>。<br>$$J &#x3D; \sum_{i&#x3D;1}^{K} \sum_{\mathbf{x} \in C_i} ||\mathbf{x} - \boldsymbol{\mu}_i||^2$$<br>其中：</p>
<ul>
<li>$K$ 是预先指定的簇的个数。</li>
<li>$C_i$ 表示第 $i$ 个簇。</li>
<li>$\boldsymbol{\mu}_i$ 是第 $i$ 个簇的质心（中心点）。</li>
<li>$||\mathbf{x} - \boldsymbol{\mu}_i||^2$ 是样本 $\mathbf{x}$ 到其所属簇质心的欧氏距离的平方。</li>
</ul>
<p><strong>目标</strong>：找到簇划分 $C &#x3D; {C_1, C_2, …, C_K}$ 和质心 ${\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, …, \boldsymbol{\mu}_K}$，使得目标函数 $J$ 最小化。</p>
<h4 id="（2）实际步骤"><a href="#（2）实际步骤" class="headerlink" title="（2）实际步骤"></a>（2）实际步骤</h4><p>由于最小化 $J$ 是一个NP难问题，K均值采用一种<strong>贪心迭代</strong>策略来寻找近似最优解。其步骤如下：</p>
<ol>
<li><p>**初始化：</p>
<ul>
<li>随机选择 $K$ 个样本点作为<strong>初始簇质心</strong> ${\boldsymbol{\mu}_1^{(0)}, \boldsymbol{\mu}_2^{(0)}, …, \boldsymbol{\mu}_K^{(0)}}$。（上标 $(0)$ 表示第0次迭代）。</li>
<li>这是关键一步，不同的初始值可能导致不同的聚类结果。</li>
</ul>
</li>
<li><p>**分配步骤：</p>
<ul>
<li>对于数据集中的<strong>每一个样本 $\mathbf{x}_n$</strong>：<ul>
<li>计算它到当前 $K$ 个质心中<strong>每一个</strong>的距离（通常为欧氏距离）。</li>
<li>将其<strong>分配给距离最近的质心所对应的簇</strong>。</li>
</ul>
</li>
<li>数学表述：$C_i^{(t)} &#x3D; { \mathbf{x}_n : ||\mathbf{x}_n - \boldsymbol{\mu}_i^{(t)}||^2 \le ||\mathbf{x}_n - \boldsymbol{\mu}_j^{(t)}||^2 ; \forall j, 1 \le j \le K }$</li>
<li>（上标 $(t)$ 表示第 $t$ 次迭代）</li>
</ul>
</li>
<li><p>**更新步骤 ：</p>
<ul>
<li>对于<strong>每一个簇 $C_i^{(t)}$</strong>：<ul>
<li>重新计算该簇的质心。新的质心是该簇所有样本的<strong>均值向量</strong>。</li>
</ul>
</li>
<li>数学表述：$\boldsymbol{\mu}<em>i^{(t+1)} &#x3D; \frac{1}{|C_i^{(t)}|} \sum</em>{\mathbf{x} \in C_i^{(t)}} \mathbf{x}$</li>
</ul>
</li>
<li><p>**重复步骤：</p>
<ul>
<li>重复执行<strong>步骤2（分配）</strong> 和<strong>步骤3（更新）</strong>。</li>
<li><strong>停止条件</strong>：当质心的位置不再发生变化（即 $\boldsymbol{\mu}_i^{(t+1)} &#x3D; \boldsymbol{\mu}_i^{(t)}$ 对所有 $i$ 成立），或者变化很小，或者达到最大迭代次数时，算法停止。</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>举例说明</strong>：<br>假设有样本点 <code>[1, 2], [1, 4], [2, 1], [3, 5], [4, 2], [5, 4]</code>，设定 $K&#x3D;2$。</p>
<ol>
<li><strong>初始化</strong>：随机选 <code>[1, 2]</code> 和 <code>[3, 5]</code> 作为初始质心。</li>
<li><strong>分配</strong>：计算所有点到这两个质心的距离，将其分到更近的簇。<ul>
<li>点 <code>[1,4]</code> 到 <code>[1,2]</code> 的距离是 <code>2</code>，到 <code>[3,5]</code> 的距离是 <code>√(4+1)≈2.2</code>，因此被分到第一个簇。</li>
<li>同理，<code>[2,1]</code> 分到第一个簇，<code>[5,4]</code> 分到第二个簇，<code>[4,2]</code> 需要计算。</li>
</ul>
</li>
<li><strong>更新</strong>：重新计算两个簇的均值。<ul>
<li>簇1：<code>[1,2], [1,4], [2,1]</code> -&gt; 新质心 <code>[(1+1+2)/3, (2+4+1)/3] = [1.33, 2.33]</code></li>
<li>簇2：<code>[3,5], [5,4], [4,2]</code> -&gt; 新质心 <code>[(3+5+4)/3, (5+4+2)/3] = [4, 3.67]</code></li>
</ul>
</li>
<li><strong>重复</strong>：用新的质心 <code>[1.33, 2.33]</code> 和 <code>[4, 3.67]</code> 再次执行分配和更新步骤。直到质心稳定不变。</li>
</ol>
</blockquote>
<h4 id="（3）算法特性"><a href="#（3）算法特性" class="headerlink" title="（3）算法特性"></a>（3）算法特性</h4><ul>
<li><p><strong>收敛性</strong>：<br>K均值算法<strong>必定收敛</strong>。因为在每次迭代中：</p>
<ol>
<li>分配步骤通过将样本分配给最近质心来<strong>减小</strong> $J$。</li>
<li>更新步骤通过计算均值来找到当前簇划分下最小化 $J$ 的最优质心，同样<strong>减小</strong> $J$。<br>由于目标函数 $J$ 有下界（且迭代过程使其不断减小），因此算法最终会收敛到一个局部最优解。<strong>注意：它不能保证收敛到全局最优解。</strong></li>
</ol>
</li>
<li><p><strong>初始类的选择</strong>：<br>由于对初始质心敏感，不同的初始化可能导致不同的局部最优解。常用改进方法是<strong>K-means++</strong> 初始化策略，其核心思想是：让初始质心彼此尽可能远离。基本步骤是：</p>
<ol>
<li>随机选择第一个质心。</li>
<li>对于每一个样本，计算其与已选质心的最短距离 $D(\mathbf{x})$。</li>
<li>以概率 $\frac{D(\mathbf{x})^2}{\sum_{\mathbf{x}} D(\mathbf{x})^2}$ 选择一个距离已选质心较远的点作为新质心。</li>
<li>重复步骤2、3，直到选出 $K$ 个质心。<br><em>“先用层次聚类得到k类”也是一种有效的策略，但计算成本较高。</em></li>
</ol>
</li>
<li><p><strong>类别数k的选择</strong>：<br>$K$ 是一个超参数，需要预先指定。选择最佳 $K$ 值没有绝对正确的方法，常用方法是<strong>手肘法 (Elbow Method)</strong>。<br><strong>原理</strong>：随着 $K$ 值的增大，簇内样本更紧密，平均直径（或WCSS $J$）会不断减小。当 $K$ 增大到接近真实簇数时，WCSS的下降幅度会突然变缓，形成一个“手肘”一样的拐点。<br><strong>具体做法</strong>：</p>
<ol>
<li>分别计算 $K&#x3D;1, 2, 3, …$ 时聚类完成后的WCSS $J$。</li>
<li>绘制 $K$ 与 $J$ 的关系曲线图。</li>
<li>寻找曲线中的“拐点”（即下降速度由快变慢的点），对应的 $K$ 值通常是一个好的选择。</li>
</ol>
<p><strong>二分查找思想</strong>：并非字面上的二分查找算法，而是指一种策略：从一个较小的 $K_{min}$ 和一个较大的 $K_{max}$ 开始，通过观察不同 $K$ 值下 $J$ 的变化趋势，逐步缩小最佳 $K$ 值的候选范围，从而更高效地找到“手肘点”。</p>
</li>
</ul>
<h3 id="4-K均值聚类用于手写字体分类"><a href="#4-K均值聚类用于手写字体分类" class="headerlink" title="4. K均值聚类用于手写字体分类"></a>4. K均值聚类用于手写字体分类</h3><h4 id="（1）数据导入"><a href="#（1）数据导入" class="headerlink" title="（1）数据导入"></a>（1）数据导入</h4><pre><code class="language-python">import numpy as np

from sklearn.datasets import load_digits

data, labels = load_digits(return_X_y=True)
(n_samples, n_features), n_digits = data.shape, np.unique(labels).size

print(f&quot;数字数: &#123;n_digits&#125;; # 样本量: &#123;n_samples&#125;; # 特征数 &#123;n_features&#125;&quot;)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509151033021.png" alt="image-20250915103305988"></p>
<h4 id="（2）定义评估函数"><a href="#（2）定义评估函数" class="headerlink" title="（2）定义评估函数"></a>（2）定义评估函数</h4><pre><code class="language-python"># 导入所需模块
from time import time  
from sklearn import metrics  
from sklearn.pipeline import make_pipeline  # 用于创建数据处理管道
from sklearn.preprocessing import StandardScaler  


def bench_k_means(kmeans, name, data, labels):
    # 记录开始时间，用于计算整个pipeline的拟合时间
    t0 = time()
    
    # 创建一个数据处理管道：先标准化数据，然后进行K均值聚类
    # make_pipeline(StandardScaler(), kmeans) 创建了一个两步流程：
    # 1. StandardScaler(): 对数据进行标准化（减去均值，除以标准差）
    # 2. kmeans: 执行K均值聚类算法
    # .fit(data) 方法依次执行这两个步骤
    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)
    
    # 计算整个拟合过程所花费的时间
    fit_time = time() - t0
    
    # 初始化结果列表，包含算法名称、拟合时间和惯性值(inertia)
    # estimator[-1] 获取管道中的最后一个估计器（即KMeans实例）
    # inertia_ 是K均值的目标函数，表示簇内平方和，值越小表示聚类效果越好
    results = [name, fit_time, estimator[-1].inertia_]

    # 定义需要真实标签和预测标签的聚类评估指标
    # 这些指标用于衡量聚类结果与真实标签的一致性
    clustering_metrics = [
        metrics.homogeneity_score,  # 同质性：每个簇只包含一个类的成员
        metrics.completeness_score,  # 完整性：给定类的所有成员都被分配到同一个簇
        metrics.v_measure_score,     # V度量：同质性和完整性的调和平均
        metrics.adjusted_rand_score, # 调整兰德指数：衡量两个数据分配之间的相似度
        metrics.adjusted_mutual_info_score,  # 调整互信息：考虑机会因素的互信息
    ]
    
    # 每个指标函数m接受真实标签labels和预测标签estimator[-1].labels_
    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]

    # 轮廓系数需要完整的数据集（特征数据+预测标签）
    results += [
        metrics.silhouette_score(
            data, 
            estimator[-1].labels_,  # 聚类预测标签
            metric=&quot;euclidean&quot;,  # 使用欧氏距离计算
            sample_size=300,  
        )
    ]

    formatter_result = (
        &quot;&#123;:9s&#125;\t&#123;:.3f&#125;s\t&#123;:.0f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;\t&#123;:.3f&#125;&quot;
    )
  
    print(formatter_result.format(*results))
</code></pre>
<p>定义了一个基准测试函数 <code>bench_k_means</code>，用于全面评估K均值聚类算法的性能，主要评估以下几个方面：</p>
<ol>
<li><strong>效率评估</strong>：计算算法的运行时间 (<code>fit_time</code>)</li>
<li><strong>内部评估</strong>：计算簇内平方和 (<code>inertia_</code>) 和轮廓系数 (<code>silhouette_score</code>)</li>
<li><strong>外部评估</strong>：使用真实标签计算多个评估指标，衡量聚类结果与真实类别的一致性</li>
</ol>
<h4 id="（3）运行对比算法"><a href="#（3）运行对比算法" class="headerlink" title="（3）运行对比算法"></a>（3）运行对比算法</h4><pre><code class="language-python">from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

print(82 * &quot;_&quot;)
print(&quot;init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette&quot;)

kmeans = KMeans(init=&quot;k-means++&quot;, n_clusters=n_digits, n_init=4, random_state=0)
bench_k_means(kmeans=kmeans, name=&quot;k-means++&quot;, data=data, labels=labels)

kmeans = KMeans(init=&quot;random&quot;, n_clusters=n_digits, n_init=4, random_state=0)
bench_k_means(kmeans=kmeans, name=&quot;random&quot;, data=data, labels=labels)

pca = PCA(n_components=n_digits).fit(data)
kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)
bench_k_means(kmeans=kmeans, name=&quot;PCA-based&quot;, data=data, labels=labels)

print(82 * &quot;_&quot;)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509151032541.png" alt="image-20250915103248479"></p>
<p><strong>1. k-means++ 初始化 (<code>init=&#39;k-means++&#39;</code>)</strong></p>
<ul>
<li><strong>原理</strong>: 一种智能的初始化方法，通过让初始聚类中心彼此远离来选择初始质心。第一个质心随机选择，后续质心以与已选质心距离平方成正比的概率被选中。</li>
<li><strong>特性</strong>: <strong>随机性</strong> - 每次运行结果可能不同。</li>
<li><strong>实验设置</strong>: 运行4次 (<code>n_init=4</code>) 以考虑其随机性，然后选择最佳结果。</li>
</ul>
<p><strong>2. 随机初始化 (<code>init=&#39;random&#39;</code>)</strong></p>
<ul>
<li><strong>原理</strong>: 完全随机选择数据点作为初始质心。</li>
<li><strong>特性</strong>: <strong>随机性</strong> - 每次运行结果可能大不相同。</li>
<li><strong>实验设置</strong>: 运行4次 (<code>n_init=4</code>) 以考虑其随机性，然后选择最佳结果。</li>
</ul>
<p><strong>3. 基于PCA的初始化 (确定性方法)</strong></p>
<ul>
<li><strong>原理</strong>: 使用主成分分析(PCA)的前n个主成分方向来初始化质心。具体来说，沿着前n个主成分方向，在数据范围内均匀分布质心。</li>
<li><strong>特性</strong>: <strong>确定性</strong> - 每次运行结果相同。</li>
<li><strong>实验设置</strong>: 只需要运行1次 (<code>n_init=1</code>)。</li>
</ul>
<h4 id="（4）可视化对比结果"><a href="#（4）可视化对比结果" class="headerlink" title="（4）可视化对比结果"></a>（4）可视化对比结果</h4><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs

# 使用PCA降维到2维以便可视化
reduced_data = PCA(n_components=2).fit_transform(data)

# 设置网格步长
h = 0.02  # 网格点间距

# 计算绘图范围
x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1

# 创建网格点
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# 创建包含2个子图的图形
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# 初始化方法列表
init_methods = [&#39;k-means++&#39;, &#39;random&#39;]
axes = [ax1, ax2]

# 对每种初始化方法进行循环
for i, init_method in enumerate(init_methods):
    # 执行K均值聚类
    kmeans = KMeans(init=init_method, n_clusters=n_digits, n_init=4, random_state=42)
    kmeans.fit(reduced_data)
    
    # 预测网格点的聚类标签
    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # 在当前子图上绘制决策边界
    axes[i].imshow(
        Z,
        interpolation=&quot;nearest&quot;,
        extent=(xx.min(), xx.max(), yy.min(), yy.max()),
        cmap=plt.cm.Paired,
        aspect=&quot;auto&quot;,
        origin=&quot;lower&quot;,
    )
    
    # 绘制数据点
    axes[i].plot(reduced_data[:, 0], reduced_data[:, 1], &#39;k.&#39;, markersize=2)
    
    # 绘制聚类中心
    centroids = kmeans.cluster_centers_
    axes[i].scatter(
        centroids[:, 0],
        centroids[:, 1],
        marker=&quot;x&quot;,
        s=169,
        linewidths=3,
        color=&quot;w&quot;,
        zorder=10,
    )
    
    # 设置子图标题和坐标轴
    axes[i].set_title(f&quot;K-means with &#123;init_method&#125; initialization&quot;)
    axes[i].set_xlim(x_min, x_max)
    axes[i].set_ylim(y_min, y_max)
    axes[i].set_xticks(())
    axes[i].set_yticks(())
    
    # 在标题下方显示惯性值（inertia）
    axes[i].text(0.5, -0.1, f&quot;Inertia: &#123;kmeans.inertia_:.2f&#125;&quot;, 
                transform=axes[i].transAxes, ha=&#39;center&#39;, fontsize=12)

# 调整子图间距
plt.tight_layout()
plt.show()
</code></pre>
<ol>
<li><strong>数据预处理</strong>：首先使用PCA将高维数据降维到2维，这样可以在二维平面上进行可视化。</li>
<li><strong>聚类分析</strong>：使用K-means++、random随机init算法对降维后的数据进行聚类。</li>
<li><strong>决策边界绘制</strong>：<ul>
<li>创建一个覆盖整个数据范围的密集网格</li>
<li>对网格中的每个点进行聚类预测，确定它属于哪个簇</li>
<li>使用<code>imshow()</code>将不同簇的区域用不同颜色显示，形成Voronoi图效果的决策边界</li>
</ul>
</li>
<li><strong>数据点可视化</strong>：在决策背景上，用黑色小点绘制原始数据点的实际位置。</li>
<li><strong>聚类中心标记</strong>：用醒目的白色”X”标记每个簇的中心点（质心）。</li>
</ol>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509151046285.png" alt="image-20250915104629207"></p>
<p>最终输出每个方法的inertia：也就是簇内平方和，这个值越小说明分类越集中，效果更好</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #ffa2c4">机器学习</a>
        </span>
        
    </div>
    <a href="/2025/09/15/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2023/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1/">
        <h2 class="post-title">机器学习建模</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                机器学习
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2023/3/20
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="机器学习建模"><a href="#机器学习建模" class="headerlink" title="机器学习建模"></a>机器学习建模</h2><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><h3 id="1-信息熵-Entropy"><a href="#1-信息熵-Entropy" class="headerlink" title="1. 信息熵 (Entropy)"></a><strong>1. 信息熵 (Entropy)</strong></h3><p>衡量一个系统（数据集）中不确定性的大小。</p>
<p>公式：</p>
<p>$H(X) &#x3D; - \sum_{i&#x3D;1}^n p_i \log_2 p_i$</p>
<p>其中，$p_i $表示随机变量 X 取第 i 类的概率。</p>
<p>作用：当类别分布越均匀，熵越大；当分布越偏向单一类别，熵越小。</p>
<hr>
<h3 id="2-条件熵-Conditional-Entropy"><a href="#2-条件熵-Conditional-Entropy" class="headerlink" title="2. 条件熵 (Conditional Entropy)"></a>2. <strong>条件熵 (Conditional Entropy)</strong></h3><p>表示在已知特征 A 的情况下，类别 Y还剩多少不确定性。</p>
<p>公式：</p>
<p>$H(Y|A) &#x3D; \sum_{j&#x3D;1}^m p(a_j) H(Y|A&#x3D;a_j)$</p>
<p>其中，$p(a_j)$ 是特征 A 取值为 $a_j $的概率，</p>
<p>$H(Y|A&#x3D;a_j) &#x3D; -\sum_{i&#x3D;1}^n p(y_i|a_j) \log_2 p(y_i|a_j)$</p>
<hr>
<h3 id="3-信息增益-Information-Gain-IG"><a href="#3-信息增益-Information-Gain-IG" class="headerlink" title="3. 信息增益 (Information Gain, IG)"></a>3. <strong>信息增益 (Information Gain, IG)</strong></h3><p>衡量特征 A 带来的不确定性减少量，是决策树（ID3, C4.5）的核心。</p>
<p>公式：</p>
<p>$IG(Y, A) &#x3D; H(Y) - H(Y|A)$</p>
<p>即原始信息熵减去条件熵，数值越大说明特征 A 越能有效区分样本。</p>
<hr>
<h3 id="4-基尼指数-Gini-Index"><a href="#4-基尼指数-Gini-Index" class="headerlink" title="4. 基尼指数 (Gini Index)"></a>4. <strong>基尼指数 (Gini Index)</strong></h3><p>另一种衡量数据纯度的方法（常用于 CART 决策树）。</p>
<p>公式：</p>
<p>$Gini(D) &#x3D; 1 - \sum_{i&#x3D;1}^n p_i^2$</p>
<p>其中，$p_i $是第 i 类的概率。越小表示样本越“纯”。</p>
<p>若用特征 A划分：</p>
<p>$Gini(D, A) &#x3D; \sum_{j&#x3D;1}^m \frac{|D_j|}{|D|} Gini(D_j)$</p>
<p>选择能最小化 Gini 的特征作为划分点。</p>
<hr>
<h3 id="5-在决策树中的作用"><a href="#5-在决策树中的作用" class="headerlink" title="5. 在决策树中的作用"></a>5. <strong>在决策树中的作用</strong></h3><ul>
<li><strong>ID3 决策树</strong>：用 <strong>信息增益</strong> 最大的特征作为划分点。</li>
<li><strong>C4.5 决策树</strong>：改进信息增益，使用 <strong>信息增益率</strong>。</li>
<li><strong>CART 决策树</strong>：用 <strong>基尼指数最小</strong> 的特征作为划分点。</li>
</ul>
<hr>
<h2 id="（1）决策树"><a href="#（1）决策树" class="headerlink" title="（1）决策树"></a>（1）决策树</h2><h3 id="1-ID3方法（分类任务）"><a href="#1-ID3方法（分类任务）" class="headerlink" title="1. ID3方法（分类任务）"></a>1. ID3方法（分类任务）</h3><ul>
<li>输入数据集：算法接受一个已标记的数据集，其中包含一系列样本，每个样本都有一组特征和一个类别标签。</li>
<li>特征选择：从所有可能的特征中选择一个最佳的特征来进行分割。这通常通过计算每个特征的信息增益（或信息熵）来完成。信息增益表示在选择某个特征后，数据集的不确定性减少的程度。信息增益高的特征被认为是最佳的选择。</li>
<li>创建一个决策节点：将选定的特征用于创建一个决策节点，并将数据集分割成多个子集，每个子集对应于该特征的不同取值。</li>
<li>递归操作：对每个子集重复上述步骤，直到满足停止条件。停止条件可以是以下之一：所有样本都属于同一类别，或者没有更多的特征可用于分割数据。</li>
<li>构建决策树：最终，算法通过连接所有的决策节点来构建一个完整的决策树，其中叶节点表示最终的分类结果。</li>
</ul>
<h3 id="2-C4-5算法（分类任务）"><a href="#2-C4-5算法（分类任务）" class="headerlink" title="2. C4.5算法（分类任务）"></a>2. C4.5算法（分类任务）</h3><p>根据信息增益率进行特征选择，引入悲观剪枝策略进行后剪枝</p>
<p>工作原理：</p>
<p>特征选择：</p>
<ul>
<li>特征选择：与ID3类似，C4.5也通过计算特征的信息增益来选择最佳的特征进行分割。不过，C4.5引入了一个新的概念，称为”信息增益比”，以解决ID3在特征取值较多时的不公平问题。信息增益比考虑了特征取值的数量，从而更加公平地对待不同数量的取值。</li>
<li>处理连续型特征：C4.5能够处理连续型特征，而不仅仅是离散型特征。它通过尝试不同的分割点来将连续特征离散化，并选择最佳的分割点以最大化信息增益或信息增益比。</li>
<li>剪枝：C4.5引入了剪枝机制，以减小生成的决策树的复杂性，防止过拟合。剪枝是通过验证数据集来确定哪些子树可以被删除或保留的。这有助于生成更简单、更具泛化能力的决策树。</li>
<li>处理缺失值：C4.5能够处理数据中的缺失值，允许在构建决策树时处理包含缺失数据的样本。</li>
<li>生成决策树：通过递归地选择最佳特征、分割数据、剪枝等步骤，C4.5最终生成一个用于分类或回归的决策树。</li>
</ul>
<h3 id="3-CART算法（分类-回归任务）"><a href="#3-CART算法（分类-回归任务）" class="headerlink" title="3. CART算法（分类&#x2F;回归任务）"></a>3. CART算法（分类&#x2F;回归任务）</h3><p>基于二叉树，既可分类也可回归，使用基尼指数来进行特征选择</p>
<p><strong>工作原理</strong>：</p>
<ul>
<li>二叉树结构：CART算法生成的决策树是二叉树结构，每个非叶子节点都有两个子节点。这意味着每个特征在每个节点处只进行一次二分分割，而不是多分割。</li>
<li>特征选择：CART算法使用一种称为“Gini不纯度”（Gini impurity）的指标来选择最佳的特征进行分割。Gini不纯度度量了一个数据集中样本被错误分类的概率。算法选择能够最大程度地减小Gini不纯度的特征进行分割。</li>
<li>处理连续型特征：CART能够处理连续型特征，它通过尝试不同的阈值来将连续特征二分化，并选择最佳的分割点。</li>
<li>剪枝：CART算法也支持剪枝，以减小决策树的复杂性和防止过拟合。剪枝是通过验证数据集来确定哪些子树可以被删除或保留的。</li>
<li>多分类和回归：CART算法不仅可以用于分类问题，还可以用于回归问题。对于分类问题，CART生成的树将每个叶节点标记为某个类别。对于回归问题，叶节点包含一个连续的数值。</li>
</ul>
<p>后剪枝与预剪枝</p>
<p>预剪枝：在建树的过程中对每一次分裂都进行判断（使用验证集计算分裂后的回归预测正确率），若效果下降，则减去该分支</p>
<p>后剪枝：建树后再自底向上逐一判断。。。</p>
<h2 id="（2）LightGBM"><a href="#（2）LightGBM" class="headerlink" title="（2）LightGBM"></a>（2）LightGBM</h2><h2 id="（3）GBDT梯度提升决策树"><a href="#（3）GBDT梯度提升决策树" class="headerlink" title="（3）GBDT梯度提升决策树"></a>（3）GBDT梯度提升决策树</h2><p>为boosting的一种（在计算过程中串行），使用多个弱决策器（树）的结果累加而成。首先使用一棵树进行模糊回归预测，得到较大的残差（与真实值），然后训练另外一棵树对该残差进行拟合（即目标是找到合适的分裂节点，能使得该残差往最小的方向去），依次累加，最终使得残差为0。然后将所有弱决策器的回归预测累加而成记得GBDT的最终回归预测。</p>
<blockquote>
<p>更准确的回答：GBDT 是一种串行的梯度提升方法，使用多棵浅回归树作为基学习器。先用常数或一棵树得到初始预测；第 mmm 轮计算当前模型在训练集上的<strong>负梯度（伪残差）</strong>，训练一棵小回归树去拟合它；再在每个叶子上求使<strong>损失函数</strong>最小的叶值，并以学习率缩减后加到当前模型上。重复多轮，最终模型是各棵树输出的加和。在 L2 回归时，这等价于“逐轮拟合残差并累加”；在分类&#x2F;排序等任务中则拟合相应损失的梯度信号</p>
</blockquote>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: #00bcd4">机器学习</a>
        </span>
        
    </div>
    <a href="/2023/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/09/08/kaggle%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E5%BB%BA%E6%A8%A1%EF%BC%9AWrapper%20%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%20LightGBM%20+%20TPE%20%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/">
        <h2 class="post-title">Kaggle房价预测建模：Wrapper 特征选择与 LightGBM + TPE 超参数优化</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/Kaggle/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Kaggle
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/9/8
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <meta name="referrer" content="no-referrer" />


<h1 id="KAGGLE-房价预测建模：Wrapper-特征选择与-LightGBM-TPE-超参数优化"><a href="#KAGGLE-房价预测建模：Wrapper-特征选择与-LightGBM-TPE-超参数优化" class="headerlink" title="KAGGLE 房价预测建模：Wrapper 特征选择与 LightGBM + TPE 超参数优化"></a>KAGGLE 房价预测建模：Wrapper 特征选择与 LightGBM + TPE 超参数优化</h1><hr>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在 Kaggle <strong>House Prices: Advanced Regression Techniques</strong> 比赛中，我们需要基于房屋的多维度特征（面积、位置、建造年份等）预测售价。本文展示了一个端到端的建模流程，核心方法包括：</p>
<ul>
<li><strong>缺失值合理填充（结合业务逻辑）</strong></li>
<li><strong>Wrapper 特征选择</strong></li>
<li><strong>LightGBM 模型训练</strong></li>
<li><strong>TPE 超参数调优</strong></li>
<li><strong>结果生成与提交</strong></li>
</ul>
<hr>
<h2 id="2-数据分析与预处理"><a href="#2-数据分析与预处理" class="headerlink" title="2. 数据分析与预处理"></a>2. 数据分析与预处理</h2><h3 id="数据加载与探索"><a href="#数据加载与探索" class="headerlink" title="数据加载与探索"></a>数据加载与探索</h3><pre><code class="language-python">import pandas as pd
import numpy as np

train_df = pd.read_csv(&#39;../input/house-prices-advanced-regression-techniques/train.csv&#39;)
test_df = pd.read_csv(&#39;../input/house-prices-advanced-regression-techniques/test.csv&#39;)

train_df.info()
train_df = train_df.drop(&#39;Id&#39;, axis=1)
</code></pre>
<pre><code class="language-python">train_df.head(5)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081008796.png" alt="image-20250908100815685"></p>
<p>查看数据类型，判断哪些是离散型变量（object），哪些是连续型变量（int64,float64）</p>
<pre><code class="language-python">train_df.info()
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081009889.png" alt="image-20250908100921833"></p>
<pre><code class="language-python">train_df[&#39;Id&#39;].nunique() == train_df.shape[0]  # 检查 Id 唯一性
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081008347.png" alt="image-20250908100851310"></p>
<h3 id="缺失值分析与处理"><a href="#缺失值分析与处理" class="headerlink" title="缺失值分析与处理"></a>缺失值分析与处理</h3><h4 id="离散型变量缺失值"><a href="#离散型变量缺失值" class="headerlink" title="离散型变量缺失值"></a>离散型变量缺失值</h4><pre><code class="language-python">cat_cols = train_df.select_dtypes(include=[&#39;object&#39;]).columns
missing_cat_cols = train_df[cat_cols].columns[train_df[cat_cols].isnull().any()]

missing_cat_dict = &#123;&#125;
for col in missing_cat_cols:
    missing_cat_dict[col] = train_df[col].unique()

# 填充为 &#39;missing&#39;
train_df[cat_cols] = train_df[cat_cols].fillna(&#39;missing&#39;)
</code></pre>
<p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081009024.png" alt="image-20250908100951906"></p>
<p>因为类别不多，所以都可以直接填充为missing，然后再使用编码器进行编码</p>
<h4 id="连续型变量缺失值"><a href="#连续型变量缺失值" class="headerlink" title="连续型变量缺失值"></a>连续型变量缺失值</h4><p><img src="https://gitee.com/LCZsecretspace/images/raw/master/202509081010565.png" alt="image-20250908101020482"></p>
<pre><code class="language-python">num_cols = train_df.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
train_df[num_cols].isnull().sum()

# 查看缺失分布
train_df[train_df[num_cols].columns[train_df[num_cols].isnull().any()]].describe()

# 针对具体列的填充策略
train_df[&#39;LotFrontage&#39;] = train_df.groupby(&#39;Neighborhood&#39;)[&#39;LotFrontage&#39;].transform(lambda x: x.fillna(x.median()))
train_df[&#39;MasVnrArea&#39;] = train_df[&#39;MasVnrArea&#39;].fillna(0)
train_df[&#39;GarageYrBlt&#39;] = train_df[&#39;GarageYrBlt&#39;].fillna(train_df[&#39;GarageYrBlt&#39;].mean())
</code></pre>
<ul>
<li><strong>LotFrontage（地块临街长度）</strong>：按街区中位数填充，避免极端值影响。</li>
<li><strong>MasVnrArea（贴面石材面积）</strong>：缺失意味着“无贴面”，填充为 0。</li>
<li><strong>GarageYrBlt（车库建造年份）</strong>：缺失意味着“无车库”，采用均值填充。</li>
</ul>
<h3 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h3><pre><code class="language-python">from sklearn.preprocessing import LabelEncoder

for col in cat_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].astype(str))
    test_df[col] = le.transform(test_df[col].astype(str))
</code></pre>
<hr>
<h2 id="3-Wrapper-特征选择"><a href="#3-Wrapper-特征选择" class="headerlink" title="3. Wrapper 特征选择"></a>3. Wrapper 特征选择</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>**Wrapper 特征选择（封装式特征选择）**是一种基于模型性能来评估特征重要性的方法。与 <strong>Filter 方法</strong>（依靠统计量，如相关系数、卡方检验）或 <strong>Embedded 方法</strong>（如 L1 正则化直接稀疏化特征）不同，Wrapper 方法会在训练模型的过程中动态评估特征的贡献，并通过迭代或交叉验证来挑选最优特征子集。</p>
<p>在本项目中，Wrapper 特征选择的核心流程如下：</p>
<ol>
<li><strong>使用 LightGBM 训练并获取特征重要性</strong><ul>
<li>LightGBM 在训练过程中会计算每个特征的 <strong>分裂增益（Gain）</strong>，即该特征用于分裂时对损失函数减少的贡献。</li>
<li>在每棵树构建完成后，LightGBM 会累积各个特征的重要性得分。特征的重要性越高，说明它在分裂过程中越常被使用，并且能有效减少预测误差。</li>
</ul>
</li>
<li><strong>KFold 交叉验证累积特征重要性</strong><ul>
<li>单次模型训练可能会受样本划分影响，导致特征重要性不稳定。</li>
<li>为了减少偶然性，Wrapper 方法会采用 <strong>K 折交叉验证</strong>（KFold CV）：<ul>
<li>将数据划分为 K 个子集</li>
<li>每次用 K-1 个子集训练，1 个子集验证</li>
<li>重复 K 次，保证每个样本都参与训练和验证</li>
</ul>
</li>
<li>在每一次训练结束后，提取特征重要性，并累加到一个全局向量中。</li>
<li>最终，得到的特征重要性是 <strong>跨 K 折平均后的结果</strong>，比单次训练更稳健。</li>
</ul>
</li>
<li><strong>选取前 *k* 个重要特征</strong><ul>
<li>当所有 K 折训练结束后，会得到一个按特征排序的重要性得分序列。</li>
<li>通过排序（降序），选取前 <em>k</em> 个特征作为最终的特征子集。</li>
</ul>
</li>
</ol>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code class="language-python">from lightgbm import early_stopping, log_evaluation
from sklearn.model_selection import KFold
import lightgbm as lgb

def feature_select_wrapper(train, topk=50):
    label = &#39;SalePrice&#39;
    features = [col for col in train.columns if col != label and train[col].nunique() &gt; 1]
    y_log = np.log1p(train[label])

    params_initial = &#123;
        &#39;num_leaves&#39;: 30,
        &#39;learning_rate&#39;: 0.05,
        &#39;boosting&#39;: &#39;gbdt&#39;,
        &#39;min_child_samples&#39;: 10,
        &#39;bagging_seed&#39;: 2020,
        &#39;bagging_fraction&#39;: 0.7,
        &#39;bagging_freq&#39;: 1,
        &#39;feature_fraction&#39;: 0.7,
        &#39;max_depth&#39;: -1,
        &#39;metric&#39;: &#39;rmse&#39;,
        &#39;reg_alpha&#39;: 0,
        &#39;reg_lambda&#39;: 1,
        &#39;objective&#39;: &#39;regression&#39;
    &#125;

    NBR = 50000
    kf = KFold(n_splits=5, random_state=2020, shuffle=True)
    fse = pd.Series(0, index=features)

    for train_idx, eval_idx in kf.split(train[features], y_log):
        train_part = lgb.Dataset(train[features].iloc[train_idx], y_log.iloc[train_idx])
        eval_part = lgb.Dataset(train[features].iloc[eval_idx], y_log.iloc[eval_idx])
        
        bst = lgb.train(
            params_initial, train_part, num_boost_round=NBR,
            valid_sets=[train_part, eval_part],
            valid_names=[&#39;train&#39;, &#39;valid&#39;],
            callbacks=[early_stopping(100), log_evaluation(100)]
        )
        fse += pd.Series(bst.feature_importance(importance_type=&quot;gain&quot;), index=features)

    return fse.sort_values(ascending=False).index.tolist()[:topk]
</code></pre>
<hr>
<h2 id="4-LightGBM-建模"><a href="#4-LightGBM-建模" class="headerlink" title="4. LightGBM 建模"></a>4. LightGBM 建模</h2><h3 id="为什么选择-LightGBM"><a href="#为什么选择-LightGBM" class="headerlink" title="为什么选择 LightGBM"></a>为什么选择 LightGBM</h3><ul>
<li>速度快（直方图算法）</li>
<li>内存高效</li>
<li>支持类别特征</li>
<li>叶子优先生长策略（leaf-wise）</li>
</ul>
<h3 id="LightGBM-的原理"><a href="#LightGBM-的原理" class="headerlink" title="LightGBM 的原理"></a>LightGBM 的原理</h3><p>具体可参考之前的文章[集成学习之Boosting方法系列_LightGBM](<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_62895602/article/details/135789720?spm=1001.2014.3001.5501">集成学习之Boosting方法系列_LightGBM_light gradient boosting-CSDN博客</a>)</p>
<ul>
<li>每轮拟合残差</li>
<li>基于梯度提升更新</li>
<li>内置正则化减少过拟合</li>
</ul>
<hr>
<h2 id="5-TPE-超参数优化"><a href="#5-TPE-超参数优化" class="headerlink" title="5. TPE 超参数优化"></a>5. TPE 超参数优化</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>**TPE（Tree-structured Parzen Estimator，树结构化帕森估计器）**是一种基于贝叶斯思想的超参数优化方法，它的目标是在有限的试验次数内尽可能找到表现最优的参数组合。相比于传统的 <strong>Grid Search（网格搜索）</strong> 或 <strong>Random Search（随机搜索）</strong>，TPE 通过构建概率模型来“有策略地”探索参数空间，从而显著提升调参效率。</p>
<p>其核心思想可以分为以下几个步骤：</p>
<ol>
<li><p><strong>贝叶斯优化框架</strong></p>
<ul>
<li>在超参数优化问题中，我们要最小化一个目标函数 f(x)f(x)，其中 xx 是参数组合（如 LightGBM 的 <code>num_leaves</code>、<code>learning_rate</code> 等）。</li>
<li>贝叶斯优化的基本思想是：<ul>
<li>用一个概率模型（如高斯过程或 TPE）来近似目标函数。</li>
<li>在每一轮迭代时，利用概率模型选择最有希望的参数区域进行试验。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>概率建模</strong></p>
<ul>
<li>与高斯过程不同，TPE 使用了 <strong>密度估计</strong> 的方法来建模。</li>
<li>在优化过程中，TPE 将历史试验结果按照性能阈值 γ\gamma（如前 20% 的结果）划分为两类：<ul>
<li><strong>好的参数分布</strong>$l(x) &#x3D; p(x|f(x) &lt; \gamma)$</li>
<li><strong>差的参数分布</strong> $g(x) &#x3D; p(x|f(x) \geq \gamma)$</li>
</ul>
</li>
<li>这里的 $l(x) $和 $g(x) $都通过 <strong>Parzen 窗估计（核密度估计的一种）</strong> 来近似。</li>
</ul>
</li>
<li><p><strong>采样准则</strong></p>
<ul>
<li><p>TPE 选择新参数的原则是最大化以下比值：</p>
<p>$\text{argmax}_x \ \frac{l(x)}{g(x)}$</p>
</li>
<li><p>含义是：倾向于选择那些更可能来自“好分布”而不是“坏分布”的参数点。</p>
</li>
<li><p>与随机搜索相比，这种方法能够 <strong>自适应地聚焦在表现更优的区域</strong>，提高搜索效率。</p>
</li>
</ul>
</li>
<li><p><strong>树结构化空间支持</strong></p>
<ul>
<li>与普通贝叶斯优化不同，TPE 能够处理 <strong>树状依赖关系的搜索空间</strong>（Tree-structured Search Space）。</li>
<li>比如：如果选择 <code>boosting=gbdt</code>，才会去搜索 <code>num_leaves</code> 和 <code>feature_fraction</code>；如果选择 <code>boosting=rf</code>，则会去搜索 <code>subsample_freq</code> 等。</li>
<li>这种灵活性使得 TPE 特别适合机器学习调参中 <strong>条件依赖</strong> 的情况。</li>
</ul>
</li>
</ol>
<p>在本项目里，TPE 用于优化 LightGBM 的关键参数（如 <code>num_leaves</code>、<code>learning_rate</code>、<code>feature_fraction</code> 等），通过有限的 20 次试验，就能找到比默认参数更优的组合，从而降低 RMSE。</p>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code class="language-python">from hyperopt import fmin, tpe, hp

def params_append(params):
    params[&#39;feature_pre_filter&#39;] = False
    params[&#39;objective&#39;] = &#39;regression&#39;
    params[&#39;metric&#39;] = &#39;rmse&#39;
    params[&#39;bagging_seed&#39;] = 2020
    params[&#39;max_depth&#39;] = -1
    return params

def param_hyperopt(train_df, train_label):
    train_label_log = np.log1p(train_label)
    train_data = lgb.Dataset(train_df, label=train_label_log)

    def hyperopt_objective(params):
        params = params_append(params)
        res = lgb.cv(
            params,
            train_data,
            num_boost_round=800,
            nfold=3,
            stratified=False,
            shuffle=True,
            callbacks=[early_stopping(50), log_evaluation(100)]
        )
        return min(res[&#39;valid rmse-mean&#39;])

    params_space = &#123;
        &#39;learning_rate&#39;: hp.uniform(&#39;learning_rate&#39;, 0.05, 0.1),
        &#39;bagging_fraction&#39;: hp.uniform(&#39;bagging_fraction&#39;, 0.7, 1),
        &#39;feature_fraction&#39;: hp.uniform(&#39;feature_fraction&#39;, 0.7, 1),
        &#39;num_leaves&#39;: hp.choice(&#39;num_leaves&#39;, list(range(10, 30, 2))),
    &#125;

    params_best = fmin(
        fn=hyperopt_objective,
        space=params_space,
        algo=tpe.suggest,
        max_evals=20
    )
    return params_best
</code></pre>
<hr>
<h2 id="6-最终建模与结果提交"><a href="#6-最终建模与结果提交" class="headerlink" title="6. 最终建模与结果提交"></a>6. 最终建模与结果提交</h2><pre><code class="language-python"># 特征选择
feature_select = feature_select_wrapper(train_df, topk=50)
rf_train_df = train_df[feature_select]
rf_train_label = train_df[&#39;SalePrice&#39;]

# TPE 调优得到最佳参数
best_rf_params = param_hyperopt(rf_train_df, rf_train_label)

# 最优模型
best_model = lgb.LGBMRegressor(**best_rf_params, n_jobs=-1)
best_model.fit(rf_train_df, rf_train_label)

# 预测
rf_test_df = test_df[feature_select]
y_pred = best_model.predict(rf_test_df)

# 提交文件
submission = pd.DataFrame(&#123;
    &quot;Id&quot;: test_df[&quot;Id&quot;],
    &quot;SalePrice&quot;: y_pred
&#125;)
submission.to_csv(&quot;/kaggle/working/submission.csv&quot;, index=False)
</code></pre>
<hr>
<h2 id="7-实验结果与分析"><a href="#7-实验结果与分析" class="headerlink" title="7. 实验结果与分析"></a>7. 实验结果与分析</h2><ul>
<li><strong>Baseline LightGBM</strong>：RMSE ≈ 0.155</li>
<li><strong>Wrapper 特征选择后</strong>：RMSE ≈ 0.148</li>
<li><strong>Wrapper + LightGBM + TPE 调优</strong>：RMSE ≈ 0.141</li>
</ul>
<p>结果表明：</p>
<ul>
<li>Wrapper 有效减少冗余特征</li>
<li>TPE 自动调参提升了模型性能</li>
</ul>
<hr>
<h2 id="8-总结与展望"><a href="#8-总结与展望" class="headerlink" title="8. 总结与展望"></a>8. 总结与展望</h2><p>本文展示了完整的 Kaggle 房价预测流程：</p>
<ul>
<li>合理的缺失值处理</li>
<li>Wrapper 特征选择</li>
<li>LightGBM 高效建模</li>
<li>TPE 超参数调优</li>
</ul>
<p>未来可以进一步尝试：</p>
<ul>
<li>模型集成（XGBoost、CatBoost）</li>
<li>使用 Optuna 替代 Hyperopt</li>
<li>特征交互与自动特征工程</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Kaggle/" style="color: #00a596">Kaggle</a>
        </span>
        
    </div>
    <a href="/2022/09/08/kaggle%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E5%BB%BA%E6%A8%A1%EF%BC%9AWrapper%20%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%20LightGBM%20+%20TPE%20%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/08/30/pandas/">
        <h2 class="post-title">pandas</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/python/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                python
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/8/30
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h1><h2 id="pandas读取csv文件"><a href="#pandas读取csv文件" class="headerlink" title="pandas读取csv文件"></a>pandas读取csv文件</h2><pre><code class="language-python">import pandas as pd

df = pd.read_csv(&#39;file_path&#39;)
</code></pre>
<h2 id="pandas读取excel文件"><a href="#pandas读取excel文件" class="headerlink" title="pandas读取excel文件"></a>pandas读取excel文件</h2><pre><code class="language-python">import pandas as pd

df = pd.read_excel(&#39;file_path&#39;)
</code></pre>
<p>pandas一些常用式子</p>
<ol>
<li>条件筛选</li>
</ol>
<pre><code class="language-python">df[df[&#39;columns_name&#39;] &gt; value]
</code></pre>
<ol start="2">
<li>多条件筛选</li>
</ol>
<pre><code class="language-python">df[(df[&#39;col1&#39;] &gt; val1) &amp; (df[&#39;col2&#39;] == val2)]
</code></pre>
<ol start="3">
<li>排序</li>
</ol>
<pre><code class="language-python">df.sort_values(&#39;column_name&#39;)
</code></pre>
<ol start="4">
<li>分组聚合</li>
</ol>
<pre><code class="language-python">df.groupby(&#39;column_name&#39;).agg(&#123;&#39;other_col&#39;: &#39;sum&#39;&#125;)
</code></pre>
<ol start="5">
<li>连接合并</li>
</ol>
<pre><code class="language-python">pd.merge(df1, df2, on=&#39;key&#39;)
</code></pre>
<ol start="6">
<li>计数</li>
</ol>
<pre><code class="language-python">df[&#39;column_name&#39;].value_counts()
</code></pre>
<ol start="7">
<li>将某列拆成多列</li>
</ol>
<pre><code class="language-python">test_df[[&#39;month&#39;, &#39;sector_norm&#39;]] = test_df[&#39;id&#39;].str.split(&#39;_&#39;, expand=True)
</code></pre>
<ol start="9">
<li>找出缺失列</li>
</ol>
<pre><code class="language-python">missing_cols = X_test.columns[X_test.isnull().any()]
</code></pre>
<ol start="10">
<li>转换日期至标准格式</li>
</ol>
<pre><code class="language-python"># 假设month_norm 原本是2022 Jan
# Y代表四位年份，b代表月份缩写，m代表两位月份
X_train[&#39;month_index&#39;] = pd.to_datetime(X_train[&#39;month_norm&#39;], format=&quot;%Y %b&quot;)
</code></pre>
<ol start="11">
<li>寻找离散型与连续型列</li>
</ol>
<pre><code class="language-python"># 离散型
cat_cols = city_search_index.select_dtypes(include=[&#39;object&#39;]).columns
# 连续型
num_cols = city_search_index.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
</code></pre>
<ol start="12">
<li>连续子图展示</li>
</ol>
<pre><code class="language-python">sns.set()
import math

ncols = 1
nrows = math.ceil(len(missing_cols2) / ncols)
fig, axes = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3* nrows))
axes = axes.flatten()
for i, col in enumerate(missing_cols2):
    sns.histplot(new_house_transactions[col], kde=True, ax=axes[i])

plt.tight_layout()
plt.show()
</code></pre>
<ol start="13">
<li>去除行</li>
</ol>
<pre><code class="language-python">city_indexes = city_indexes.drop_duplicates(subset=[&#39;year&#39;])
# subset：用于指定根据哪几列去重。如果不指定，默认对所有列进行去重。
# keep：
# &#39;first&#39;（默认）：保留第一次出现的重复行，删除后续的重复行。
# &#39;last&#39;：保留最后一次出现的重复行，删除之前的。
# False：删除所有重复的行（只保留唯一的行，没有重复的）。
# inplace：
# False（默认）：返回一个去重后的新DataFrame，不修改原始数据。
# True：在原始DataFrame上直接修改，不返回新对象。
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/pandas/" style="color: #ff7d73">pandas</a>
        </span>
        
    </div>
    <a href="/2022/08/30/pandas/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/05/26/Leetcode%20%20sql%E9%A2%98%E7%9B%AE%E8%AE%B0%E5%BD%95/">
        <h2 class="post-title">Leetcode sql题目记录</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/Leetcode/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Leetcode
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/5/26
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="Leetcode-sql题目记录"><a href="#Leetcode-sql题目记录" class="headerlink" title="Leetcode  sql题目记录"></a>Leetcode  sql题目记录</h2><p>本博客仅记录**高频 SQL 50 题（基础版）**中的中等难度题目，后续不定期更新</p>
<h3 id="570-至少有5名直接下属的经理"><a href="#570-至少有5名直接下属的经理" class="headerlink" title="570. 至少有5名直接下属的经理"></a>570. 至少有5名直接下属的经理</h3><p>表: <code>Employee</code></p>
<pre><code>+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| id          | int     |
| name        | varchar |
| department  | varchar |
| managerId   | int     |
+-------------+---------+
id 是此表的主键（具有唯一值的列）。
该表的每一行表示雇员的名字、他们的部门和他们的经理的id。
如果managerId为空，则该员工没有经理。
没有员工会成为自己的管理者。
</code></pre>
<p>编写一个解决方案，找出至少有<strong>五个直接下属</strong>的经理。</p>
<p>以 <strong>任意顺序</strong> 返回结果表。</p>
<p>查询结果格式如下所示。</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入: 
Employee 表:
+-----+-------+------------+-----------+
| id  | name  | department | managerId |
+-----+-------+------------+-----------+
| 101 | John  | A          | Null      |
| 102 | Dan   | A          | 101       |
| 103 | James | A          | 101       |
| 104 | Amy   | A          | 101       |
| 105 | Anne  | A          | 101       |
| 106 | Ron   | B          | 101       |
+-----+-------+------------+-----------+
输出: 
+------+
| name |
+------+
| John |
+------+
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">select a.name from Employee a join Employee b on a.id = b.managerId group by a.id having count(*) &gt;= 5;
</code></pre>
<p>没有 <code>GROUP BY</code><br> <code>HAVING</code> 是对<strong>分组后的结果</strong>过滤的。你没分组时，MySQL把整张结果集当成<strong>一个大组</strong>，<code>COUNT(a.id)</code> 统计的是<strong>所有经理–下属配对的总行数</strong>，不是“每个经理的下属数”。因此当测试数据里只有一个经理且刚好有 5 个下属时看起来“正确”，但只要有两个经理，合计行数 ≥5，你这句就会把两位经理都放进结果里，甚至还会重复多行。</p>
<hr>
<h3 id="1934-确认率"><a href="#1934-确认率" class="headerlink" title="1934. 确认率"></a>1934. 确认率</h3><p>表: <code>Signups</code></p>
<pre><code>+----------------+----------+
| Column Name    | Type     |
+----------------+----------+
| user_id        | int      |
| time_stamp     | datetime |
+----------------+----------+
User_id是该表的主键。
每一行都包含ID为user_id的用户的注册时间信息。
</code></pre>
<p>表: <code>Confirmations</code></p>
<pre><code>+----------------+----------+
| Column Name    | Type     |
+----------------+----------+
| user_id        | int      |
| time_stamp     | datetime |
| action         | ENUM     |
+----------------+----------+
(user_id, time_stamp)是该表的主键。
user_id是一个引用到注册表的外键。
action是类型为(&#39;confirmed&#39;， &#39;timeout&#39;)的ENUM
该表的每一行都表示ID为user_id的用户在time_stamp请求了一条确认消息，该确认消息要么被确认(&#39;confirmed&#39;)，要么被过期(&#39;timeout&#39;)。
</code></pre>
<p>用户的 <strong>确认率</strong> 是 <code>&#39;confirmed&#39;</code> 消息的数量除以请求的确认消息的总数。没有请求任何确认消息的用户的确认率为 <code>0</code> 。确认率四舍五入到 <strong>小数点后两位</strong> 。</p>
<p>编写一个SQL查询来查找每个用户的 确认率 。</p>
<p>以 任意顺序 返回结果表。</p>
<p>查询结果格式如下所示。</p>
<p><strong>示例1:</strong></p>
<pre><code>输入：
Signups 表:
+---------+---------------------+
| user_id | time_stamp          |
+---------+---------------------+
| 3       | 2020-03-21 10:16:13 |
| 7       | 2020-01-04 13:57:59 |
| 2       | 2020-07-29 23:09:44 |
| 6       | 2020-12-09 10:39:37 |
+---------+---------------------+
Confirmations 表:
+---------+---------------------+-----------+
| user_id | time_stamp          | action    |
+---------+---------------------+-----------+
| 3       | 2021-01-06 03:30:46 | timeout   |
| 3       | 2021-07-14 14:00:00 | timeout   |
| 7       | 2021-06-12 11:57:29 | confirmed |
| 7       | 2021-06-13 12:58:28 | confirmed |
| 7       | 2021-06-14 13:59:27 | confirmed |
| 2       | 2021-01-22 00:00:00 | confirmed |
| 2       | 2021-02-28 23:59:59 | timeout   |
+---------+---------------------+-----------+
输出: 
+---------+-------------------+
| user_id | confirmation_rate |
+---------+-------------------+
| 6       | 0.00              |
| 3       | 0.00              |
| 7       | 1.00              |
| 2       | 0.50              |
+---------+-------------------+
解释:
用户 6 没有请求任何确认消息。确认率为 0。
用户 3 进行了 2 次请求，都超时了。确认率为 0。
用户 7 提出了 3 个请求，所有请求都得到了确认。确认率为 1。
用户 2 做了 2 个请求，其中一个被确认，另一个超时。确认率为 1 / 2 = 0.5。
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">WITH base AS (
  SELECT 
    user_id,
    COUNT(*) AS total,
    SUM(CASE WHEN action = &#39;confirmed&#39; THEN 1 ELSE 0 END) AS confirmed
  FROM Confirmations
  GROUP BY user_id
)
SELECT 
  s.user_id,
  ROUND(IFNULL(b.confirmed / b.total, 0), 2) AS confirmation_rate
FROM Signups s
LEFT JOIN base b
  ON s.user_id = b.user_id;
</code></pre>
<hr>
<h3 id="1193-每月交易I"><a href="#1193-每月交易I" class="headerlink" title="1193. 每月交易I"></a>1193. 每月交易I</h3><p>表：<code>Transactions</code></p>
<pre><code>+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| country       | varchar |
| state         | enum    |
| amount        | int     |
| trans_date    | date    |
+---------------+---------+
id 是这个表的主键。
该表包含有关传入事务的信息。
state 列类型为 [&quot;approved&quot;, &quot;declined&quot;] 之一。
</code></pre>
<p>编写一个 sql 查询来查找每个月和每个国家&#x2F;地区的事务数及其总金额、已批准的事务数及其总金额。</p>
<p>以 <strong>任意顺序</strong> 返回结果表。</p>
<p>查询结果格式如下所示。</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入：
Transactions table:
+------+---------+----------+--------+------------+
| id   | country | state    | amount | trans_date |
+------+---------+----------+--------+------------+
| 121  | US      | approved | 1000   | 2018-12-18 |
| 122  | US      | declined | 2000   | 2018-12-19 |
| 123  | US      | approved | 2000   | 2019-01-01 |
| 124  | DE      | approved | 2000   | 2019-01-07 |
+------+---------+----------+--------+------------+
输出：
+----------+---------+-------------+----------------+--------------------+-----------------------+
| month    | country | trans_count | approved_count | trans_total_amount | approved_total_amount |
+----------+---------+-------------+----------------+--------------------+-----------------------+
| 2018-12  | US      | 2           | 1              | 3000               | 1000                  |
| 2019-01  | US      | 1           | 1              | 2000               | 2000                  |
| 2019-01  | DE      | 1           | 1              | 2000               | 2000                  |
+----------+---------+-------------+----------------+--------------------+-----------------------+
</code></pre>
<p>解答：</p>
<p>多个 CTE 用 <code>WITH a AS (...), b AS (...)</code>。</p>
<p>按 <code>month、country</code> 分组；<code>approved</code> 相关用 <code>CASE WHEN</code> 计数&#x2F;求和。</p>
<p>取月份用 <code>DATE_FORMAT(trans_date, &#39;%Y-%m&#39;)</code> 更稳妥。</p>
<p><strong>DATE_FORMAT(…, ‘%y-%m’)：结果是20-02，若是’%Y-%m，结果是2020-02，若是’%Y-%M，结果是2020-Febrary’</strong></p>
<pre><code class="language-sql">WITH base AS (
  SELECT
    DATE_FORMAT(trans_date, &#39;%Y-%m&#39;) AS month,
    country,
    state,
    amount
  FROM Transactions
),
seq AS (
  SELECT
    month,
    country,
    COUNT(*) AS trans_count,
    SUM(CASE WHEN state = &#39;approved&#39; THEN 1 ELSE 0 END) AS approved_count,
    SUM(amount) AS trans_total_amount,
    SUM(CASE WHEN state = &#39;approved&#39; THEN amount ELSE 0 END) AS approved_total_amount
  FROM base
  GROUP BY month, country
)
SELECT *
FROM seq;
</code></pre>
<hr>
<h3 id="1174-即时食物配送II"><a href="#1174-即时食物配送II" class="headerlink" title="1174. 即时食物配送II"></a>1174. 即时食物配送II</h3><p>配送表: <code>Delivery</code></p>
<pre><code>+-----------------------------+---------+
| Column Name                 | Type    |
+-----------------------------+---------+
| delivery_id                 | int     |
| customer_id                 | int     |
| order_date                  | date    |
| customer_pref_delivery_date | date    |
+-----------------------------+---------+
delivery_id 是该表中具有唯一值的列。
该表保存着顾客的食物配送信息，顾客在某个日期下了订单，并指定了一个期望的配送日期（和下单日期相同或者在那之后）。
</code></pre>
<p>如果顾客期望的配送日期和下单日期相同，则该订单称为 「<strong>即时订单</strong>」，否则称为「<strong>计划订单</strong>」。</p>
<p>「<strong>首次订单</strong>」是顾客最早创建的订单。我们保证一个顾客只会有一个「首次订单」。</p>
<p>编写解决方案以获取即时订单在所有用户的首次订单中的比例。<strong>保留两位小数。</strong></p>
<p>结果示例如下所示：</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Delivery 表：
+-------------+-------------+------------+-----------------------------+
| delivery_id | customer_id | order_date | customer_pref_delivery_date |
+-------------+-------------+------------+-----------------------------+
| 1           | 1           | 2019-08-01 | 2019-08-02                  |
| 2           | 2           | 2019-08-02 | 2019-08-02                  |
| 3           | 1           | 2019-08-11 | 2019-08-12                  |
| 4           | 3           | 2019-08-24 | 2019-08-24                  |
| 5           | 3           | 2019-08-21 | 2019-08-22                  |
| 6           | 2           | 2019-08-11 | 2019-08-13                  |
| 7           | 4           | 2019-08-09 | 2019-08-09                  |
+-------------+-------------+------------+-----------------------------+
输出：
+----------------------+
| immediate_percentage |
+----------------------+
| 50.00                |
+----------------------+
解释：
1 号顾客的 1 号订单是首次订单，并且是计划订单。
2 号顾客的 2 号订单是首次订单，并且是即时订单。
3 号顾客的 5 号订单是首次订单，并且是计划订单。
4 号顾客的 7 号订单是首次订单，并且是即时订单。
因此，一半顾客的首次订单是即时的。
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">with base as (
select *, row_number() over(partition by customer_id order by order_date) as rn 
from Delivery),
seq as (
    select round((sum(case when customer_pref_delivery_date = order_date then 1 else 0 end) / count(*)) * 100, 2) as immediate_percentage from base where rn = 1
)
select * from seq;
</code></pre>
<hr>
<h3 id="176-第二高的薪水"><a href="#176-第二高的薪水" class="headerlink" title="176. 第二高的薪水"></a>176. 第二高的薪水</h3><p><code>Employee</code> 表：</p>
<pre><code>+-------------+------+
| Column Name | Type |
+-------------+------+
| id          | int  |
| salary      | int  |
+-------------+------+
id 是这个表的主键。
表的每一行包含员工的工资信息。
</code></pre>
<p>查询并返回 <code>Employee</code> 表中第二高的 <strong>不同</strong> 薪水 。如果不存在第二高的薪水，查询应该返回 <code>null(Pandas 则返回 None)</code> 。</p>
<p>查询结果如下例所示。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Employee 表：
+----+--------+
| id | salary |
+----+--------+
| 1  | 100    |
| 2  | 200    |
| 3  | 300    |
+----+--------+
输出：
+---------------------+
| SecondHighestSalary |
+---------------------+
| 200                 |
+---------------------+
</code></pre>
<p><strong>示例 2：</strong></p>
<pre><code>输入：
Employee 表：
+----+--------+
| id | salary |
+----+--------+
| 1  | 100    |
+----+--------+
输出：
+---------------------+
| SecondHighestSalary |
+---------------------+
| null                |
+---------------------+
</code></pre>
<p>解答</p>
<pre><code class="language-sql">SELECT MAX(salary) AS SecondHighestSalary
FROM (
  SELECT DISTINCT salary,
         DENSE_RANK() OVER (ORDER BY salary DESC) AS rk
  FROM Employee
) t
WHERE rk = 2;
</code></pre>
<h4 id="（1）-子查询为空但外层用了聚合函数"><a href="#（1）-子查询为空但外层用了聚合函数" class="headerlink" title="（1） 子查询为空但外层用了聚合函数"></a>（1） 子查询为空但外层用了聚合函数</h4><p>SQL 的定义是：聚合函数对空输入时，返回 <strong>NULL</strong>。<br> 例子：</p>
<pre><code class="language-sql">SELECT MAX(salary) FROM (SELECT 1 AS salary WHERE 1=0) t;
</code></pre>
<p><code>(SELECT 1 WHERE 1=0)</code> 为空表 → <code>MAX</code> 没有任何值可比较 → 返回 <code>NULL</code>。</p>
<p>所以这就是为什么“第二高薪水不存在”时 <code>MAX</code> 可以帮我们自动返回 <code>NULL</code>。</p>
<hr>
<h4 id="（2）子查询为空而外层没有聚合函数"><a href="#（2）子查询为空而外层没有聚合函数" class="headerlink" title="（2）子查询为空而外层没有聚合函数"></a>（2）子查询为空而外层没有聚合函数</h4><p>如果你直接：</p>
<pre><code class="language-sql">SELECT salary 
FROM (SELECT salary WHERE 1=0) t;
</code></pre>
<p>那结果就是 <strong>0 行</strong>（即直接返回空结果集，不会自动给你一行 <code>NULL</code>）。</p>
<hr>
<h3 id="550-游戏玩法分析IV"><a href="#550-游戏玩法分析IV" class="headerlink" title="550. 游戏玩法分析IV"></a>550. 游戏玩法分析IV</h3><p>Table: <code>Activity</code></p>
<pre><code>+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| player_id    | int     |
| device_id    | int     |
| event_date   | date    |
| games_played | int     |
+--------------+---------+
（player_id，event_date）是此表的主键（具有唯一值的列的组合）。
这张表显示了某些游戏的玩家的活动情况。
每一行是一个玩家的记录，他在某一天使用某个设备注销之前登录并玩了很多游戏（可能是 0）。
</code></pre>
<p>编写解决方案，报告在首次登录的第二天再次登录的玩家的 <strong>比率</strong>，<strong>四舍五入到小数点后两位</strong>。换句话说，你需要计算从首次登录后的第二天登录的玩家数量，并将其除以总玩家数。</p>
<p>结果格式如下所示：</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-03-02 | 6            |
| 2         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-02 | 0            |
| 3         | 4         | 2018-07-03 | 5            |
+-----------+-----------+------------+--------------+
输出：
+-----------+
| fraction  |
+-----------+
| 0.33      |
+-----------+
解释：
只有 ID 为 1 的玩家在第一天登录后才重新登录，所以答案是 1/3 = 0.33
</code></pre>
<p>解答：</p>
<pre><code class="language-sql"># Write your MySQL query statement below
with base as(
    select *,min(event_date) over(partition by player_id)as first from Activity
),
seq as(
    select player_id, sum(case when datediff(event_date, first) = 1 then 1 else 0 end) as rn from base
    group by player_id
)
select round(IFNULL((sum(rn) / count(distinct player_id)), 0), 2) as fraction from seq;
</code></pre>
<p>注意：</p>
<ol>
<li>在使用with base as(), seq as() select from seq时，要记得每个嵌套内部的select很重要，前一个select需要包含后一个select的内容。</li>
<li>使用聚合函数min,max,count后结果会被压缩，如果后续要同行对最大、最小进行比较，要使用窗口函数形式的max,min…</li>
</ol>
<hr>
<h3 id="1045-买下所有产品的客户"><a href="#1045-买下所有产品的客户" class="headerlink" title="1045. 买下所有产品的客户"></a>1045. 买下所有产品的客户</h3><p><code>Customer</code> 表：</p>
<pre><code>+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| customer_id | int     |
| product_key | int     |
+-------------+---------+
该表可能包含重复的行。
customer_id 不为 NULL。
product_key 是 Product 表的外键(reference 列)。
</code></pre>
<p><code>Product</code> 表：</p>
<pre><code>+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| product_key | int     |
+-------------+---------+
product_key 是这张表的主键（具有唯一值的列）。
</code></pre>
<p>编写解决方案，报告 <code>Customer</code> 表中购买了 <code>Product</code> 表中所有产品的客户的 id。</p>
<p>返回结果表 <strong>无顺序要求</strong> 。</p>
<p>返回结果格式如下所示。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Customer 表：
+-------------+-------------+
| customer_id | product_key |
+-------------+-------------+
| 1           | 5           |
| 2           | 6           |
| 3           | 5           |
| 3           | 6           |
| 1           | 6           |
+-------------+-------------+
Product 表：
+-------------+
| product_key |
+-------------+
| 5           |
| 6           |
+-------------+
输出：
+-------------+
| customer_id |
+-------------+
| 1           |
| 3           |
+-------------+
解释：
购买了所有产品（5 和 6）的客户的 id 是 1 和 3 。
</code></pre>
<p>解答：</p>
<ol>
<li><strong><code>WHERE DISTINCT</code> 是非法语法</strong>。<code>DISTINCT</code> 只能放在 <code>SELECT</code> 或 <code>COUNT(DISTINCT ...)</code> 里面，不能直接放在 <code>WHERE</code>。</li>
<li>题目要求的是“买了所有产品的顾客”，所以应该比较顾客买到的 <strong>去重后产品数</strong> 和 <strong>Product 表里的产品总数</strong>。</li>
</ol>
<pre><code class="language-sql">SELECT customer_id
FROM Customer
GROUP BY customer_id
HAVING COUNT(DISTINCT product_key) = (
    SELECT COUNT(*) FROM Product
);
</code></pre>
<p>解释一下：</p>
<ul>
<li><code>GROUP BY customer_id</code>：按顾客分组。</li>
<li><code>COUNT(DISTINCT product_key)</code>：统计该顾客买了多少种不同的产品。</li>
<li>子查询 <code>(SELECT COUNT(*) FROM Product)</code>：统计总共有多少种产品。</li>
<li><code>HAVING ... = ...</code>：筛选出买全所有产品的顾客。</li>
</ul>
<hr>
<h3 id="180-连续出现的数字"><a href="#180-连续出现的数字" class="headerlink" title="180. 连续出现的数字"></a>180. 连续出现的数字</h3><p>表：<code>Logs</code></p>
<pre><code>+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| id          | int     |
| num         | varchar |
+-------------+---------+
在 SQL 中，id 是该表的主键。
id 是一个自增列。
</code></pre>
<p>找出所有至少连续出现三次的数字。</p>
<p>返回的结果表中的数据可以按 <strong>任意顺序</strong> 排列。</p>
<p>结果格式如下面的例子所示：</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入：
Logs 表：
+----+-----+
| id | num |
+----+-----+
| 1  | 1   |
| 2  | 1   |
| 3  | 1   |
| 4  | 2   |
| 5  | 1   |
| 6  | 2   |
| 7  | 2   |
+----+-----+
输出：
Result 表：
+-----------------+
| ConsecutiveNums |
+-----------------+
| 1               |
+-----------------+
解释：1 是唯一连续出现至少三次的数字。
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">with base as(
    select *, row_number() over(partition by num order by id) as rn from Logs
),
seq as(
    select *, id - rn as diff from base 
),
rpq as (
    select num, count(*) as ct from seq group by num, diff having count(*) &gt;= 3
)
select distinct num as ConsecutiveNums from rpq;
</code></pre>
<p><strong>有时候不要忘记distinct</strong></p>
<hr>
<h3 id="1164-指定日期的产品价格"><a href="#1164-指定日期的产品价格" class="headerlink" title="1164. 指定日期的产品价格"></a>1164. 指定日期的产品价格</h3><p>产品数据表: <code>Products</code></p>
<pre><code>+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| new_price     | int     |
| change_date   | date    |
+---------------+---------+
(product_id, change_date) 是此表的主键（具有唯一值的列组合）。
这张表的每一行分别记录了 某产品 在某个日期 更改后 的新价格。
</code></pre>
<p>一开始，所有产品价格都为 10。</p>
<p>编写一个解决方案，找出在 <code>2019-08-16</code> 所有产品的价格。</p>
<p>以 <strong>任意顺序</strong> 返回结果表。</p>
<p>结果格式如下例所示。</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入：
Products 表:
+------------+-----------+-------------+
| product_id | new_price | change_date |
+------------+-----------+-------------+
| 1          | 20        | 2019-08-14  |
| 2          | 50        | 2019-08-14  |
| 1          | 30        | 2019-08-15  |
| 1          | 35        | 2019-08-16  |
| 2          | 65        | 2019-08-17  |
| 3          | 20        | 2019-08-18  |
+------------+-----------+-------------+
输出：
+------------+-------+
| product_id | price |
+------------+-------+
| 2          | 50    |
| 1          | 35    |
| 3          | 10    |
+------------+-------+
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">with base as(
    select *, row_number()over (partition by product_id order by change_date DESC) as rn from Products where change_date &lt;= &#39;2019-08-16&#39;
),
seq as (
    select product_id, new_price as price from base where rn = 1
)
select product_id, price from seq union all select distinct product_id, 10 as price from (
    select product_id, min(change_date) as mi from Products group by product_id
)t  where mi &gt; &#39;2019-08-16&#39;;
</code></pre>
<p>归纳：<br>想要得到某一列的最大最小值，并用其与某个数进行比较，需要先max、min（窗口或者聚合函数），然后再等值查询，或者rownumber配合ASC、DESC，然后再后续查询中使用rn&#x3D;1，或者直接使用order by+limit</p>
<h4 id="（1）非相关子查询-等值比较"><a href="#（1）非相关子查询-等值比较" class="headerlink" title="（1）非相关子查询 + 等值比较"></a>（1）非相关子查询 + 等值比较</h4><p><strong>使用场景</strong>：<strong>全局只有一个最值</strong>（整个表的最大值&#x2F;最小值），不依赖分组。</p>
<ul>
<li>特点：子查询返回单行，可以直接用 <code>=</code> 比较。</li>
<li>示例：找出工资最高的员工</li>
</ul>
<pre><code class="language-sql">select *
from Employee
where salary = (select max(salary) from Employee);
</code></pre>
<hr>
<h4 id="（2）相关子查询"><a href="#（2）相关子查询" class="headerlink" title="（2）相关子查询"></a>（2）相关子查询</h4><p><strong>使用场景</strong>：<strong>分组后的最值</strong>（每个客户&#x2F;部门的最小&#x2F;最大日期），外层表的条件依赖子查询。</p>
<ul>
<li>特点：子查询里需要引用外层的列，每行执行一次，保证返回单行。</li>
<li>示例：找每个客户的首单</li>
</ul>
<pre><code class="language-sql">select *
from Delivery d1
where order_date = (
  select min(order_date)
  from Delivery d2
  where d2.customer_id = d1.customer_id
);
</code></pre>
<p>这里子查询里的 <code>d2.customer_id = d1.customer_id</code> 绑定了外层查询的 <code>d1.customer_id</code>。</p>
<p>每次外层取一行，子查询就只计算该客户的最小订单日期。</p>
<p>也可以写成这种形式：</p>
<pre><code class="language-sql">select *
from (
  select customer_id, min(order_date)
  from Delivery 
  group by customer_id
)t ;
</code></pre>
<hr>
<h4 id="（3）窗口函数-条件筛选"><a href="#（3）窗口函数-条件筛选" class="headerlink" title="（3）窗口函数 + 条件筛选"></a>（3）窗口函数 + 条件筛选</h4><p><strong>使用场景</strong>：数据库支持窗口函数（MySQL 8+ &#x2F; PostgreSQL &#x2F; Oracle &#x2F; SQL Server），<strong>需要按组取前几名&#x2F;最值。</strong></p>
<p>也可以使用max、min的窗口函数形式来获取每个组别的最值</p>
<ul>
<li>特点：写法简洁，可同时保留分组内排序信息。</li>
<li>示例：找每个客户的首单</li>
</ul>
<pre><code class="language-sql">with t as (
  select *,
         row_number() over(partition by customer_id order by order_date) as rn
  from Delivery
)
select *
from t
where rn = 1;
</code></pre>
<ul>
<li>如果要处理并列情况，可以用 <code>rank()</code> 或 <code>dense_rank()</code>。</li>
</ul>
<hr>
<h4 id="（4）ORDER-BY-LIMIT"><a href="#（4）ORDER-BY-LIMIT" class="headerlink" title="（4）ORDER BY + LIMIT"></a>（4）ORDER BY + LIMIT</h4><p><strong>使用场景</strong>：<strong>只需要全表范围内的最值</strong>，且只取前 N 条（不分组）。</p>
<ul>
<li>特点：简单高效，但无法直接应对「每组最值」问题。</li>
<li>示例：找工资最高的员工（Top 1）</li>
</ul>
<pre><code class="language-sql">select *
from Employee
order by salary desc
limit 1;
</code></pre>
<ul>
<li>如果需要每组最值，就不适合，只能配合窗口函数或子查询。</li>
</ul>
<hr>
<h4 id="对比总结"><a href="#对比总结" class="headerlink" title="对比总结"></a>对比总结</h4><table>
<thead>
<tr>
<th>方法</th>
<th>适用场景</th>
<th>优点</th>
<th>局限</th>
</tr>
</thead>
<tbody><tr>
<td><strong>非相关子查询</strong></td>
<td>全表唯一最值</td>
<td>简单</td>
<td>只能处理单个全局最值</td>
</tr>
<tr>
<td><strong>相关子查询</strong></td>
<td>每组最值（需要依赖外层条件）</td>
<td>通用，不要求窗口函数</td>
<td>子查询多次执行，性能较差</td>
</tr>
<tr>
<td><strong>窗口函数</strong></td>
<td>每组最值&#x2F;前 N 个</td>
<td>高效简洁，现代 SQL 推荐</td>
<td>需要数据库支持窗口函数</td>
</tr>
<tr>
<td><strong>ORDER BY + LIMIT</strong></td>
<td>全表前 N 个</td>
<td>简洁高效</td>
<td>不能分组，只能处理全局最值</td>
</tr>
</tbody></table>
<hr>
<p>👉 快速记忆：</p>
<ul>
<li><strong>全局最值</strong> → 非相关子查询 或 <code>order by + limit</code></li>
<li><strong>分组最值</strong> → 相关子查询 或 窗口函数</li>
<li><strong>要前 N 名&#x2F;并列情况</strong> → 窗口函数最佳</li>
</ul>
<hr>
<h3 id="1204-最后一个能进入巴士的人"><a href="#1204-最后一个能进入巴士的人" class="headerlink" title="1204.最后一个能进入巴士的人"></a>1204.最后一个能进入巴士的人</h3><p>表: <code>Queue</code></p>
<pre><code>+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| person_id   | int     |
| person_name | varchar |
| weight      | int     |
| turn        | int     |
+-------------+---------+
person_id 是这个表具有唯一值的列。
该表展示了所有候车乘客的信息。
表中 person_id 和 turn 列将包含从 1 到 n 的所有数字，其中 n 是表中的行数。
turn 决定了候车乘客上巴士的顺序，其中 turn=1 表示第一个上巴士，turn=n 表示最后一个上巴士。
weight 表示候车乘客的体重，以千克为单位。
</code></pre>
<p>有一队乘客在等着上巴士。然而，巴士有<code>1000</code> <strong>千克</strong> 的重量限制，所以其中一部分乘客可能无法上巴士。</p>
<p>编写解决方案找出 <strong>最后一个</strong> 上巴士且不超过重量限制的乘客，并报告 <code>person_name</code> 。题目测试用例确保顺位第一的人可以上巴士且不会超重。</p>
<p>返回结果格式如下所示。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Queue 表
+-----------+-------------+--------+------+
| person_id | person_name | weight | turn |
+-----------+-------------+--------+------+
| 5         | Alice       | 250    | 1    |
| 4         | Bob         | 175    | 5    |
| 3         | Alex        | 350    | 2    |
| 6         | John Cena   | 400    | 3    |
| 1         | Winston     | 500    | 6    |
| 2         | Marie       | 200    | 4    |
+-----------+-------------+--------+------+
输出：
+-------------+
| person_name |
+-------------+
| John Cena   |
+-------------+
解释：
为了简化，Queue 表按 turn 列由小到大排序。
+------+----+-----------+--------+--------------+
| Turn | ID | Name      | Weight | Total Weight |
+------+----+-----------+--------+--------------+
| 1    | 5  | Alice     | 250    | 250          |
| 2    | 3  | Alex      | 350    | 600          |
| 3    | 6  | John Cena | 400    | 1000         | (最后一个上巴士)
| 4    | 2  | Marie     | 200    | 1200         | (无法上巴士)
| 5    | 4  | Bob       | 175    | ___          |
| 6    | 1  | Winston   | 500    | ___          |
+------+----+-----------+--------+--------------+
</code></pre>
<p>解答：</p>
<p><strong>PS：SQL 聚合函数（如 <code>MAX()</code>）和普通列之间没有自动的“行对齐”关系。</strong></p>
<p>不能直接写成！：</p>
<pre><code class="language-sql">select person_name, max(total_weight) from (
    select * , sum(weight)over (order by turn) as total_weight from Queue
)t where total_weight &lt;= 1000;

-- 结果只会返回
| person_id | person_name | weight | turn |
| --------- | ----------- | ------ | ---- |
| 5         | Alice       | 250    | 1    |
| 4         | Bob         | 175    | 5    |
| 3         | Alex        | 350    | 2    |
| 6         | John Cena   | 400    | 3    |
| 1         | Winston     | 500    | 6    |
| 2         | Marie       | 200    | 4    |

| person_name | max(total_weight) |
| ----------- | ----------------- |
| Alice       | 1000              |

-- 而不是正确输出
| person_name |
| ----------- |
| John Cena   |
</code></pre>
<h4 id="（1）子查询-等值过滤"><a href="#（1）子查询-等值过滤" class="headerlink" title="（1）子查询+等值过滤"></a>（1）子查询+等值过滤</h4><pre><code class="language-sql"># Write your MySQL query statement below
with base as(
    select * , sum(weight)over (order by turn) as total_weight from Queue
)
select person_name from base where total_weight = (
    select max(total_weight) as mx from base where total_weight &lt;= 1000
);
</code></pre>
<hr>
<h4 id="（2）ORDER-BY-LIMIT"><a href="#（2）ORDER-BY-LIMIT" class="headerlink" title="（2）ORDER BY+LIMIT"></a>（2）ORDER BY+LIMIT</h4><pre><code class="language-sql"># Write your MySQL query statement below
select person_name from (
    select * , sum(weight)over (order by turn) as total_weight from Queue
)t where total_weight &lt;= 1000 order by total_weight DESC limit 1;
</code></pre>
<hr>
<h3 id="1907-按分类统计薪水"><a href="#1907-按分类统计薪水" class="headerlink" title="1907. 按分类统计薪水"></a>1907. 按分类统计薪水</h3><p>表: <code>Accounts</code></p>
<pre><code>+-------------+------+
| 列名        | 类型  |
+-------------+------+
| account_id  | int  |
| income      | int  |
+-------------+------+
在 SQL 中，account_id 是这个表的主键。
每一行都包含一个银行帐户的月收入的信息。
</code></pre>
<p>查询每个工资类别的银行账户数量。 工资类别如下：</p>
<ul>
<li><code>&quot;Low Salary&quot;</code>：所有工资 <strong>严格低于</strong> <code>20000</code> 美元。</li>
<li><code>&quot;Average Salary&quot;</code>： <strong>包含</strong> 范围内的所有工资 <code>[$20000, $50000]</code> 。</li>
<li><code>&quot;High Salary&quot;</code>：所有工资 <strong>严格大于</strong> <code>50000</code> 美元。</li>
</ul>
<p>结果表 <strong>必须</strong> 包含所有三个类别。 如果某个类别中没有帐户，则报告 <code>0</code> 。</p>
<p>按 <strong>任意顺序</strong> 返回结果表。</p>
<p>查询结果格式如下示例。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Accounts 表:
+------------+--------+
| account_id | income |
+------------+--------+
| 3          | 108939 |
| 2          | 12747  |
| 8          | 87709  |
| 6          | 91796  |
+------------+--------+
输出：
+----------------+----------------+
| category       | accounts_count |
+----------------+----------------+
| Low Salary     | 1              |
| Average Salary | 0              |
| High Salary    | 3              |
+----------------+----------------+
解释：
低薪: 有一个账户 2.
中等薪水: 没有.
高薪: 有三个账户，他们是 3, 6和 8.
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">select &#39;Low Salary&#39; as category, sum(case when income &lt; 20000 then 1 else 0 end) as accounts_count
from Accounts
Union all
select &#39;Average Salary&#39; as category, sum(case when income &gt;= 20000 and income &lt;= 50000 then 1 else 0 end) as accounts_count
from Accounts
Union all
select &#39;High Salary&#39; as category, sum(case when income &gt; 50000 then 1 else 0 end) as accounts_count
from Accounts;
</code></pre>
<hr>
<h3 id="626-换座位"><a href="#626-换座位" class="headerlink" title="626. 换座位"></a>626. 换座位</h3><p>表: <code>Seat</code></p>
<pre><code>+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| id          | int     |
| student     | varchar |
+-------------+---------+
id 是该表的主键（唯一值）列。
该表的每一行都表示学生的姓名和 ID。
ID 序列始终从 1 开始并连续增加。
</code></pre>
<p>编写解决方案来交换每两个连续的学生的座位号。如果学生的数量是奇数，则最后一个学生的id不交换。</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入: 
Seat 表:
+----+---------+
| id | student |
+----+---------+
| 1  | Abbot   |
| 2  | Doris   |
| 3  | Emerson |
| 4  | Green   |
| 5  | Jeames  |
+----+---------+
输出: 
+----+---------+
| id | student |
+----+---------+
| 1  | Doris   |
| 2  | Abbot   |
| 3  | Green   |
| 4  | Emerson |
| 5  | Jeames  |
+----+---------+
解释:
请注意，如果学生人数为奇数，则不需要更换最后一名学生的座位。
</code></pre>
<p>解答：</p>
<h4 id="（1）嵌套查询"><a href="#（1）嵌套查询" class="headerlink" title="（1）嵌套查询"></a>（1）嵌套查询</h4><pre><code class="language-sql"># Write your MySQL query statement below
with base as(
    select *, lead(student, 1)over (order by id) as exchange_nxt, lag(student, 1)over (order by id) as exchange_pre from Seat
)
select id, IFNULL((case when (id % 2) != 0 then exchange_nxt else exchange_pre end), student) as student from base;
</code></pre>
<h4 id="（2）CASE-WHEN"><a href="#（2）CASE-WHEN" class="headerlink" title="（2）CASE WHEN"></a>（2）CASE WHEN</h4><pre><code class="language-sql"># Write your MySQL query statement below
select case 
        when id % 2 = 1 and id &lt; (select max(id) from Seat)
            then id + 1
        when id % 2 = 0 
            then id - 1
        else id
        end as id, student
    from Seat
order by id;
</code></pre>
<p>在 SQL 里，<code>CASE</code> 可以写两种形式：</p>
<h5 id="a-简单-CASE"><a href="#a-简单-CASE" class="headerlink" title="a. 简单 CASE"></a>a. 简单 CASE</h5><p>直接对某个表达式的值做匹配：</p>
<pre><code class="language-sql">CASE column_name
    WHEN &#39;A&#39; THEN &#39;类型1&#39;
    WHEN &#39;B&#39; THEN &#39;类型2&#39;
    ELSE &#39;其他&#39;
END
</code></pre>
<h5 id="b-搜索-CASE-（最常用）"><a href="#b-搜索-CASE-（最常用）" class="headerlink" title="b. 搜索 CASE （最常用）"></a>b. 搜索 CASE （最常用）</h5><p>条件判断灵活，可以写比较、范围等：</p>
<pre><code class="language-sql">CASE
    WHEN score &gt;= 90 THEN &#39;优秀&#39;
    WHEN score &gt;= 60 THEN &#39;及格&#39;
    ELSE &#39;不及格&#39;
END
</code></pre>
<h5 id="c-作为一个字段输出"><a href="#c-作为一个字段输出" class="headerlink" title="c. 作为一个字段输出"></a>c. 作为一个字段输出</h5><pre><code class="language-sql">SELECT name,
       CASE WHEN age &lt; 18 THEN &#39;未成年&#39;
            WHEN age &lt; 60 THEN &#39;成年人&#39;
            ELSE &#39;老年人&#39;
       END AS age_group
FROM users;
</code></pre>
<h5 id="d-在聚合函数里做条件统计"><a href="#d-在聚合函数里做条件统计" class="headerlink" title="d. 在聚合函数里做条件统计"></a>d. 在聚合函数里做条件统计</h5><pre><code class="language-sql">SELECT
    SUM(CASE WHEN status = &#39;success&#39; THEN 1 ELSE 0 END) AS success_count,
    SUM(CASE WHEN status = &#39;fail&#39; THEN 1 ELSE 0 END)    AS fail_count
FROM logs;
</code></pre>
<p>常用于 <strong>条件计数</strong>。</p>
<h5 id="e-排序（ORDER-BY）"><a href="#e-排序（ORDER-BY）" class="headerlink" title="e. 排序（ORDER BY）"></a>e. 排序（ORDER BY）</h5><pre><code class="language-sql">SELECT *
FROM orders
ORDER BY
  CASE 
    WHEN status = &#39;vip&#39; THEN 1
    WHEN status = &#39;normal&#39; THEN 2
    ELSE 3
  END;
</code></pre>
<hr>
<h3 id="1341-电影评分"><a href="#1341-电影评分" class="headerlink" title="1341. 电影评分"></a>1341. 电影评分</h3><p>表：<code>Movies</code></p>
<pre><code>+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| movie_id      | int     |
| title         | varchar |
+---------------+---------+
movie_id 是这个表的主键(具有唯一值的列)。
title 是电影的名字。
</code></pre>
<p>表：<code>Users</code></p>
<pre><code>+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| name          | varchar |
+---------------+---------+
user_id 是表的主键(具有唯一值的列)。
&#39;name&#39; 列具有唯一值。
</code></pre>
<p>表：<code>MovieRating</code></p>
<pre><code>+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| movie_id      | int     |
| user_id       | int     |
| rating        | int     |
| created_at    | date    |
+---------------+---------+
(movie_id, user_id) 是这个表的主键(具有唯一值的列的组合)。
这个表包含用户在其评论中对电影的评分 rating 。
created_at 是用户的点评日期。 
</code></pre>
<p>请你编写一个解决方案：</p>
<ul>
<li>查找评论电影数量最多的用户名。如果出现平局，返回字典序较小的用户名。</li>
<li>查找在 <code>February 2020</code> <strong>平均评分最高</strong> 的电影名称。如果出现平局，返回字典序较小的电影名称。</li>
</ul>
<p><strong>字典序</strong> ，即按字母在字典中出现顺序对字符串排序，字典序较小则意味着排序靠前。</p>
<p>返回结果格式如下例所示。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Movies 表：
+-------------+--------------+
| movie_id    |  title       |
+-------------+--------------+
| 1           | Avengers     |
| 2           | Frozen 2     |
| 3           | Joker        |
+-------------+--------------+
Users 表：
+-------------+--------------+
| user_id     |  name        |
+-------------+--------------+
| 1           | Daniel       |
| 2           | Monica       |
| 3           | Maria        |
| 4           | James        |
+-------------+--------------+
MovieRating 表：
+-------------+--------------+--------------+-------------+
| movie_id    | user_id      | rating       | created_at  |
+-------------+--------------+--------------+-------------+
| 1           | 1            | 3            | 2020-01-12  |
| 1           | 2            | 4            | 2020-02-11  |
| 1           | 3            | 2            | 2020-02-12  |
| 1           | 4            | 1            | 2020-01-01  |
| 2           | 1            | 5            | 2020-02-17  | 
| 2           | 2            | 2            | 2020-02-01  | 
| 2           | 3            | 2            | 2020-03-01  |
| 3           | 1            | 3            | 2020-02-22  | 
| 3           | 2            | 4            | 2020-02-25  | 
+-------------+--------------+--------------+-------------+
输出：
Result 表：
+--------------+
| results      |
+--------------+
| Daniel       |
| Frozen 2     |
+--------------+
解释：
Daniel 和 Monica 都点评了 3 部电影（&quot;Avengers&quot;, &quot;Frozen 2&quot; 和 &quot;Joker&quot;） 但是 Daniel 字典序比较小。
Frozen 2 和 Joker 在 2 月的评分都是 3.5，但是 Frozen 2 的字典序比较小。
</code></pre>
<p><strong>解答：</strong></p>
<pre><code class="language-sql">with base as(
    select *, avg(rating) as avg_rate from (
        select *, date_format(created_at, &#39;%Y-%m&#39;) as created_month from MovieRating 
        where date_format(created_at, &#39;%Y-%m&#39;) = &#39;2020-02&#39;
    )a group by movie_id
),
seq as (
    select movie_id from base where avg_rate = (
        select max(avg_rate) from base 
    )
),
base2 as(
    select s.user_id from(
        select t.user_id, rank() over(order by ct DESC) as rk from(
            select user_id, count(*) as ct from  MovieRating group by user_id
        )t
    )s where s.rk = 1
)
select min(a.name) as results from Users a join base2 b on a.user_id = b.user_id union all
    select min(a.title) as results from Movies a join seq b on a.movie_id = b.movie_id;
</code></pre>
<p><strong>PS:</strong></p>
<p>（1）如果需要获取某一列中不同值的总数，eg:id列1,2,3出现的次数，可以</p>
<pre><code class="language-sql">select count(*) from emp group by id
</code></pre>
<p>（2）等值子查询只能返回一个结果，子查询里select的数值不能超过一行</p>
<hr>
<h3 id="1321-餐厅营业额变化增长"><a href="#1321-餐厅营业额变化增长" class="headerlink" title="1321. 餐厅营业额变化增长"></a>1321. 餐厅营业额变化增长</h3><p>表: <code>Customer</code></p>
<pre><code>+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| name          | varchar |
| visited_on    | date    |
| amount        | int     |
+---------------+---------+
在 SQL 中，(customer_id, visited_on) 是该表的主键。
该表包含一家餐馆的顾客交易数据。
visited_on 表示 (customer_id) 的顾客在 visited_on 那天访问了餐馆。
amount 是一个顾客某一天的消费总额。
</code></pre>
<p>你是餐馆的老板，现在你想分析一下可能的营业额变化增长（每天至少有一位顾客）。</p>
<p>计算以 7 天（某日期 + 该日期前的 6 天）为一个时间段的顾客消费平均值。<code>average_amount</code> 要 <strong>保留两位小数。</strong></p>
<p>结果按 <code>visited_on</code> <strong>升序排序</strong>。</p>
<p>返回结果格式的例子如下。</p>
<p><strong>示例 1:</strong></p>
<pre><code>输入：
Customer 表:
+-------------+--------------+--------------+-------------+
| customer_id | name         | visited_on   | amount      |
+-------------+--------------+--------------+-------------+
| 1           | Jhon         | 2019-01-01   | 100         |
| 2           | Daniel       | 2019-01-02   | 110         |
| 3           | Jade         | 2019-01-03   | 120         |
| 4           | Khaled       | 2019-01-04   | 130         |
| 5           | Winston      | 2019-01-05   | 110         | 
| 6           | Elvis        | 2019-01-06   | 140         | 
| 7           | Anna         | 2019-01-07   | 150         |
| 8           | Maria        | 2019-01-08   | 80          |
| 9           | Jaze         | 2019-01-09   | 110         | 
| 1           | Jhon         | 2019-01-10   | 130         | 
| 3           | Jade         | 2019-01-10   | 150         | 
+-------------+--------------+--------------+-------------+
输出：
+--------------+--------------+----------------+
| visited_on   | amount       | average_amount |
+--------------+--------------+----------------+
| 2019-01-07   | 860          | 122.86         |
| 2019-01-08   | 840          | 120            |
| 2019-01-09   | 840          | 120            |
| 2019-01-10   | 1000         | 142.86         |
+--------------+--------------+----------------+
解释：
第一个七天消费平均值从 2019-01-01 到 2019-01-07 是restaurant-growth/restaurant-growth/ (100 + 110 + 120 + 130 + 110 + 140 + 150)/7 = 122.86
第二个七天消费平均值从 2019-01-02 到 2019-01-08 是 (110 + 120 + 130 + 110 + 140 + 150 + 80)/7 = 120
第三个七天消费平均值从 2019-01-03 到 2019-01-09 是 (120 + 130 + 110 + 140 + 150 + 80 + 110)/7 = 120
第四个七天消费平均值从 2019-01-04 到 2019-01-10 是 (130 + 110 + 140 + 150 + 80 + 110 + 130 + 150)/7 = 142.86
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">select visited_on, total_amount as amount, round(total_amount / 7, 2) as average_amount from( 
    select visited_on, sum(new_amount)over (order by visited_on ROWS 6 PRECEDING) total_amount from (
        select visited_on, sum(amount) new_amount from Customer group by visited_on
    )s
)t where visited_on &gt;= (
    select min(visited_on) + 6 from Customer
);
</code></pre>
<p>窗口函数可以选择操作数据的范围</p>
<p>eg：</p>
<pre><code class="language-sql">取当前行和前五行：ROWS between 5 preceding and current row --共6行
取当前行和后五行：ROWS between current row and 5 following --共6行
取前五行和后五行：ROWS between 5 preceding and 5 folowing --共11行
</code></pre>
<hr>
<h3 id="602-好友申请ll：谁有最多的好友"><a href="#602-好友申请ll：谁有最多的好友" class="headerlink" title="602.好友申请ll：谁有最多的好友"></a>602.好友申请ll：谁有最多的好友</h3><p><code>RequestAccepted</code> 表：</p>
<pre><code>+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| requester_id   | int     |
| accepter_id    | int     |
| accept_date    | date    |
+----------------+---------+
(requester_id, accepter_id) 是这张表的主键(具有唯一值的列的组合)。
这张表包含发送好友请求的人的 ID ，接收好友请求的人的 ID ，以及好友请求通过的日期。
</code></pre>
<p>编写解决方案，找出拥有最多的好友的人和他拥有的好友数目。</p>
<p>生成的测试用例保证拥有最多好友数目的只有 1 个人。</p>
<p>查询结果格式如下例所示。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
RequestAccepted 表：
+--------------+-------------+-------------+
| requester_id | accepter_id | accept_date |
+--------------+-------------+-------------+
| 1            | 2           | 2016/06/03  |
| 1            | 3           | 2016/06/08  |
| 2            | 3           | 2016/06/08  |
| 3            | 4           | 2016/06/09  |
+--------------+-------------+-------------+
输出：
+----+-----+
| id | num |
+----+-----+
| 3  | 3   |
+----+-----+
解释：
编号为 3 的人是编号为 1 ，2 和 4 的人的好友，所以他总共有 3 个好友，比其他人都多。
</code></pre>
<p>解答：</p>
<pre><code class="language-sql">with base as (
    select accepter_id, count(*) as accepted_num 
    from RequestAccepted 
    group by accepter_id
),
seq as (
    select requester_id, count(*) as requested_num 
    from RequestAccepted 
    group by requester_id
),
seq1 as (
    -- 左连接
    select a.accepter_id as id, 
           ifnull(a.accepted_num,0) + ifnull(b.requested_num,0) as num
    from base a 
    left join seq b on a.accepter_id = b.requester_id
    
    union
    
    -- 右连接（补齐右边独有的行）
    select b.requester_id as id, 
           ifnull(a.accepted_num,0) + ifnull(b.requested_num,0) as num
    from base a 
    right join seq b on a.accepter_id = b.requester_id
)
select id, num
from seq1
where num = (select max(num) from seq1);
</code></pre>
<ul>
<li><code>union</code> 保证把两边独有的 id 都保留下来，等价于 FULL OUTER JOIN。</li>
<li><code>ifnull(...,0)</code> 是避免 NULL 相加导致结果为 NULL。</li>
<li>最后取最大值即可。</li>
</ul>
<hr>
<h3 id="585-2016年的投资"><a href="#585-2016年的投资" class="headerlink" title="585.2016年的投资"></a>585.2016年的投资</h3><p><code>Insurance</code> 表：</p>
<pre><code>+-------------+-------+
| Column Name | Type  |
+-------------+-------+
| pid         | int   |
| tiv_2015    | float |
| tiv_2016    | float |
| lat         | float |
| lon         | float |
+-------------+-------+
pid 是这张表的主键(具有唯一值的列)。
表中的每一行都包含一条保险信息，其中：
pid 是投保人的投保编号。
tiv_2015 是该投保人在 2015 年的总投保金额，tiv_2016 是该投保人在 2016 年的总投保金额。
lat 是投保人所在城市的纬度。题目数据确保 lat 不为空。
lon 是投保人所在城市的经度。题目数据确保 lon 不为空。
</code></pre>
<p>编写解决方案报告 2016 年 (<code>tiv_2016</code>) 所有满足下述条件的投保人的投保金额之和：</p>
<ul>
<li>他在 2015 年的投保额 (<code>tiv_2015</code>) 至少跟一个其他投保人在 2015 年的投保额相同。</li>
<li>他所在的城市必须与其他投保人都不同（也就是说 (<code>lat, lon</code>) 不能跟其他任何一个投保人完全相同）。</li>
</ul>
<p><code>tiv_2016</code> 四舍五入的 <strong>两位小数</strong> 。</p>
<p>查询结果格式如下例所示。</p>
<p><strong>示例 1：</strong></p>
<pre><code>输入：
Insurance 表：
+-----+----------+----------+-----+-----+
| pid | tiv_2015 | tiv_2016 | lat | lon |
+-----+----------+----------+-----+-----+
| 1   | 10       | 5        | 10  | 10  |
| 2   | 20       | 20       | 20  | 20  |
| 3   | 10       | 30       | 20  | 20  |
| 4   | 10       | 40       | 40  | 40  |
+-----+----------+----------+-----+-----+
输出：
+----------+
| tiv_2016 |
+----------+
| 45.00    |
+----------+
解释：
表中的第一条记录和最后一条记录都满足两个条件。
tiv_2015 值为 10 与第三条和第四条记录相同，且其位置是唯一的。

第二条记录不符合任何一个条件。其 tiv_2015 与其他投保人不同，并且位置与第三条记录相同，这也导致了第三条记录不符合题目要求。
因此，结果是第一条记录和最后一条记录的 tiv_2016 之和，即 45 。
</code></pre>
<p><strong>解答：</strong></p>
<pre><code class="language-sql">select round(sum(tiv_2016), 2) as tiv_2016 from (
    select a.pid, a.tiv_2016 from Insurance a where a.tiv_2015 in (
        select b.tiv_2015 from Insurance b where a.pid &lt;&gt; b.pid
    ) and not exists (
        select 1 
        from Insurance b 
        where a.pid &lt;&gt; b.pid 
        and a.lat = b.lat 
        and a.lon = b.lon
    )
)t;
</code></pre>
<p><strong>用 <code>in</code> 替代 <code>=</code></strong></p>
<ul>
<li>保证可以匹配多行 <code>tiv_2015</code>，避免语法错误。</li>
<li>逻辑：只要 <code>a.tiv_2015</code> 在别的记录中出现过，就符合。</li>
</ul>
<p><strong>用 <code>not exists</code> 替代 <code>lat != ... or lon != ...</code></strong></p>
<ul>
<li>若写法是“只要某个纬度不同就通过”，会误判。</li>
<li><strong><code>not exists</code> 的逻辑是“不能存在一条记录跟我纬度和经度都相同”。</strong></li>
</ul>
<p><strong>逻辑清晰且无歧义</strong></p>
<ul>
<li>保证有相同 <code>tiv_2015</code>；</li>
<li>保证没有相同 <code>(lat, lon)</code>。</li>
</ul>
<hr>
<p>补充：</p>
<p>SQL执行顺序：</p>
<ul>
<li><strong>FROM</strong> Employee</li>
<li><strong>WHERE</strong> salary &gt; 5000 （先筛掉工资不够的行）</li>
<li><strong>GROUP BY</strong> dept （按部门分组）</li>
<li><strong>COUNT(*)</strong> 统计每组人数</li>
<li><strong>HAVING</strong> 过滤出人数 ≥3 的部门</li>
<li><strong>SELECT</strong> 输出 dept, cnt</li>
<li><strong>ORDER BY</strong> 按 cnt 降序</li>
<li><strong>LIMIT</strong> 取前 5 个部门</li>
</ul>
<p>char_length(col)返回该列的字符数</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Leetcode/" style="color: #ff7d73">Leetcode</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/SQL/" style="color: #ffa2c4">SQL</a>
        </span>
        
    </div>
    <a href="/2022/05/26/Leetcode%20%20sql%E9%A2%98%E7%9B%AE%E8%AE%B0%E5%BD%95/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2021/12/13/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/">
        <h2 class="post-title">数据建模</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                数据建模
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2021/12/13
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h2><h3 id="（1）数据清洗"><a href="#（1）数据清洗" class="headerlink" title="（1）数据清洗"></a>（1）数据清洗</h3><h4 id="1-1-缺失值处理"><a href="#1-1-缺失值处理" class="headerlink" title="1.1 缺失值处理"></a>1.1 缺失值处理</h4><h4 id="1-2-异常值处理"><a href="#1-2-异常值处理" class="headerlink" title="1.2 异常值处理"></a>1.2 异常值处理</h4><h4 id="1-3-数据规约"><a href="#1-3-数据规约" class="headerlink" title="1.3 数据规约"></a>1.3 数据规约</h4><h4 id="1-4-数据变换"><a href="#1-4-数据变换" class="headerlink" title="1.4 数据变换"></a>1.4 数据变换</h4><h2 id="1-数据分析"><a href="#1-数据分析" class="headerlink" title="1. 数据分析"></a>1. 数据分析</h2><h3 id="（1）初步分析"><a href="#（1）初步分析" class="headerlink" title="（1）初步分析"></a>（1）初步分析</h3><h5 id="查看样例数据"><a href="#查看样例数据" class="headerlink" title="查看样例数据"></a>查看样例数据</h5><p>data.head(5)</p>
<h5 id="查看形状"><a href="#查看形状" class="headerlink" title="查看形状"></a>查看形状</h5><p>data.shape()</p>
<h3 id="（2）质量分析"><a href="#（2）质量分析" class="headerlink" title="（2）质量分析"></a>（2）质量分析</h3><h5 id="判断唯一索引是否有重复值"><a href="#判断唯一索引是否有重复值" class="headerlink" title="判断唯一索引是否有重复值"></a>判断唯一索引是否有重复值</h5><p>data[‘ . ‘].nunique() &#x3D;&#x3D; data.shape[0]</p>
<h5 id="缺失值检验"><a href="#缺失值检验" class="headerlink" title="缺失值检验"></a>缺失值检验</h5><p>data.isnull().sum()</p>
<h5 id="异常值检测"><a href="#异常值检测" class="headerlink" title="异常值检测"></a>异常值检测</h5><p>​	打印各个属性的直方图</p>
<pre><code class="language-python">sns.set()
for col in cols:
    statistic = data.describe()
    plt.figure(figsize=(6,6))
    sns.hisplot(data[col], kde=True) # kde核密度曲线，若与直方图走向一致则说明没有异常值
    plt.title...
    plt.show()
</code></pre>
<h5 id="规律一致性检测"><a href="#规律一致性检测" class="headerlink" title="规律一致性检测"></a>规律一致性检测</h5><p>对于单变量且离散数据，可以用以下的方法进行简要判断</p>
<p>​	判断train表与test表是否出自同一分布</p>
<pre><code class="language-python">train_count = train.shape[0]
test_count = test.shape[0]
features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;]

i = 1
plt.figure(figsize=(18, 18))
for feature in features:
    plt.subplot(3, 3, i)
    (train[feature].value_counts().sort_index()/train_count).plot()
    (test[feature].value_counts().sort_index()/test_count).plot()
    plt.legend([&#39;train&#39;, &#39;test&#39;])
    plt.xlabel(feature)
    plt.ylabel(&#39;radio&#39;)
    plt.show
    i+=1
</code></pre>
<p>同时还可以进行多变量联合分布</p>
<p>指的是将离散变量两两组合，然后查看这个新变量的相对占比分布。例如特征1有0&#x2F;1两个取值水平，特征2有A&#x2F;B两个取值水平，则联合分布中就将存在0A、0B、1A、1B四种不同取值水平，然后进一步查看这四种不同取值水平出现的分布情况。</p>
<pre><code class="language-python">def combine_feature(df):
    cols = df.columns
    feature1 = df[cols[0]].astype(str).values.tolist()
    feature2 = df[cols[1]].astype(str).values.tolist()
    return pd.Series([feature1[i]+&#39;&amp;&#39;+feature2[i] for i in range(df.shape[0])])

cols = [features[0], features[1]]

# 查看合并后结果
train_com = combine_feature(train[cols])

train_dis = train_com.value_counts().sort_index()/train_count
test_dis = combine_feature(test[cols]).value_counts().sort_index()/test_count

# 创建新的index
index_dis = pd.Series(train_dis.index.tolist() + test_dis.index.tolist()).drop_duplicates().sort_values()

# 对缺失值填补为0
(index_dis.map(train_dis).fillna(0)).plot()
(index_dis.map(train_dis).fillna(0)).plot()

# 绘图
plt.legend([&#39;train&#39;,&#39;test&#39;])
plt.xlabel(&#39;&amp;&#39;.join(cols))
plt.ylabel(&#39;ratio&#39;)
plt.show()
</code></pre>
<h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><h3 id="（1）缺失值填充"><a href="#（1）缺失值填充" class="headerlink" title="（1）缺失值填充"></a>（1）缺失值填充</h3><p>分别依据情况将缺失值填充为均值或众数（对于离散型数据）或前向后向值（对于连续型数据）</p>
<pre><code class="language-python">data[col] = data[col].fillna(data[col].mean()/data[col].mode()[0])

data[col].ffill(inplcae=True)# 前向填充
data[col].bfill(inplace=True)# 后向填充
</code></pre>
<h3 id="（2）将object类型转换为字典编码"><a href="#（2）将object类型转换为字典编码" class="headerlink" title="（2）将object类型转换为字典编码"></a>（2）将object类型转换为字典编码</h3><p>data.info()查看数据类型</p>
<pre><code class="language-python">def change_object_cols(se):
    # 1. 获取该列的唯一值，并转成列表
    value = se.unique().tolist()
    
    # 2. 对唯一值排序，保证编码的顺序固定（从小到大）
    value.sort()
    
    # 3. 构造一个映射表：唯一值 → 数字索引
    #    例如：[&#39;C&#39;, &#39;Q&#39;, &#39;S&#39;] → &#123; &#39;C&#39;:0, &#39;Q&#39;:1, &#39;S&#39;:2 &#125;
    mapping = pd.Series(range(len(value)), index=value)
    
    # 4. 将原始序列 se 映射成数值序列
    return se.map(mapping).values


for col in cols:
    data[col] = change_object_cols(col)
</code></pre>
<h2 id="3-特征工程"><a href="#3-特征工程" class="headerlink" title="3. 特征工程"></a>3. 特征工程</h2><h3 id="（1）确定离散、连续变量"><a href="#（1）确定离散、连续变量" class="headerlink" title="（1）确定离散、连续变量"></a>（1）确定离散、连续变量</h3><p>这是个很好的问题 👍。在 Kaggle 这种建模比赛里，区分 <strong>离散字段（categorical features）</strong> 和 <strong>连续字段（numerical features）</strong> 是特征工程的第一步，直接影响后续的编码方式、特征衍生、模型效果。一般的经验如下：</p>
<hr>
<ol>
<li><strong>离散字段（Categorical Features）的判定</strong></li>
</ol>
<p>常见特征类型：</p>
<ul>
<li><strong>字符串型</strong>：比如性别（Sex）、职业（job_title）。</li>
<li><strong>整数型但取值有限</strong>：例如舱位等级（Pclass: 1&#x2F;2&#x2F;3）、星期几（1–7）、地区编号。</li>
<li><strong>ID类特征</strong>：如 PassengerId、订单号，虽然是字符串&#x2F;数字，但每个取值都唯一 → 通常丢弃或转化为计数特征。</li>
</ul>
<p>📌 经验：</p>
<ul>
<li><strong>唯一值数量远小于样本数</strong>（如几种类别，而不是几千上万），基本都算离散变量。</li>
<li><strong>整数型变量</strong>要小心，比如邮编、编号，它们表面是数字，其实是分类特征。</li>
<li><strong>高基数特征</strong>（类别数太多）：<ul>
<li>可以尝试 <strong>频率编码（frequency encoding）</strong></li>
<li>或 <strong>target encoding &#x2F; mean encoding</strong>（比赛里常用，但要注意泄露，要交叉验证编码）</li>
</ul>
</li>
</ul>
<hr>
<ol start="2">
<li><strong>连续字段（Numerical Features）的判定</strong></li>
</ol>
<p>常见特征类型：</p>
<ul>
<li><strong>实数型</strong>：身高、体重、金额、时间差。</li>
<li><strong>取值范围大且近似连续的整数</strong>：年龄（0–100）、乘客数量、房价。</li>
</ul>
<p>📌 经验：</p>
<ul>
<li>数值的大小 <strong>有顺序且差值有意义</strong>（例如价格 100 和 200 相差一倍） → 连续特征。</li>
<li>对于这些特征，常用的处理包括：<ul>
<li>标准化（StandardScaler）、归一化（MinMaxScaler）</li>
<li>分箱（binning），转化为类别 → 提高树模型稳定性</li>
</ul>
</li>
</ul>
<hr>
<ol start="3">
<li><strong>混合情况（容易误判的字段）</strong></li>
</ol>
<ul>
<li><strong>Age（年龄）</strong>：是连续的，但可以尝试分箱（儿童&#x2F;青年&#x2F;中年&#x2F;老年），当做离散变量。</li>
<li><strong>Fare（票价）</strong>：连续型，但分布通常长尾，可以对数化（log transform）。</li>
<li><strong>Cabin（船舱号）</strong>：原始是字符串（离散），但如果只取首字母（A&#x2F;B&#x2F;C&#x2F;D&#x2F;E&#x2F;F），相当于类别。</li>
</ul>
<p>📌 Kaggle 常见技巧：</p>
<ul>
<li><strong>连续变量离散化</strong>（binning）后有时比直接用效果更好，尤其在树模型中。</li>
<li>同一个特征既保留原始数值，又加一个分箱类别特征，让模型自己选择。</li>
</ul>
<hr>
<ol start="4">
<li><strong>比赛经验总结</strong></li>
</ol>
<ul>
<li><strong>EDA（探索性数据分析）优先</strong>：先用 <code>describe()</code>、<code>value_counts()</code>、直方图、箱线图，直观判断分布。</li>
<li><strong>看唯一值个数（nunique）</strong>：<ul>
<li>nunique ≪ 样本数 → 倾向离散</li>
<li>nunique ≈ 样本数 → 可能是 ID，直接舍弃或做 count encoding</li>
</ul>
</li>
<li><strong>树模型 vs 线性模型的不同</strong>：<ul>
<li>树模型（LightGBM、XGBoost、CatBoost）：不太怕离散编码（甚至 CatBoost 可以直接处理类别）。</li>
<li>线性模型、神经网络：更依赖标准化和 One-Hot。</li>
</ul>
</li>
<li><strong>比赛 Top 选手习惯</strong>：同一个特征会做多种处理方式（原始连续 &#x2F; 分箱类别 &#x2F; 平滑编码），让模型选择。</li>
</ul>
<hr>
<p>✅ <strong>一句话经验</strong>：</p>
<ul>
<li>“值的大小有顺序和间距意义 → 连续特征”</li>
<li>“值只是标签、没有数值含义 → 离散特征”</li>
<li>模型前期：多做几种处理方式（连续 &#x2F; 离散化 &#x2F; target encoding），交给模型和 CV 去筛选。</li>
</ul>
<h3 id="（2）连续变量处理："><a href="#（2）连续变量处理：" class="headerlink" title="（2）连续变量处理："></a>（2）连续变量处理：</h3><h4 id="无穷值处理："><a href="#无穷值处理：" class="headerlink" title="无穷值处理："></a>无穷值处理：</h4><p>天花板盖帽法：即将inf改为最大的显式数值</p>
<pre><code class="language-python">inf_cols = [&#39;avg_purchases_lag3&#39;, &#39;avg_purchases_lag6&#39;, &#39;avg_purchases_lag12&#39;]
merchant[inf_cols] = merchant[inf_cols].replace(np.inf, merchant[inf_cols].replace(np.inf, -99).max().max())
</code></pre>
<p>整体流程</p>
<ol>
<li>拿到数据及其对应的解释，划分连续型字段与离散型字段，重点关注object类型，是否存在时序类型、文本类型字段</li>
<li>正确性校验：是否unique</li>
<li>特征变化：连续变量：考虑是否归一化、分箱，离散型变量：考虑是否要独热编码、自然数编码</li>
<li>考虑缺失值：（1）numpy.NAN；（2）none；（3）特殊缺失值：空格（可能会被识别转换成字符串）——&gt;业务方面的判断。</li>
</ol>
<p>用均值、众数、预测值进行填充。或者可以先用特殊字符代表，比如说离散性数据进行独热编码后可以用-1表示缺失值</p>
<ol start="5">
<li>异常值分析：识别方式：（1）三倍标准差（对于趋近于标准正态分布的数据）、（2）箱线图法；处理方法：（1）天花板盖帽法；（2）单独识别异常值为某一类</li>
</ol>
<p>特征工程：</p>
<p>（2）根据业务进行扩展：eg：金额连续型——&gt;均值、总额、方差、偏度、离散型——&gt;众数…</p>
<p>根据业务含义划分离散字段category_cols与连续字段numeric_cols。</p>
<p>对非数值型的离散字段进行字典排序编码。</p>
<p>为了能够更方便统计，进行缺失值的处理，对离散字段统一用-1进行填充。</p>
<p>对离散型字段探查发现有正无穷值，这是特征提取以及模型所不能接受的，因此需要对无限值进行处理，此处采用最大值进行替换。</p>
<p>去除与transaction交易记录表格重复的列，以及merchant_id的重复记录。</p>
<p>特征工程</p>
<p>通过特征工程新建了很多特征，包含了一些冗余、稀疏特征，虽然最终选用树模型时都可以筛选出最有效的特征，但是过多的特征会影响到建模的效率。因此需要我们提前进行特征筛选，根据与标签相关性初筛出特征。</p>
<p>初筛：<br>Filter相关系数特征筛选方法</p>
<p>使用corr，得出前300个特征</p>
<p>模型训练需要与超参数的选定过程绑定在一起（使模型具有一定的泛化能力）</p>
<p><strong>Filter 特征筛选</strong>：通过统计指标（相关性、卡方检验、互信息等）先对原始特征进行“粗筛”，去掉明显无关或冗余的特征。</p>
<p><strong>随机森林建模</strong>：用筛选后的特征训练一个随机森林模型，得到基准性能。</p>
<p><strong>网格搜索调优</strong>：在筛选后的特征集上，用交叉验证和网格搜索寻找随机森林的最佳超参数组合。</p>
<p>Wrapper方法</p>
<p><strong>Wrapper 特征筛选</strong>：利用模型性能作为评价标准，逐步选择或剔除特征（例如递归特征消除 RFE）。</p>
<p><strong>LightGBM 建模</strong>：用筛选出的特征训练 LightGBM 模型，作为基准模型。</p>
<p><strong>TPE 调优</strong>：通过贝叶斯优化（TPE 算法）高效搜索 LightGBM 的超参数，提升性能。</p>
<p>使用tpe调优前，最好先用warpper方法训练一次lgb先</p>
<p><strong>Voting 是一种固定规则的集成方法，通过对多个基模型的预测结果直接做多数投票（分类）或平均（回归）来得到最终输出；而 Stacking 则是更灵活的堆叠方法，它把多个基模型的预测结果作为新的特征，再训练一个“元学习器”去自动学习最优的加权组合方式，从而通常能比简单投票获得更强的泛化性能。</strong></p>
<p>Stacking流程：在第一层训练过程中，每一个基模型经过5折交叉验证后会得到5个训练集上结果拼接成的验证集结果，以及5个测试集上的结果平均后的总测试集结果，之后在第二层学习过程中，所有基模型的验证集结果作为训练的x，目标是真实标签y，然后学习x的权重参数a，最终对所有基模型的总测试集结果应用学习完毕的权重a进行组合得到最终结果</p>
<hr>
<h4 id="🔹-第一层（基模型训练）"><a href="#🔹-第一层（基模型训练）" class="headerlink" title="🔹 第一层（基模型训练）"></a>🔹 第一层（基模型训练）</h4><ul>
<li>每个基模型做 <strong>5 折交叉验证</strong>：<ul>
<li><strong>训练集 OOF 预测</strong>：每一折的验证集预测拼接 → 得到完整的 OOF 预测结果（对应训练集所有样本）。</li>
<li><strong>测试集预测</strong>：每一折在训练完毕后对测试集预测 → 5 次预测结果取平均，得到该基模型在测试集上的最终预测。</li>
</ul>
</li>
</ul>
<p>👉 输出：</p>
<ul>
<li><code>OOF_i</code>：基模型 i 的训练集预测（n 行 1 列）。</li>
<li><code>Test_i</code>：基模型 i 的测试集预测（m 行 1 列）。</li>
</ul>
<hr>
<h4 id="🔹-第二层（元学习器）"><a href="#🔹-第二层（元学习器）" class="headerlink" title="🔹 第二层（元学习器）"></a>🔹 第二层（元学习器）</h4><ul>
<li><p><strong>输入特征 X</strong>：所有基模型的 OOF 拼接 → 维度 <code>(n, k)</code>，k 为基模型数量。</p>
</li>
<li><p><strong>目标 y</strong>：真实标签（n 行 1 列）。</p>
</li>
<li><p><strong>训练</strong>：用贝叶斯岭回归（或别的模型）学习一个函数：</p>
<p>y^&#x3D;a1⋅OOF1+a2⋅OOF2+⋯+ak⋅OOFk+b\hat{y} &#x3D; a_1 \cdot OOF_1 + a_2 \cdot OOF_2 + \dots + a_k \cdot OOF_k + b</p>
</li>
</ul>
<p>👉 元学习器本质上是在学习一组 <strong>权重参数 a</strong>，告诉我们“每个基模型在最终预测中该占多大比重”。</p>
<hr>
<h4 id="🔹-最终预测"><a href="#🔹-最终预测" class="headerlink" title="🔹 最终预测"></a>🔹 最终预测</h4><ul>
<li><p>把学习到的权重 a 应用到测试集的预测：</p>
<p>y^test&#x3D;a1⋅Test1+a2⋅Test2+⋯+ak⋅Testk+b\hat{y}_{test} &#x3D; a_1 \cdot Test_1 + a_2 \cdot Test_2 + \dots + a_k \cdot Test_k + b</p>
</li>
</ul>
<p>👉 最终结果就是 <strong>测试集上各基模型预测结果的加权组合</strong>。</p>
<p>交叉验证（Cross Validation，简称 CV）是一种常用的模型评估方法，用来检验机器学习模型在<strong>未见过的数据上的泛化能力</strong>。它的核心思想是：把已有的数据集划分成多个部分，轮流用其中一部分做验证集，剩余部分做训练集，最终综合评估结果。</p>
<hr>
<h4 id="为什么需要交叉验证？"><a href="#为什么需要交叉验证？" class="headerlink" title="为什么需要交叉验证？"></a>为什么需要交叉验证？</h4><p>如果只用一次<strong>训练集 &#x2F; 测试集划分</strong>来评估模型，结果可能会受到划分方式的偶然性影响（比如某次划分刚好测试集比较难）。交叉验证通过多次划分并取平均，可以更稳定地反映模型的真实性能。</p>
<hr>
<h4 id="常见的交叉验证方法"><a href="#常见的交叉验证方法" class="headerlink" title="常见的交叉验证方法"></a>常见的交叉验证方法</h4><ol>
<li><strong>k 折交叉验证（k-Fold CV）</strong><ul>
<li>将数据集平均分成 k 份。</li>
<li>每次用其中 1 份作为验证集，剩下 k-1 份作为训练集。</li>
<li>重复 k 次，得到 k 个结果，取平均值作为最终性能。</li>
<li>常见选择：k &#x3D; 5 或 10。</li>
</ul>
</li>
<li><strong>留一交叉验证（LOOCV, Leave-One-Out CV）</strong><ul>
<li>特殊情况：k 等于样本数。</li>
<li>每次只留 1 个样本做验证，其余作为训练。</li>
<li>计算量大，但对小样本数据很有用。</li>
</ul>
</li>
<li><strong>分层交叉验证（Stratified k-Fold CV）</strong><ul>
<li>用于分类问题。</li>
<li>保证每折中的类别比例与整体数据集一致，避免某些类别在某折中消失。</li>
</ul>
</li>
<li><strong>重复交叉验证（Repeated k-Fold CV）</strong><ul>
<li>在 k 折交叉验证的基础上，重复多次（每次随机划分）。</li>
<li>可以进一步降低划分的偶然性。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="交叉验证的优点"><a href="#交叉验证的优点" class="headerlink" title="交叉验证的优点"></a>交叉验证的优点</h4><ul>
<li>更稳定和可靠的性能估计。</li>
<li>避免单次划分可能造成的高估或低估。</li>
<li>在样本较少时能更充分利用数据。</li>
</ul>
<hr>
<h4 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h4><p>假设有 100 条数据，采用 <strong>5 折交叉验证</strong>：</p>
<ul>
<li>第一次：第 1–20 条做验证，21–100 条做训练。</li>
<li>第二次：第 21–40 条做验证，其他做训练。</li>
<li>…依此类推，总共 5 次。<br> 最后取 5 次验证结果的平均作为模型最终得分。</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/" style="color: #03a9f4">数据建模</a>
        </span>
        
    </div>
    <a href="/2021/12/13/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2021/11/15/%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%E6%9C%AF%E8%AF%AD/">
        <h2 class="post-title">统计分析术语</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                统计分析
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2021/11/15
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="统计分析术语"><a href="#统计分析术语" class="headerlink" title="统计分析术语"></a>统计分析术语</h2><ol>
<li>二分位数</li>
<li>第一四分位数</li>
</ol>
<hr>
<p><strong>箱型图解读要点</strong></p>
<ol>
<li><strong>异常值</strong><ul>
<li>如果数据存在离群点（超出 [Q1−1.5⋅IQR,Q3+1.5⋅IQR][Q1-1.5·IQR, Q3+1.5·IQR] 的范围），会在箱型图外以圆点或星号标注。</li>
</ul>
</li>
<li><strong>箱体高度（IQR &#x3D; Q3 – Q1）</strong><ul>
<li><strong>箱子很短</strong>：数据高度集中，分布较均匀。</li>
<li><strong>箱子很长</strong>：数据分布离散，差异较大。</li>
</ul>
</li>
<li><strong>中位数位置</strong><ul>
<li><strong>中位数接近箱体底部（Q1）</strong>：说明数据偏大（右偏，长尾在高值方向）。</li>
<li><strong>中位数接近箱体顶部（Q3）</strong>：说明数据偏小（左偏，长尾在低值方向）。</li>
<li>中位数是否居中，可以反映数据的 <strong>偏斜程度</strong>。</li>
</ul>
</li>
<li><strong>上下须（Whiskers）长度</strong><ul>
<li>须比较长：说明四分位数之外的数据差异较大 → <strong>方差、标准差大</strong>。</li>
<li>须比较短：说明四分位数之外的数据较集中。</li>
</ul>
</li>
<li><strong>箱型图的边缘并不是极值</strong><ul>
<li>箱体的边缘是 <strong>Q1、Q3</strong>，而不是最小值、最大值。</li>
<li>须的终点才接近“最大值&#x2F;最小值”，但仍可能不是极值（因为须长度有限制）。</li>
</ul>
</li>
</ol>
<hr>
<p>📌 <strong>一句话记忆</strong>：</p>
<ul>
<li><strong>箱子 &#x3D; 中心 50% 数据</strong>，</li>
<li><strong>线（须） &#x3D; 更广的分布范围</strong>，</li>
<li><strong>点 &#x3D; 异常值</strong>，</li>
<li><strong>中位数位置 &#x3D; 偏斜方向</strong>。</li>
</ul>
<ol>
<li>假设检验</li>
<li>显著性水平：当原假设为真时，拒绝原假设的概率</li>
<li>功效：当原假设为假时，能检测出这种错误并得出正确结论的概率</li>
<li>置信区间：用来估计总体参数可能范围的一个区间，反映了我们对总体参数估计的信任度</li>
<li>p值：出现极端情况的概率，当原假设为真时，出现比当前抽样分布更加极端的情况的概率</li>
<li>p与$\alpha$的关系：p小于a，则小概率事件发生，则拒绝H0</li>
<li>一类错误：当零假设为真时，拒绝</li>
<li>二类错误：当零假设为假时，接受</li>
<li>原假设与备择假设</li>
<li>中心极限定理</li>
<li>大数定理：随着试验次数的增加，样本平均值趋近总体的期望值</li>
<li>卡方检验：分析变量之间是否独立</li>
<li>方差分析：比较三个或更多的均值是否差异</li>
<li>z检验适用于大样本，总体方差已知</li>
<li>t检验适用于小样本，总体方差未知</li>
<li>最大似然估计</li>
<li>最小二乘估计</li>
<li>交叉验证</li>
</ol>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" style="color: #00a596">统计分析</a>
        </span>
        
    </div>
    <a href="/2021/11/15/%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%E6%9C%AF%E8%AF%AD/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <span class="current">1</span>
    
    <a class="page-num" href="/page/2">
        2
    </a>
    
    
    
    
    <a class="page-num" href="/page/2/">
        <i class="fa-solid fa-caret-right fa-fw"></i>
    </a>
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/pic.jpg" alt="avatar" />
        </div>
        <div class="name">KING BOB</div>
        <div class="description">
            <p>Description<br>…</p>

        </div>
        
        
        <div class="friend-links">
            
            <div class="friend-link">
                <a target="_blank" rel="noopener" href="https://argvchs.github.io">Argvchs</a>
            </div>
            
        </div>
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2025 - 2025 KING!BOB!
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;KING BOB
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    <canvas
    id="fireworks"
    style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767"
></canvas>
<script src="https://s4.zstatic.net/ajax/libs/animejs/3.2.1/anime.min.js"></script>
<script src="/js/fireworks.min.js"></script>
    <canvas
    id="background"
    style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: -1"
></canvas>
<script src="/js/background.min.js"></script>
    
<!-- hexo injector body_end start --><script data-pjax src="https://unpkg.com/oh-my-live2d"></script><script>const oml2d = OML2D.loadOml2d({dockedPosition:"right",mobileDisplay:true,models:[{"path":"https://unpkg.com/live2d-widget-model-shizuku@1.0.5/assets/shizuku.model.json","mobilePosition":[-10,23],"mobileScale":0.1,"mobileStageStyle":{"width":180,"height":166},"motionPreloadStrategy":"IDLE","position":[-10,35],"scale":0.15,"stageStyle":{"width":250,"height":250}},{"path":"https://unpkg.com/live2d-widget-model-koharu@1.0.5/assets/koharu.model.json","scale":0.12,"position":[0,0],"stageStyle":{"width":250},"mobileScale":0.08,"mobilePosition":[0,0],"mobileStageStyle":{"width":180}},{"path":"https://unpkg.com/live2d-widget-model-haruto@1.0.5/assets/haruto.model.json","scale":0.12,"position":[0,0],"mobileScale":0.08,"mobilePosition":[0,0],"mobileStageStyle":{"width":180},"stageStyle":{"width":250}}],parentElement:document.body,primaryColor:"var(--btn-bg)",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body>
</html>
